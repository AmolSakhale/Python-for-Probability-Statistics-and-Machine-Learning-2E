{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Sampling Methods\n",
    "\n",
    "So far, we have studied analytical ways to transform random variables and how\n",
    "to augment these methods using Python. In spite of all this, we frequently must\n",
    "resort to purely numerical methods to solve real-world problems.  Hopefully,\n",
    "now that we have seen the deeper theory, these numerical methods will feel more\n",
    "concrete.  Suppose we want to generate samples of a given density, $f(x)$,\n",
    "given we already can generate samples from a uniform distribution,\n",
    "$\\mathcal{U}[0,1]$.  How do we know a random sample $v$ comes from the $f(x)$\n",
    "distribution?  One approach is to look at how a histogram of samples of $v$\n",
    "approximates $f(x)$. Specifically,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:mc01\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{P}( v \\in N_{\\Delta}(x) )  = f(x) \\Delta x \n",
    "\\end{equation}\n",
    "\\label{eq:mc01} \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-probability/Sampling_Monte_Carlo_000.png, width=500 frac=0.75] The histogram approximates the target probability density. <div id=\"fig:Sampling_Monte_Carlo_000\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Sampling_Monte_Carlo_000\"></div>\n",
    "\n",
    "<p>The histogram approximates the target probability density.</p>\n",
    "<img src=\"fig-probability/Sampling_Monte_Carlo_000.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    " which says that the probability that a sample is in some $N_\\Delta$\n",
    "neighborhood of $x$ is approximately $f(x)\\Delta x$. [Figure](#fig:Sampling_Monte_Carlo_000) shows the target probability density function\n",
    "$f(x)$ and a histogram that approximates it. The histogram is generated from\n",
    "samples $v$. The hatched rectangle in the center illustrates Equation\n",
    "[1](#eq:mc01). The area of this rectangle is approximately $f(x)\\Delta x$ where\n",
    "$x=0$, in this case. The width of the rectangle is $N_{\\Delta}(x)$ The quality\n",
    "of the approximation may be clear visually, but to know that $v$ samples are\n",
    "characterized by $f(x)$, we need the statement of Equation [1](#eq:mc01), which\n",
    "says that the proportion of samples $v$  that fill the hatched rectangle is\n",
    "approximately equal to $f(x)\\Delta x$.\n",
    "\n",
    "Now that we know how to evaluate samples $v$ that are characterized by the density\n",
    "$f(x)$, let's consider how to create these samples for both discrete and\n",
    "continuous random variables.\n",
    "\n",
    "## Inverse CDF Method for Discrete Variables\n",
    "\n",
    "Suppose we want to generate samples from a fair six-sided die.  Our workhouse\n",
    "uniform random variable is defined continuously over the unit interval  and the\n",
    "fair six-sided die is discrete. We must first create a mapping between the\n",
    "continuous random variable $u$ and the discrete outcomes of the die. This\n",
    "mapping is shown in [Figure](#fig:Sampling_Monte_Carlo_0001) where the unit\n",
    "interval is broken up into segments, each of length $1/6$. Each individual\n",
    "segment is assigned to one of the die outcomes. For example, if $u \\in\n",
    "[1/6,2/6)$, then the outcome for the die is $2$. Because the die is fair, all\n",
    "segments on the unit interval are the same length. Thus, our new random\n",
    "variable $v$ is derived from $u$ by this assignment.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-probability/Sampling_Monte_Carlo_0001.png, width=500 frac=0.75] A uniform distribution random variable on the unit interval is assigned to the six outcomes of a fair die using these segements. <div id=\"fig:Sampling_Monte_Carlo_0001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Sampling_Monte_Carlo_0001\"></div>\n",
    "\n",
    "<p>A uniform distribution random variable on the unit interval is assigned to the six outcomes of a fair die using these segements.</p>\n",
    "<img src=\"fig-probability/Sampling_Monte_Carlo_0001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "For example, for $v=2$, we have,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(v=2) = \\mathbb{P}(u\\in [1/6,2/6)) = 1/6\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where, in the language of the Equation [1](#eq:mc01), $f(x)=1$\n",
    "(uniform distribution), $\\Delta x = 1/6$, and $N_\\Delta (2)=[1/6,2/6)$.\n",
    "Naturally, this pattern holds for all the other die outcomes in\n",
    "$\\left\\{1,2,3,..,6\\right\\}$.  Let's consider a quick simulation to make this\n",
    "concrete. The following code generates uniform random samples and stacks them\n",
    "in a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "u= np.random.rand(100)\n",
    "df = DataFrame(data=u,columns=['u'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The next block uses `pd.cut` to map the individual samples to\n",
    "the set $\\left\\{1,2,\\ldots,6\\right\\}$ labeled `v`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = [1,2,3,4,5,6]\n",
    "df['v']=pd.cut(df.u,np.linspace(0,1,7),\n",
    "               include_lowest=True,labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is what the dataframe contains. The `v` column contains\n",
    "the samples drawn from the fair die."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following is a count of the number of samples in each group. There\n",
    "should be roughly the same number of samples in each group because the die is fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupby('v').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " So far, so good. We now have a way to simulate a fair\n",
    "die from a uniformly distributed random variable.\n",
    "\n",
    "To extend this to unfair die, we need only make some small adjustments to this\n",
    "code.  For example, suppose that we want an unfair die so that\n",
    "$\\mathbb{P}(1)=\\mathbb{P}(2)=\\mathbb{P}(3)=1/12$ and\n",
    "$\\mathbb{P}(4)=\\mathbb{P}(5)=\\mathbb{P}(6)=1/4$. The only change we have to\n",
    "make is with `pd.cut` as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['v']=pd.cut(df.u,[0,1/12,2/12,3/12,2/4,3/4,1],\n",
    "               include_lowest=True,labels=labels)\n",
    "df.groupby('v').count()/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where now these are the individual probabilities of each digit. You\n",
    "can take more than `100` samples to get a clearer view of the individual\n",
    "probabilities but the mechanism for generating them is the same. The method is\n",
    "called the inverse CDF [^CDF] method because the CDF\n",
    "(namely,$\\texttt{[0,1/12,2/12,3/12,2/4,3/4,1]}$) in the last example has been\n",
    "inverted (using the `pd.cut` method) to generate the samples.  \n",
    "The inversion is easier to see for continuous variables, which we consider\n",
    "next.\n",
    "\n",
    "[^CDF]: Cumulative density function. Namely, $F(x)=\\mathbb{P}(X < x)$.\n",
    "\n",
    "\n",
    "## Inverse CDF Method for Continuous Variables\n",
    "\n",
    "The method above applies to continuous random variables, but now we have to \n",
    "squeeze the  intervals down to individual points. In the example above, our\n",
    "inverse function was a piecewise function that operated on uniform random\n",
    "samples. In this case, the piecewise function collapses to a continuous inverse\n",
    "function.  We want to generate random samples for a CDF that is invertible.\n",
    "As before, the criterion for generating an appropriate sample $v$ is the\n",
    "following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(F(x) < v < F(x+\\Delta x)) =  F(x+\\Delta x) - F(x) = \\int_x^{x+\\Delta x} f(u) du \\approx  f(x) \\Delta x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which says that the probability that the sample $v$ is contained in a\n",
    "$\\Delta x$ interval is approximately equal to $f(x) \\Delta x$, at that point.\n",
    "Once again, the trick is to use a uniform random sample $u$ and an invertible\n",
    "CDF $F(x)$ to construct these samples.  Note that for a uniform random variable\n",
    "$u \\sim \\mathcal{U}[0,1]$, we have,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(x < F^{-1}(u) < x+\\Delta x) & = \\mathbb{P}(F(x) < u < F(x+\\Delta x)) \\\\\\\n",
    "                                       & =  F(x+\\Delta x) - F(x) \\\\\\\n",
    "                                       & = \\int_x^{x+\\Delta x} f(p) dp \\approx  f(x) \\Delta x\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This means that $ v=F^{-1}(u) $ is distributed according to $f(x)$,\n",
    "which is what we want.  \n",
    "\n",
    "Let's try this to generate samples from the\n",
    "exponential distribution,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f_{\\alpha}(x) = \\alpha e^{ -\\alpha x }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which has the following CDF,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(x) = 1-e^{ -\\alpha x  }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and corresponding inverse,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F^{-1}(u)  = \\frac{1}{\\alpha}\\ln \\frac{1}{(1-u)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, all we have to do is generate some uniformly distributed\n",
    "random samples and then feed them into $F^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import array, log\n",
    "import scipy.stats\n",
    "alpha = 1.   # distribution parameter\n",
    "nsamp = 1000 # num of samples\n",
    "# define uniform random variable\n",
    "u=scipy.stats.uniform(0,1)\n",
    "# define inverse function\n",
    "Finv=lambda u: 1/alpha*log(1/(1-u))\n",
    "# apply inverse function to samples\n",
    "v = array(list(map(Finv,u.rvs(nsamp))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, we have the samples from the exponential distribution, but how\n",
    "do we know the method is correct with samples distributed accordingly?\n",
    "Fortunately, `scipy.stats` already has a exponential distribution, so we can\n",
    "check our work against the reference using a *probability plot* (i.e.,  also\n",
    "known as a *quantile-quantile* plot).  The following code sets up the\n",
    "probability plot from `scipy.stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import setp, subplots\n",
    "fig,ax = subplots()\n",
    "fig.set_size_inches((7,5))\n",
    "_=scipy.stats.probplot(v,(1,),dist='expon',plot=ax)\n",
    "line=ax.get_lines()[0]\n",
    "_=setp(line,'color','k')\n",
    "_=setp(line,'alpha',.1)\n",
    "line=ax.get_lines()[1]\n",
    "_=setp(line,'color','gray')\n",
    "_=setp(line,'lw',3.0)\n",
    "_=setp(ax.yaxis.get_label(),'fontsize',18)\n",
    "_=setp(ax.xaxis.get_label(),'fontsize',18)\n",
    "_=ax.set_title('Probability Plot',fontsize=18)\n",
    "_=ax.grid()\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-probability/Sampling_Monte_Carlo_005.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "scipy.stats.probplot(v,(1,),dist='expon',plot=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that we have to supply an axes object (`ax`) for it to draw on.\n",
    "The result is [Figure](#fig:Sampling_Monte_Carlo_005). The more the samples\n",
    "line match the diagonal line, the more they match the reference distribution\n",
    "(i.e., exponential distribution in this case). You may also want to try\n",
    "`dist=norm` in the code above To see what happens when the normal distribution\n",
    "is the reference distribution.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-probability/Sampling_Monte_Carlo_005.png, width=500 frac=0.85] The samples created using the inverse cdf method match the exponential reference distribution. <div id=\"fig:Sampling_Monte_Carlo_005\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Sampling_Monte_Carlo_005\"></div>\n",
    "\n",
    "<p>The samples created using the inverse cdf method match the exponential reference distribution.</p>\n",
    "<img src=\"fig-probability/Sampling_Monte_Carlo_005.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "## Rejection Method\n",
    "\n",
    "In some cases, inverting the CDF may be impossible. The *rejection*\n",
    "method can handle this situation. The idea is to pick two uniform random\n",
    "variables $u_1,u_2 \\sim \\mathcal{U}[a,b]$ so that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}\\left(u_1 \\in N_{\\Delta}(x) \\bigwedge u_2 < \\frac{f(u_1)}{M} \\right) \\hspace{0.5em} \\approx \\frac{\\Delta x}{b-a} \\frac{f(u_1)}{M}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where we take $x=u_1$ and $f(x) < M $.  This is a two-step process.\n",
    "First, draw $u_1$ uniformly from the interval $[a,b]$. Second,  feed it into\n",
    "$f(x)$ and if $u_2 < f(u_1)/M$, then you have a valid sample for $f(x)$. Thus,\n",
    "$u_1$ is the proposed sample from $f$ that may or may not be rejected depending\n",
    "on $u_2$. The only job of the $M$ constant is to scale down the $f(x)$ so that\n",
    "the $u_2$ variable can span the range.  The *efficiency* of this method is the\n",
    "probability of accepting $u_1$ which comes from integrating out the above\n",
    "approximation,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\int \\frac{f(x)}{M(b-a)} dx = \\frac{1}{M(b-a)} \\int f(x)dx =\\frac{1}{M(b-a)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This means that we don't want an unecessarily large $M$ because that\n",
    "makes it more likely that samples will be discarded. \n",
    "\n",
    "Let's try this method for a density that does not have a continuous inverse [^normalization]. \n",
    "\n",
    "[^normalization]: Note that this example density does not *exactly* integrate\n",
    "out to one like a probability density function should, but the normalization\n",
    "constant for this is distracting for our purposes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = \\exp\\left(-\\frac{(x-1)^2}{2x} \\right) (x+1)/12\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $x>0$. The following code implements the rejection plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.linspace(0.001,15,100)\n",
    "f= lambda x: np.exp(-(x-1)**2/2./x)*(x+1)/12.\n",
    "fx = f(x)\n",
    "M=0.3                          # scale factor\n",
    "u1 = np.random.rand(10000)*15  # uniform random samples scaled out\n",
    "u2 = np.random.rand(10000)     # uniform random samples\n",
    "idx,= np.where(u2<=f(u1)/M)    # rejection criterion\n",
    "v = u1[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "fig.set_size_inches((9,5))\n",
    "_=ax.hist(v,density=True,bins=40,alpha=.3,color='gray')\n",
    "_=ax.plot(x,fx,'k',lw=3.,label='$f(x)$')\n",
    "_=ax.set_title('Estimated Efficency=%3.1f%%'%(100*len(v)/len(u1)),\n",
    "             fontsize=18)\n",
    "_=ax.legend(fontsize=18)\n",
    "_=ax.set_xlabel('$x$',fontsize=24)\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-probability/Sampling_Monte_Carlo_007.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Figure](#fig:Sampling_Monte_Carlo_007) shows a histogram of the\n",
    "so-generated samples that nicely fits the probability density function. The\n",
    "title in the figure shows the efficiency (the number of rejected samples),\n",
    "which is poor. It means that we threw away most of the proposed samples.  Thus,\n",
    "even though there is nothing conceptually wrong with this result, the low\n",
    "efficiency must be fixed, as a practical matter.  [Figure](#fig:Sampling_Monte_Carlo_008) shows where the proposed samples were\n",
    "rejected. Samples under the curve were retained (i.e., $u_2 <\n",
    "\\frac{f(u_1)}{M}$) but the vast majority of the samples are outside this\n",
    "umbrella.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-probability/Sampling_Monte_Carlo_007.png, width=500 frac=0.75] The rejection method generate samples in the histogram that nicely match the target distribution. Unfortunately, the efficiency is not so good. <div id=\"fig:Sampling_Monte_Carlo_007\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Sampling_Monte_Carlo_007\"></div>\n",
    "\n",
    "<p>The rejection method generate samples in the histogram that nicely match the target distribution. Unfortunately, the efficiency is not so good.</p>\n",
    "<img src=\"fig-probability/Sampling_Monte_Carlo_007.png\" width=500>\n",
    "\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "fig.set_size_inches((9,5))\n",
    "_=ax.plot(u1,u2,'+',label='rejected',alpha=.3,color='gray')\n",
    "_=ax.plot(u1[idx],u2[idx],'.',label='accepted',alpha=.3,color='k')\n",
    "_=ax.legend(fontsize=22)\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-probability/Sampling_Monte_Carlo_008.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-probability/Sampling_Monte_Carlo_008.png, width=500 frac=0.75] The proposed samples under the curve were accepted and the others were not. This shows the majority of samples were rejected.  <div id=\"fig:Sampling_Monte_Carlo_008\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Sampling_Monte_Carlo_008\"></div>\n",
    "\n",
    "<p>The proposed samples under the curve were accepted and the others were not. This shows the majority of samples were rejected.</p>\n",
    "<img src=\"fig-probability/Sampling_Monte_Carlo_008.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "The rejection method uses $u_1$ to select along the domain of $f(x)$ and the\n",
    "other $u_2$ uniform random variable  decides whether to accept or not. One idea\n",
    "would be to choose $u_1$ so that $x$ values are coincidentally those that are\n",
    "near the peak of $f(x)$, instead of uniformly anywhere in the domain,\n",
    "especially near the tails, which are low probability anyway. Now, the trick is\n",
    "to find a new density function $g(x)$ to sample from that has a similiar\n",
    "concentration of probability density. One way it to familiarize oneself with\n",
    "the probability density functions that have adjustable parameters and fast random\n",
    "sample generators already. There are lots of places to look and, chances are,\n",
    "there is likely already such a generator for your problem. Otherwise, the\n",
    "family of $\\beta$ densities is a good place to start. \n",
    "\n",
    "To be explicit, what we want is $u_1 \\sim g(x)$ so that, returning to our\n",
    "earlier argument,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}\\left( u_1 \\in N_{\\Delta}(x) \\bigwedge u_2 < \\frac{f(u_1)}{M} \\right) \\approx g(x) \\Delta x \\frac{f(u_1)}{M}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " but this is *not* what we need here. The problem is with the\n",
    "second part of the logical $\\bigwedge$ conjunction. We need to put\n",
    "something there that will give us something proportional to $f(x)$.\n",
    "Let us  define the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:rej01\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " h(x) = \\frac{f(x)}{g(x)} \n",
    "\\end{equation}\n",
    "\\label{eq:rej01} \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " with corresponding maximum on the domain as $h_{\\max}$ and\n",
    "then go back and construct the second part of the clause as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}\\left(u_1 \\in N_{\\Delta}(x) \\bigwedge u_2 < \\frac{h(u_1)}{h_{\\max}} \\right) \\approx g(x) \\Delta x \\frac{h(u_1)}{h_{\\max}} = f(x)/h_{\\max}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Recall that satisfying this criterion means that $u_1=x$. As before,\n",
    "we can estimate the probability of acceptance of the $u_1$ as $1/h_{\\max}$.\n",
    "\n",
    "Now, how to construct the $g(x)$ function in the denominator of Equation\n",
    "[2](#eq:rej01)? Here's where familarity with some standard probability densities\n",
    "pays off. For this case, we choose the $\\chi^2$ distribution. The following\n",
    "plots the $g(x)$ and $f(x)$ (left plot) and the corresponding $h(x)=f(x)/g(x)$\n",
    "(right plot). Note that $g(x)$ and $f(x)$ have peaks that almost coincide,\n",
    "which is what we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ch=scipy.stats.chi2(4) # chi-squared\n",
    "h = lambda x: f(x)/ch.pdf(x) # h-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,axs=subplots(1,2,sharex=True)\n",
    "fig.set_size_inches(12,4)\n",
    "_=axs[0].plot(x,fx,label='$f(x)$',color='k')\n",
    "_=axs[0].plot(x,ch.pdf(x),'--',lw=2,label='$g(x)$',color='gray')\n",
    "_=axs[0].legend(loc=0,fontsize=24)\n",
    "_=axs[0].set_xlabel(r'$x$',fontsize=22)\n",
    "_=axs[1].plot(x,h(x),'-k',lw=3)\n",
    "_=axs[1].set_title('$h(x)=f(x)/g(x)$',fontsize=24)\n",
    "_=axs[1].set_xlabel(r'$x$',fontsize=22)\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-probability/Sampling_Monte_Carlo_009.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-probability/Sampling_Monte_Carlo_009.png, width=500 frac=0.95] The plot on the right shows $h(x)=f(x)/g(x)$ and the one on the left shows $f(x)$ and $g(x)$ separately. <div id=\"fig:Sampling_Monte_Carlo_009\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Sampling_Monte_Carlo_009\"></div>\n",
    "\n",
    "<p>The plot on the right shows $h(x)=f(x)/g(x)$ and the one on the left shows $f(x)$ and $g(x)$ separately.</p>\n",
    "<img src=\"fig-probability/Sampling_Monte_Carlo_009.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    " Now, let's generate  some samples from this $\\chi^2$\n",
    "distribution with the rejection method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hmax=h(x).max()\n",
    "u1 = ch.rvs(5000)        # samples from chi-square distribution\n",
    "u2 = np.random.rand(5000)# uniform random samples\n",
    "idx = (u2 <= h(u1)/hmax) # rejection criterion\n",
    "v = u1[idx]              # keep these only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "fig.set_size_inches((7,3))\n",
    "_=ax.hist(v,density=True,bins=40,alpha=.3,color='gray')\n",
    "_=ax.plot(x,fx,color='k',lw=3.,label='$f(x)$')\n",
    "_=ax.set_title('Estimated Efficency=%3.1f%%'%(100*len(v)/len(u1)))\n",
    "_=ax.axis(xmax=15)\n",
    "_=ax.legend(fontsize=18)\n",
    "fig.savefig('fig-probability/Sampling_Monte_Carlo_010.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-probability/Sampling_Monte_Carlo_010.png, width=500 frac=0.85]  Using the updated method, the histogram matches the target probability density function with high efficiency. <div id=\"fig:Sampling_Monte_Carlo_010\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Sampling_Monte_Carlo_010\"></div>\n",
    "\n",
    "<p>Using the updated method, the histogram matches the target probability density function with high efficiency.</p>\n",
    "<img src=\"fig-probability/Sampling_Monte_Carlo_010.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "Using the $\\chi^2$ distribution with the rejection method results in throwing\n",
    "away less than 10% of the generated samples compared with our prior example\n",
    "where we threw out at least 80%. This is dramatically more\n",
    "efficient!  [Figure](#fig:Sampling_Monte_Carlo_010) shows that the histogram\n",
    "and the probability density function match.  For completeness, [Figure](#fig:Sampling_Monte_Carlo_011) shows the samples with the corresponding\n",
    "threshold $h(x)/h_{\\max}$ that was used to select them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "fig.set_size_inches((7,4))\n",
    "_=ax.plot(u1,u2,'+',label='rejected',alpha=.3,color='gray')\n",
    "_=ax.plot(u1[idx],u2[idx],'g.',label='accepted',alpha=.3,color='k')\n",
    "_=ax.plot(x,h(x)/hmax,color='k',lw=3.,label='$h(x)$')\n",
    "_=ax.legend(fontsize=16,loc=0) \n",
    "_=ax.set_xlabel('$x$',fontsize=24)\n",
    "_=ax.set_xlabel('$h(x)$',fontsize=24)\n",
    "_=ax.axis(xmax=15,ymax=1.1)\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-probability/Sampling_Monte_Carlo_011.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-probability/Sampling_Monte_Carlo_011.png, width=500 frac=0.95]  Fewer proposed points were rejected in this case, which means better efficiency. <div id=\"fig:Sampling_Monte_Carlo_011\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Sampling_Monte_Carlo_011\"></div>\n",
    "\n",
    "<p>Fewer proposed points were rejected in this case, which means better efficiency.</p>\n",
    "<img src=\"fig-probability/Sampling_Monte_Carlo_011.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "# Sampling Importance Resampling\n",
    "\n",
    "An alternative to the Rejection Method that does not involve rejecting samples\n",
    "or coming up with $M$ bounds or bounding functions is the Sampling Importance\n",
    "Resampling (SIR) method.  Choose a tractable $g$ probability density function\n",
    "and draw a $n$ samples from it, $\\lbrace x_i \\rbrace_{i=1}^n$. Our objective is\n",
    "to derive samples $f$.  Next, compute the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "q_i = \\frac{w_i}{\\sum w_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w_i = \\frac{f(x_i)}{g(x_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The $q_i$ define a probability mass function whose samples\n",
    "approximate samples from $f$. To see this, consider,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(X\\le a) =& \\sum_{i=1}^n q_i \\mathbb{I}_{(-\\infty,a]}(x_i) \\\\\n",
    "                   =& \\frac{\\sum_{i=1}^n w_i \\mathbb{I}_{(-\\infty,a]}(x_i)}{\\sum_{i=1}^n w_i} \\\\\n",
    "                   =& \\frac{\\frac{1}{n}\\sum_{i=1}^n \\frac{f(x_i)}{g(x_i)} \\mathbb{I}_{(-\\infty,a]}(x_i)}{\\frac{1}{n}\\sum_{i=1}^n \\frac{f(x_i)}{g(x_i)}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Because the samples are generated from the $g$ probability distribution, \n",
    "the numerator is approximately,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}_g\\left(\\frac{f(x)}{g(x)}\\right) = \\int_{-\\infty}^a f(x) dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which gives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(X\\le a) =  \\int_{-\\infty}^a f(x) dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which shows that the samples generated this way are $f$-distributed.\n",
    "Note more samples have to be generated from this probability mass function  the\n",
    "further away $g$ is from the desired function $f$. Further, because there is no\n",
    "rejection step, we no longer have the issue of efficiency.\n",
    "\n",
    "For example, let us choose a beta distribution for $g$, as in the following code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = scipy.stats.beta(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This distribution does not bear a strong resemblance to our desired\n",
    "$f$ function from last section.  as shown in the [Figure](#fig:Sampling_Monte_Carlo_012) below.  Note that we scaled the domain of the\n",
    "beta distribution to get it close to the support of $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots(figsize=[10,5])\n",
    "g = scipy.stats.beta(2,3)\n",
    "_=ax.plot(x,fx,label='$f(x)$',color='k')\n",
    "_=ax.plot(np.linspace(0,1,100)*15,1/15*g.pdf(np.linspace(0,1,100)),label='$g(x)$',color='r')\n",
    "_=ax.legend(fontsize=16)\n",
    "_=ax.set_xlabel('$x$',fontsize=18)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "fig.savefig('fig-probability/Sampling_Monte_Carlo_012.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-probability/Sampling_Monte_Carlo_012.png, width=500 frac=0.95]  Histogram of samples generated using SIR comparted to target probability density function. <div id=\"fig:Sampling_Monte_Carlo_012\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Sampling_Monte_Carlo_012\"></div>\n",
    "\n",
    "<p>Histogram of samples generated using SIR comparted to target probability density function.</p>\n",
    "<img src=\"fig-probability/Sampling_Monte_Carlo_012.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    " In the next block, we sample from the $g$ distribution and compute\n",
    "the weights as described above. The final step is to sample from this new\n",
    "probability mass function.  The resulting normalized histogram is shown\n",
    "compared to the target $f$ probability density function in [Figure](#fig:Sampling_Monte_Carlo_013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xi = g.rvs(500)\n",
    "w = np.array([f(i*15)/g.pdf(i) for i in xi])\n",
    "fsamples=np.random.choice(xi*15,5000,p = w/w.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots(figsize=[10,5])\n",
    "_=ax.hist(fsamples,rwidth=.8,density=True,bins=20,color='gray')\n",
    "_=ax.plot(x,fx,label='$f(x)$',color='k')\n",
    "_=ax.legend(fontsize=14)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "fig.savefig('fig-probability/Sampling_Monte_Carlo_013.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-probability/Sampling_Monte_Carlo_013.png, width=500 frac=0.95]  Histogram and probability density function using SIR. <div id=\"fig:Sampling_Monte_Carlo_013\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Sampling_Monte_Carlo_013\"></div>\n",
    "\n",
    "<p>Histogram and probability density function using SIR.</p>\n",
    "<img src=\"fig-probability/Sampling_Monte_Carlo_013.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "In this section, we investigated how to generate random samples from a given\n",
    "distribution, beit discrete or continuous. For the continuous case, the key\n",
    "issue was whether or not the cumulative density function had a continuous\n",
    "inverse. If not, we had to turn to the rejection method, and find an\n",
    "appropriate related density that we could easily sample from to use as part of\n",
    "a rejection threshold. Finding such a function is an art, but many families of\n",
    "probability densities have been studied over the years that already have fast\n",
    "random number generators.\n",
    "\n",
    "The rejection method has many complicated extensions that involve careful\n",
    "partitioning of the domains and lots of special methods for corner cases.\n",
    "Nonetheless, all of these advanced techniques are still variations on the same\n",
    "fundamental theme we illustrated here [[dunn2011exploring]](#dunn2011exploring),\n",
    "[[johnson1995continuous]](#johnson1995continuous)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
