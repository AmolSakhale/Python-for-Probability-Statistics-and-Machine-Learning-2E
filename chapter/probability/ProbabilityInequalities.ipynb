{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import textwrap\n",
    "import sys, re\n",
    "old_displayhook = sys.displayhook\n",
    "def displ(x):\n",
    "   if x is None: return\n",
    "   print(\"\\n\".join(textwrap.wrap(repr(x).replace(' ',''),width=60)))\n",
    "\n",
    "sys.displayhook=displ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Inequalities\n",
    "\n",
    "In practice, few quantities can be analytically calculated. Some knowledge\n",
    "of bounding inequalities helps find the ballpark for potential solutions. This\n",
    "sections discusses three key inequalities that are important for \n",
    "probability, statistics, and machine learning.\n",
    "\n",
    "## Markov's Inequality\n",
    "\n",
    "Let $X$ be a non-negative random variable\n",
    "and suppose that $\\mathbb{E}(X) < \\infty$. Then,\n",
    "for any $t>0$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(X>t)\\leq \\frac{\\mathbb{E}(X)}{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is a foundational inequality that is\n",
    "used as a stepping stone to other inequalities. It is easy\n",
    "to prove. Because $X>0$, we have the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(X)&=\\int_0^\\infty x f_x(x)dx =\\underbrace{\\int_0^t x f_x(x)dx}_{\\text{omit this}}+\\int_t^\\infty x f_x(x)dx \\\\\\ \n",
    "             &\\ge\\int_t^\\infty x f_x(x)dx \\ge t\\int_t^\\infty f_x(x)dx = t \\mathbb{P}(X>t)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The step that establishes the inequality is the part where the\n",
    "$\\int_0^t x f_x(x)dx$ is omitted.  For a particular $f_x(x)$ that may be\n",
    "concentrated around the $[0,t]$ interval, this could be a lot to throw out.\n",
    "For that reason, the Markov Inequality is considered a *loose* inequality,\n",
    "meaning that there is a substantial gap between both sides of the inequality.\n",
    "For example, as shown in [Figure](#fig:ProbabilityInequalities_001), the\n",
    "$\\chi^2$ distribution has a lot of its mass on the left, which would be omitted\n",
    "in the  Markov Inequality. [Figure](#fig:ProbabilityInequalities_002) shows\n",
    "the two curves established by the Markov Inequality. The gray shaded region is\n",
    "the gap between the two terms and indicates that looseness of the bound\n",
    "(fatter shaded region) for this case.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-probability/ProbabilityInequalities_001.png, width=500 frac=0.75] The $\\chi_1^2$ density has much of its weight on the left, which is excluded in the establishment of the Markov Inequality. <div id=\"fig:ProbabilityInequalities_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:ProbabilityInequalities_001\"></div>\n",
    "\n",
    "<p>The $\\chi_1^2$ density has much of its weight on the left, which is excluded in the establishment of the Markov Inequality.</p>\n",
    "<img src=\"fig-probability/ProbabilityInequalities_001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig-probability/ProbabilityInequalities_002.png, width=500 frac=0.75] The shaded area shows the region between the curves on either side of the Markov Inequality.  <div id=\"fig:ProbabilityInequalities_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:ProbabilityInequalities_002\"></div>\n",
    "\n",
    "<p>The shaded area shows the region between the curves on either side of the Markov Inequality.</p>\n",
    "<img src=\"fig-probability/ProbabilityInequalities_002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "## Chebyshev's Inequality\n",
    "\n",
    "Chebyshev's Inequality drops out directly from the Markov Inequality.  Let\n",
    "$\\mu=\\mathbb{E}(X)$ and $\\sigma^2=\\mathbb{V}(X)$. Then, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(\\vert X-\\mu\\vert \\ge t) \\le \\frac{\\sigma^2}{t^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that if we normalize so that $Z=(X-\\mu)/\\sigma$, we\n",
    "have $\\mathbb{P}(\\vert Z\\vert \\ge k) \\le 1/k^2$. In particular,\n",
    "$\\mathbb{P}(\\vert Z\\vert \\ge 2) \\le 1/4$. We can illustrate this\n",
    "inequality using Sympy statistics module,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sympy\n",
    "import sympy.stats as ss\n",
    "t=sympy.symbols('t',real=True)\n",
    "x=ss.ChiSquared('x',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  To get the left side of the Chebyshev inequality, we\n",
    "have to write this out as the following conditional probability,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = ss.P((x-1) > t,x>1)+ss.P(-(x-1) > t,x<1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We could take the above expression, which is a function of $t$ and\n",
    "attempt to compute the integral, but that would take a very long time (the\n",
    "expression is very long and complicated, which is why we did not print it out\n",
    "above).  In this situation, it's better to use the built-in cumulative density\n",
    "function as in the following (after some rearrangement of the terms),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w=(1-ss.cdf(x)(t+1))+ss.cdf(x)(1-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To plot this, we can evaluated at a variety of `t` values by using\n",
    "the `.subs` substitution method, but it is more convenient to use the\n",
    "`lambdify` method to convert the expression to a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fw=sympy.lambdify(t,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then, we can evaluate this function using something like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[fw(i) for i in [0,1,2,3,4,5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " to produce the following [Figure](#fig:ProbabilityInequalities_003). \n",
    "\n",
    "<!-- dom:FIGURE: [fig-probability/ProbabilityInequalities_003.png,width=500 frac=0.85] The shaded area shows the region between the curves on either side of the Chebyshev Inequality.  <div id=\"fig:ProbabilityInequalities_003\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:ProbabilityInequalities_003\"></div>\n",
    "\n",
    "<p>The shaded area shows the region between the curves on either side of the Chebyshev Inequality.</p>\n",
    "<img src=\"fig-probability/ProbabilityInequalities_003.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "**Programming Tip.**\n",
    "\n",
    "Note that we cannot use vectorized inputs for the `lambdify` function because\n",
    "it contains embedded functions that are only available in Sympy. Otherwise, we\n",
    "could have used `lambdify(t,fw,numpy)` to specify the corresponding functions\n",
    "in Numpy to use for the expression.\n",
    "\n",
    "\n",
    "\n",
    "## Hoeffding's Inequality\n",
    "<div id=\"ch:prob:sec:ineq\"></div>\n",
    "\n",
    "Hoeffding's Inequality is similar, but less loose, than Markov's Inequality.\n",
    "Let $X_1,\\ldots,X_n$ be iid observations such that $\\mathbb{E}(X_i)=\\mu$ and\n",
    "$a\\le X_i \\le b$. Then, for any $\\epsilon>0$, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(\\vert \\overline{X}_n -\\mu\\vert \\ge \\epsilon) \\le 2 \\exp(-2 n\\epsilon^2/(b-a)^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\overline{X}_n = \\tfrac{1}{n}\\sum_i^n X_i$. Note that we\n",
    "further assume that the individual random variables are bounded.\n",
    "\n",
    "**Corollary.** If $X_1,\\ldots,X_n$ are independent with $\\mathbb{P}(a\\le X_i\\le b)=1$\n",
    "and all with $\\mathbb{E}(X_i)=\\mu$. Then, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\vert\\overline{X}_n-\\mu\\vert \\le \\sqrt{\\frac{c}{2 n}\\log \\frac{2}{\\delta}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $c=(b-a)^2$. We will see this inequality again in the machine\n",
    "learning chapter. [Figure](#fig:ProbabilityInequalities_004) shows the Markov\n",
    "and Hoeffding bounds for the case of ten identically and uniformly distributed\n",
    "random variables, $X_i \\sim \\mathcal{U}[0,1]$.  The solid line shows\n",
    "$\\mathbb{P}(\\vert \\overline{X}_n - 1/2 \\vert > \\epsilon)$.  Note that the\n",
    "Hoeffding Inequality is tighter than the Markov Inequality and that both of\n",
    "them merge when $\\epsilon$ gets big enough.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-probability/ProbabilityInequalities_004.png,width=500 frac=0.75] This shows the Markov and Hoeffding bounds for the case of ten identically and uniformly distributed random variables.  <div id=\"fig:ProbabilityInequalities_004\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:ProbabilityInequalities_004\"></div>\n",
    "\n",
    "<p>This shows the Markov and Hoeffding bounds for the case of ten identically and uniformly distributed random variables.</p>\n",
    "<img src=\"fig-probability/ProbabilityInequalities_004.png\" width=500>\n",
    "\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.displayhook= old_displayhook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of Hoeffding's Inequality\n",
    "\n",
    "We will need the following lemma to prove Hoeffding's inequality.\n",
    "\n",
    "**Lemma** Let $X$ be a random variable with $\\mathbb{E}(X)=0$ and\n",
    "$a\\le X\\le b$. Then, for any $s>0$, we have the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{E}(e^{s X}) \\le e^{s^2(b-a)^2/8}\n",
    "\\label{_auto1} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $X$ is contained in the closed interval $[a,b]$, we can write it as a convex \n",
    "combination of the endpoints of the interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "X = \\upalpha_1 a + \\upalpha_2 b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\upalpha_1+\\upalpha_2=1$. Solving for the $\\upalpha_i$ terms, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\upalpha_1 = & \\frac{x-a}{b-a} \\\\\n",
    "\\upalpha_2 = & \\frac{b-x}{b-a}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From Jensen's inequality,  for a convex functions $f$, we know that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f\\left(\\sum \\upalpha_i x_i\\right) \\le \\sum \\upalpha_i f(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Given the convexity of $e^X$, we therefore have,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "e^{s X} \\le \\upalpha_1 e^{s a} + \\upalpha_2 e^ {s b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " With $\\mathbb{E}(X)=0$, we can write the expectation of both sides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(e^{s X}) \\le \\mathbb{E}(\\upalpha_1) e^{s a} +\\mathbb{E}(\\upalpha_2) e^{s b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " with $\\mathbb{E}(\\upalpha_1)=\\frac{b}{b-a}$ and\n",
    "$\\mathbb{E}(\\upalpha_2)=\\frac{-a}{b-a}$. Thus, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(e^{s X}) \\le \\frac{b}{b-a} e^{s a} -\\frac{a}{b-a} e^{s b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Using $p:=\\frac{-a}{b-a}$, we can rewrite the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{b}{b-a} e^{s a} -\\frac{a}{b-a} e^{s b} = (1-p)e^{s a} + p e^{s b} =: e^{\\phi(u)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\phi(u)=-p u + \\log(1-p+p e^{u})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and $u=s(b-a)$. Note that $\\phi(0)=\\phi'(0)=0$. Also, $\\phi''(0) = p(1-p)\\le 1/4$. Thus,\n",
    "the Taylor expansion of $\\phi(u)\\approx \\frac{u^2}{2}\\phi''(t) \\le \\frac{u^2}{8}$ for\n",
    "$t\\in [0,u] \\blacksquare$.\n",
    "\n",
    "To prove Hoeffding's inequality, we start with Markov's inequality,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(X\\ge\\epsilon)\\le \\frac{\\mathbb{E}(X)}{\\epsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then, given $s>0$, we have the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(X\\ge\\epsilon)=\\mathbb{P}(e^{s X} \\ge e^{s\\epsilon}) \\le \\frac{\\mathbb{E}(e^{s X})}{e^{s \\epsilon}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can write the one-sided Hoeffding inequality as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(\\overline{X}_n -\\mu\\ge\\epsilon) & \\le e^{-s\\epsilon}\\mathbb{E}(\\exp(\\frac{s}{n}\\sum_{i=1}^n (X_i-\\mathbb{E}(X_i)))) \\\\\n",
    "    & = e^{-s\\epsilon}\\prod_{i=1}^n\\mathbb{E}(e^{ \\frac{s}{n} (X_i-\\mathbb{E}(X_i)) }) \\\\\n",
    "    & \\le e^{-s\\epsilon}\\prod_{i=1}^n e^{\\frac{s^2}{n^2}(b-a)^2/8 } \\\\\n",
    "    & = e^{-s\\epsilon} e^{\\frac{s^2}{n}(b-a)^2/8}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, we want to pick $s>0$ to minimize this upper bound. Then, with\n",
    "$s=\\frac{4 n\\epsilon}{(b-a)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(\\overline{X}_n-\\mu\\ge\\epsilon)\\le e^{-\\frac{2 n\\epsilon^2}{(b-a)^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The other side of the inequality follows similarly to obtain Hoeffding's \n",
    "inequality $\\blacksquare$.\n",
    "\n",
    "## Jensen's Inequality\n",
    "<div id=\"ch:prob:sec:ineq\"></div>\n",
    "\n",
    "If $f$ is a convex function with random variable $v$, then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(f(v))\\ge f(\\mathbb{E}(v))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The proof of this is straightforward. Define $L(v) = a v +b $ with\n",
    "$a,b\\in \\mathbb{R}$. Choose $a$ and $b$ so that\n",
    "$L(\\mathbb{E}(v))=f(\\mathbb{E}(v))$ which makes $L$ tangent to $f$ at\n",
    "$\\mathbb{E}(v)$. By the convexity of $f$, we have $f(v)\\ge L(v)$. We can take\n",
    "the expectation of both sides of this,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(f(v)) \\ge & \\mathbb{E}(L(v)) \\\\\n",
    "                  =  & \\mathbb{E}(a v+b)  \\\\\n",
    "                  =  & a\\mathbb{E}(v)+b  \\\\\n",
    "                  =  & L(\\mathbb{E}(v))  \\\\\n",
    "                  =  & f(\\mathbb{E}(v)) \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " equality holds when $f$ is linear. For a concave function $f$, the\n",
    "sense of the inequality is reversed."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
