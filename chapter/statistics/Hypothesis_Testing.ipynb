{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is sometimes very difficult to unequivocally attribute outcomes to causal\n",
    "factors. For example, did your  experiment generate the outcome you were hoping\n",
    "for or not?  Maybe something did happen, but the effect is not pronounced\n",
    "enough to separate it from inescapable measurement errors or other\n",
    "factors in the ambient environment?  Hypothesis testing is a powerful\n",
    "statistical method to address these questions.  Let's begin by again\n",
    "considering our coin-tossing experiment with unknown parameter $p$.  Recall\n",
    "that the individual coin-flips are Bernoulli distributed.  The first step is\n",
    "to establish separate hypotheses.  First, $H_0$ is the so-called null\n",
    "hypothesis. In our case this can be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H_0 \\colon \\theta <  \\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and the alternative hypothesis is then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H_1 \\colon \\theta \\geq  \\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " With this set up, the question now boils down to figuring out which\n",
    "hypothesis the data is most consistent with.  To choose between these, we need\n",
    "a statistical test that is a function, $G$, of the sample set\n",
    "$\\mathbf{X}_n=\\left\\{ X_i \\right\\}_n $ into the real line, where $X_i$ is the\n",
    "heads or tails outcome ($X_i \\in \\lbrace 0,1 \\rbrace$). In other words, we\n",
    "compute $G(\\mathbf{X}_n)$ and check if it exceeds a threshold $c$. If not, then\n",
    "we declare $H_0$ (otherwise, declare $H_1$). Notationally, this is the\n",
    "following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    " G(\\mathbf{X}_n) < c & \\Rightarrow H_0   \\\\\\\n",
    " G(\\mathbf{X}_n) \\geq c & \\Rightarrow H_1\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In summary, we have the observed data $\\mathbf{X}_n$ and a function\n",
    "$G$ that maps that data onto the real line. Then, using the\n",
    "constant $c$ as a threshold, the inequality effectively divides the real line\n",
    "into two parts, one corresponding to each of the hypotheses.\n",
    "\n",
    "Whatever this test $G$ is, it will make mistakes of two types --- false\n",
    "negatives and false positives. The false positives arise from the case where we\n",
    "declare $H_0$ when the test says we should declare $H_1$.  This is\n",
    "summarized in the Table [1](#tbl:decision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"tbl:decision\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{table}\n",
    "\\footnotesize\n",
    "\\centering\n",
    "\\begin{tabular}{l|p{1.3in}|p{1.3in}}\n",
    "\\multicolumn{1}{c}{ } & \\multicolumn{1}{c}{Declare $H_0$ } & \\multicolumn{1}{c}{ Declare $H_1$ } \\\\\n",
    "\\hline\n",
    "$H_0\\:$ True & Correct                        & False positive (Type I error) \\\\\n",
    "\\hline\n",
    "$H_1\\:$ True & False negative (Type II error) & Correct (true-detect)      \\\\\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\caption{Truth table for hypotheses testing.}\n",
    "\\label{tbl:decision} \\tag{1}\n",
    "\\end{table}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For this example, here are the false positives (aka false alarms):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P_{FA} = \\mathbb{P}\\left( G(\\mathbf{X}_n) > c \\mid \\theta \\leq \\frac{1}{2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Or, equivalently,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P_{FA} = \\mathbb{P}\\left( G(\\mathbf{X}_n) > c \\mid H_0 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Likewise, the other error is a false negative, which we can write\n",
    "analogously as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P_{FN} = \\mathbb{P}\\left( G(\\mathbf{X}_n) < c \\vert H_1\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " By choosing some acceptable values for either of these errors,\n",
    "we can solve for the other one.  The practice is usually to pick a value of\n",
    "$P_{FA}$ and then find the corresponding value of $P_{FN}$.  Note that it is\n",
    "traditional in engineering to speak about *detection probability*, which is\n",
    "defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P_{D} = 1- P_{FN} = \\mathbb{P}\\left( G(\\mathbf{X}_n) > c \\mid H_1\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In other words, this is the probability of declaring $H_1$ when the\n",
    "test exceeds the threshold. This is otherwise known as the *probability of a\n",
    "true detection* or *true-detect*.\n",
    "\n",
    "## Back to the Coin Flipping Example\n",
    "\n",
    "In our previous maximum likelihood discussion, we wanted to derive an\n",
    "estimator for the *value* of the probability of heads for the coin\n",
    "flipping experiment. For hypthesis testing, we want to ask a softer\n",
    "question: is the probability of heads greater or less than $\\nicefrac{1}{2}$? As we\n",
    "just established, this leads to the two hypotheses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H_0 \\colon \\theta < \\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " versus,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H_1 \\colon \\theta > \\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's assume we have five observations.  Now we need the $G$ function\n",
    "and a threshold $c$ to help pick between the two hypotheses. Let's count the\n",
    "number of heads observed in five observations as our\n",
    "criterion. Thus, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "G(\\mathbf{X}_5) := \\sum_{i=1}^5 X_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and, suppose further that we pick $H_1$  only if exactly five out of\n",
    "five observations are heads. We'll call this the *all-heads* test.\n",
    "\n",
    "Now, because all of the $X_i$ are random variables, so is $G$ and we must\n",
    "find the corresponding probability mass function for $G$.  Assuming the\n",
    "individual coin tosses are independent, the probability of five heads is $\\theta^5$.\n",
    "This means that the probability of rejecting the $H_0$ hypothesis (and choosing\n",
    "$H_1$, because there are only two choices here) based on the unknown underlying\n",
    "probability is $\\theta^5$. In the parlance, this is known and the *power function*\n",
    "as in denoted by $\\beta$ as in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta(\\theta) = \\theta^5\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's get a quick plot this in [Figure](#fig:Hypothesis_testing_001).\n",
    "\n",
    "<!-- @@@CODE src-statistics/Hypothesis_Testing.py fromto: import numpy as np@plt.savefig -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import subplots\n",
    "import numpy as np\n",
    "fig,ax=subplots()\n",
    "fig.set_size_inches((6,3))\n",
    "xi = np.linspace(0,1,50)\n",
    "_=ax.plot(xi, (xi)**5,'-k',label='all heads')\n",
    "_=ax.set_xlabel(r'$\\theta$',fontsize=22)\n",
    "_=ax.plot(0.5,(0.5)**5,'ko')\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-statistics/Hypothesis_Testing_001.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-statistics/Hypothesis_Testing_001.png, width=500 frac=0.85] Power function for the all-heads test. The dark circle indicates the value of the function indicating $\\alpha$. <div id=\"fig:Hypothesis_testing_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Hypothesis_testing_001\"></div>\n",
    "\n",
    "<p>Power function for the all-heads test. The dark circle indicates the value of the function indicating $\\alpha$.</p>\n",
    "<img src=\"fig-statistics/Hypothesis_Testing_001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    " Now, we have the following false alarm probability,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P_{FA} = \\mathbb{P}( G(\\mathbf{X}_n)= 5 \\vert H_0) =\\mathbb{P}( \\theta^5 \\vert H_0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Notice that this is a function of $\\theta$, which means there are\n",
    "many false alarm probability values that correspond to this test. To be on the\n",
    "conservative side, we'll pick the supremum (i.e., maximum) of this function,\n",
    "which is known as the *size* of the test, traditionally denoted by $\\alpha$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha = \\sup_{\\theta \\in \\Theta_0} \\beta(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " with domain $\\Theta_0 = \\lbrace \\theta < 1/2 \\rbrace$ which in our case is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha = \\sup_{\\theta < \\frac{1}{2}} \\theta^5 = \\left(\\frac{1}{2}\\right)^5 = 0.03125\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Likewise, for the detection probability,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}_{D}(\\theta) = \\mathbb{P}( \\theta^5 \\vert H_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which is again a function of the parameter $\\theta$. The problem with\n",
    "this test is that the $P_{D}$ is pretty low for most of the domain of\n",
    "$\\theta$. For instance, values in the nineties for $P_{D}$\n",
    "only happen when $\\theta > 0.98$. In other words, if the coin produces\n",
    "heads 98 times out of 100, then we can detect $H_1$ reliably. Ideally, we want\n",
    "a test that is zero for the domain corresponding to $H_0$ (i.e., $\\Theta_0$) and\n",
    "equal to one otherwise. Unfortunately, even if we increase the length of the\n",
    "observed sequence, we cannot escape this effect with this test. You can try\n",
    "plotting $\\theta^n$ for larger and larger values of $n$ to see this.\n",
    "\n",
    "### Majority Vote Test\n",
    "\n",
    "Due to the problems with the detection probability in the all-heads test, maybe\n",
    "we can think of another test that will have the performance we want? Suppose we\n",
    "reject $H_0$ if the majority of the observations are heads. Then, using the\n",
    "same reasoning as above, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta(\\theta) = \\sum_{k=3}^5 \\binom{5}{k} \\theta^k(1-\\theta)^{5-k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Figure](#fig:Hypothesis_testing_002) shows the power function\n",
    "for both the majority vote and the all-heads tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "fig.set_size_inches((6,3))\n",
    "from sympy.abc import theta,k # get some variable symbols\n",
    "import sympy as S\n",
    "xi = np.linspace(0,1,50)\n",
    "expr=S.Sum(S.binomial(5,k)*theta**(k)*(1-theta)**(5-k),(k,3,5)).doit()\n",
    "_=ax.plot(xi, (xi)**5,'-k',label='all heads')\n",
    "_=ax.plot(xi, S.lambdify(theta,expr)(xi),'--k',label='majority vote')\n",
    "_=ax.plot(0.5, (0.5)**5,'ko')\n",
    "_=ax.plot(0.5, S.lambdify(theta,expr)(0.5),'ko')\n",
    "_=ax.set_xlabel(r'$\\theta$',fontsize=22)\n",
    "_=ax.legend(loc=0)\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-statistics/Hypothesis_Testing_002.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-statistics/Hypothesis_Testing_002.png, width=500 frac=0.85]  Compares the power function for the all-heads test with that of the majority-vote test. <div id=\"fig:Hypothesis_testing_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Hypothesis_testing_002\"></div>\n",
    "\n",
    "<p>Compares the power function for the all-heads test with that of the majority-vote test.</p>\n",
    "<img src=\"fig-statistics/Hypothesis_Testing_002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    " In this case, the new test has *size*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha = \\sup_{\\theta < \\frac{1}{2}} \\theta^{5} + 5 \\theta^{4} \\left(- \\theta + 1\\right) + 10 \\theta^{3} \\left(- \\theta + 1\\right)^{2} = \\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As before we only get to upwards of 90% for detection\n",
    "probability only when the underlying parameter $\\theta > 0.75$. \n",
    "Let's see what happens when we consider more than five samples. For\n",
    "example, let's suppose that we have $n=100$ samples and we want to\n",
    "vary the threshold for the majority vote test. For example, let's have\n",
    "a new test where we declare $H_1$ when $k=60$ out of the 100 trials\n",
    "turns out to be heads. What is the $\\beta$ function in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta(\\theta) = \\sum_{k=60}^{100} \\binom{100}{k} \\theta^k(1-\\theta)^{100-k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is too complicated to write by hand, but the statistics module\n",
    "in Sympy has all the tools we need to compute this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sympy.stats import P, Binomial\n",
    "theta = S.symbols('theta',real=True)\n",
    "X = Binomial('x',100,theta)\n",
    "beta_function = P(X>60)\n",
    "print (beta_function.subs(theta,0.5)) # alpha\n",
    "print (beta_function.subs(theta,0.70))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " These results are much better than before because the $\\beta$\n",
    "function is much steeper. If we declare $H_1$ when we observe 60 out of 100\n",
    "trials are heads, then we wrongly declare heads approximately 1.8% of the\n",
    "time.  Otherwise, if it happens that the true value for $p>0.7$, we will\n",
    "conclude correctly approximately 97% of the time. A quick simulation can sanity\n",
    "check these results as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "rv=stats.bernoulli(0.5) # true p = 0.5\n",
    "# number of false alarms ~ 0.018\n",
    "print (sum(rv.rvs((1000,100)).sum(axis=1)>60)/1000.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The above code is pretty dense so let's unpack it. In the first line, we use the `scipy.stats` module to define the\n",
    "Bernoulli random variable for the coin flip. Then, we use the `rvs` method of\n",
    "the variable to generate 1000 trials of the experiment where each trial\n",
    "consists of 100 coin flips. This generates a $1000 \\times 100$ matrix where the\n",
    "rows are the individual trials and the columns are the outcomes of each\n",
    "respective set of 100 coin flips. The `sum(axis=1)` part computes the sum across the\n",
    "columns. Because the values of the embedded matrix are only `1` or `0` this\n",
    "gives us the count of flips that are heads per row. The next `>60` part\n",
    "computes the boolean 1000-long vector of values that are bigger than 60. The\n",
    "final `sum` adds these up. Again, because the entries in the array are `True`\n",
    "or `False` the `sum` computes the count of times the number of heads has\n",
    "exceeded 60 per 100 coin flips in each of 1000 trials. Then, dividing this\n",
    "number by 1000 gives a quick approximation of false alarm probability we\n",
    "computed above for this case where the true value of $p=0.5$.\n",
    "\n",
    "## Receiver Operating Characteristic\n",
    "\n",
    "Because the majority vote test is a binary test, we can compute the *Receiver\n",
    "Operating Characteristic* (ROC) which is the graph of the $(P_{FA},\n",
    "P_D)$. The term comes from radar systems but is a very general method for\n",
    "consolidating all of these issues into a single graph. Let's consider a typical\n",
    "signal processing example with two hypotheses. In $H_0$, there is noise but no\n",
    "signal present at the receiver,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H_0 \\colon X =  \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ represents additive\n",
    "noise. In the alternative hypothesis, there is a  deterministic signal at the receiver,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H_1 \\colon X = \\mu + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Again, the problem is to choose between these two hypotheses. For\n",
    "$H_0$, we have $X \\sim \\mathcal{N}(0,\\sigma^2)$ and for $H_1$, we have $ X \\sim\n",
    "\\mathcal{N}(\\mu,\\sigma^2)$.  Recall that we only observe values for $x$ and\n",
    "must pick either $H_0$ or $H_1$ from these observations. Thus, we need a\n",
    "threshold, $c$, to compare $x$ against in order to distinguish the two\n",
    "hypotheses. [Figure](#fig:Hypothesis_testing_003) shows the probability density\n",
    "functions under each of the hypotheses. The dark vertical line is the threshold\n",
    "$c$. The gray shaded area is the probability of detection, $P_D$ and the shaded\n",
    "area is the probability of false alarm, $P_{FA}$. The test evaluates every\n",
    "observation of $x$ and concludes $H_0$ if $x<c$ and $H_1$ otherwise.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-statistics/Hypothesis_Testing_003.png, width=500 frac=0.85] The two density functions for the $H_0$ and $H_1$ hypotheses. The shaded gray area is the detection probability and the shaded dark gray area is the probability of false alarm. The vertical line is the decision threshold. <div id=\"fig:Hypothesis_testing_003\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Hypothesis_testing_003\"></div>\n",
    "\n",
    "<p>The two density functions for the $H_0$ and $H_1$ hypotheses. The shaded gray area is the detection probability and the shaded dark gray area is the probability of false alarm. The vertical line is the decision threshold.</p>\n",
    "<img src=\"fig-statistics/Hypothesis_Testing_003.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "**Programming Tip.**\n",
    "\n",
    "The shading shown in [Figure](#fig:Hypothesis_testing_003) comes from\n",
    "Matplotlib's `fill_between` function. This function has a `where` keyword\n",
    "argument to specify which part of the plot to apply shading with specified\n",
    "`color` keyword argument. Note there is also a `fill_betweenx` function that\n",
    "fills horizontally.  The `text` function can place formatted\n",
    "text anywhere in the plot and can utilize basic \\LaTeX{} formatting.\n",
    "\n",
    "\n",
    "\n",
    "As we slide the threshold left and right along the horizontal axis, we naturally change the corresponding areas under\n",
    "each of the curves shown in [Figure](#fig:Hypothesis_testing_003) and thereby\n",
    "change the values of $P_D$ and $P_{FA}$. The contour that emerges from sweeping\n",
    "the threshold this way is the ROC as shown in [Figure](#fig:Hypothesis_testing_004). This figure also shows the diagonal line which\n",
    "corresponds to making decisions based on the flip of a fair coin. Any\n",
    "meaningful test must do better than coin flipping so the more the ROC bows up\n",
    "to the top left corner of the graph, the better. Sometimes ROCs are quantified\n",
    "into a single number called the *area under the curve* (AUC), which varies from\n",
    "0.5 to 1.0 as shown. In our example, what separates the two probability density\n",
    "functions is the value of $\\mu$. In a real situation, this would be determined\n",
    "by signal processing methods that include many complicated trade-offs. The key\n",
    "idea is that whatever those trade-offs are, the test itself boils down to the\n",
    "separation between these two density functions --- good tests separate the two\n",
    "density functions and bad tests do not. Indeed, when there is no separation, we\n",
    "arrive at the diagonal-line coin-flipping situation we just discussed.\n",
    "\n",
    "What values for $P_D$ and $P_{FA}$ are considered *acceptable* depends on the\n",
    "application. For example, suppose you are testing for a fatal disease. It could\n",
    "be that you are willing to except a relatively high $P_{FA}$ value if that\n",
    "corresponds to a good $P_D$ because the test is relatively cheap to administer\n",
    "compared to the alternative of missing a detection. On the other hand,\n",
    "may be a false alarm triggers an expensive response, so that minimizing\n",
    "these alarms is more important than potentially missing a detection. These\n",
    "trade-offs can only be determined by the application and design factors.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-statistics/Hypothesis_Testing_004.png, width=500 frac=0.65] The Receiver Operating Characteristic (ROC) corresponding to [Figure](#fig:Hypothesis_testing_003). <div id=\"fig:Hypothesis_testing_004\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Hypothesis_testing_004\"></div>\n",
    "\n",
    "<p>The Receiver Operating Characteristic (ROC) corresponding to [Figure](#fig:Hypothesis_testing_003).</p>\n",
    "<img src=\"fig-statistics/Hypothesis_Testing_004.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "## P-Values\n",
    "\n",
    "There are a lot of moving parts in hypothesis testing. What we need\n",
    "is a way to consolidate the findings. The idea is that we want to find\n",
    "the minimum level at which the test rejects $H_0$. Thus, the p-value\n",
    "is the probability, under $H_0$, that the test-statistic is at least\n",
    "as extreme as what was actually observed.  Informally, this means\n",
    "that smaller values imply that $H_0$ should be rejected, although\n",
    "this doesn't mean that large values imply that $H_0$ should be\n",
    "retained. This is because a large p-value can arise from either $H_0$\n",
    "being true or the test having low statistical power.\n",
    "\n",
    "If $H_0$ is true, the p-value is uniformly distributed in the interval $(0,1)$.\n",
    "If $H_1$ is true, the distribution of the p-value will concentrate closer to\n",
    "zero. For continuous distributions, this can be proven rigorously and implies\n",
    "that if we reject $H_0$ when the corresponding p-value is less than $\\alpha$,\n",
    "then the probability of a false alarm is $\\alpha$. Perhaps it helps to\n",
    "formalize this a bit before computing it. Suppose $\\tau(X)$ is a test\n",
    "statistic that rejects $H_0$ as it gets bigger. Then, for each sample $x$,\n",
    "corresponding to the data we actually have on-hand, we define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x) = \\sup_{\\theta \\in \\Theta_0} \\mathbb{P}_{\\theta}(\\tau(X) > \\tau(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This equation states that the supremum (i.e., maximum)\n",
    "probability that the test statistic, $\\tau(X)$, exceeds the value for\n",
    "the test statistic on this particular data ($\\tau(x)$) over the\n",
    "domain $\\Theta_0$ is defined as the p-value. Thus, this embodies a\n",
    "worst-case scenario over all values of $\\theta$.\n",
    "\n",
    "Here's one way to think about this. Suppose you rejected $H_0$, and someone\n",
    "says that you just got *lucky* and somehow just drew data that happened to\n",
    "correspond to a rejection of $H_0$.   What p-values provide is a way to address\n",
    "this by capturing the odds of just a favorable data-draw.  Thus, suppose that\n",
    "your p-value is 0.05. Then, what you are showing is that the odds of just\n",
    "drawing that data sample, given $H_0$ is in force, is just 5%. This means that\n",
    "there's a 5% chance that you somehow lucked out and got a favorable draw of\n",
    "data.\n",
    "\n",
    "Let's make this concrete with an example. Given, the majority-vote rule above,\n",
    "suppose we actually do observe three of five heads. Given the $H_0$, the\n",
    "probability of observing this event is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x) =\\sup_{\\theta \\in \\Theta_0} \\sum_{k=3}^5\\binom{5}{k} \\theta^k(1-\\theta)^{5-k} = \\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For the all-heads test, the corresponding computation is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x) =\\sup_{\\theta \\in \\Theta_0} \\theta^5 = \\frac{1}{2^5} = 0.03125\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From just looking at these p-values, you might get the feeling that the second\n",
    "test is better, but we still have the same detection probability issues we\n",
    "discussed above; so, p-values help in summarizing some aspects of our\n",
    "hypothesis testing, but they do *not* summarize all the salient aspects of the\n",
    "*entire* situation.\n",
    "\n",
    "## Test Statistics\n",
    "\n",
    "As we have seen, it is difficult to derive good test statistics for hypothesis\n",
    "testing without a systematic process.  The Neyman-Pearson Test is derived from\n",
    "fixing a false-alarm value ($\\alpha$) and then maximizing  the detection\n",
    "probability. This results in the Neyman-Pearson Test,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\mathbf{x}) = \\frac{f_{X|H_1}(\\mathbf{x})}{f_{X|H_0}(\\mathbf{x})} \\stackrel[H_0]{H_1}{\\gtrless} \\gamma\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $L$ is the likelihood ratio and where the threshold\n",
    "$\\gamma$ is chosen such that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\int_{x:L(\\mathbf{x})>\\gamma} f_{X|H_0}(\\mathbf{x}) d\\mathbf{x}=\\alpha\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The Neyman-Pearson Test is one of a family of tests that use\n",
    "the likelihood ratio.\n",
    "\n",
    "**Example.** Suppose we have a receiver and we want to distinguish\n",
    "whether just noise ($H_0$) or signal pluse noise ($H_1$) is received.\n",
    "For the noise-only case, we have  $x\\sim \\mathcal{N}(0,1)$ and for the\n",
    "signal pluse noise case we have $x\\sim \\mathcal{N}(1,1)$.  In other\n",
    "words, the mean of the distribution shifts in the presence of the\n",
    "signal. This is a very common problem in signal processing and\n",
    "communications. The Neyman-Pearson Test then boils down to the\n",
    "following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(x)= e^{-\\frac{1}{2}+x}\\stackrel[H_0]{H_1}{\\gtrless}\\gamma\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we have to find the threshold $\\gamma$ that solves the\n",
    "maximization problem that characterizes the Neyman-Pearson Test. Taking\n",
    "the natural  logarithm and re-arranging gives,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x\\stackrel[H_0]{H_1}{\\gtrless} \\frac{1}{2}+\\log\\gamma\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The next step is find $\\gamma$ corresponding to the desired\n",
    "$\\alpha$ by computing it from the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\int_{1/2+\\log\\gamma}^{\\infty} f_{X|H_0}(x)dx = \\alpha\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For example, taking $\\alpha=1/100$, gives\n",
    "$\\gamma\\approx 6.21$. To summarize the test in this case, we have,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x\\stackrel[H_0]{H_1}{\\gtrless} 2.32\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Thus, if we measure $X$ and see that its value\n",
    "exceeds the threshold above, we declare $H_1$ and otherwise\n",
    "declare $H_0$. The following code shows how to\n",
    "solve this example using Sympy and Scipy. First, we\n",
    "set up the likelihood ratio,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sympy as S\n",
    "from sympy import stats\n",
    "s = stats.Normal('s',1,1) # signal+noise\n",
    "n = stats.Normal('n',0,1) # noise\n",
    "x = S.symbols('x',real=True)\n",
    "L = stats.density(s)(x)/stats.density(n)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next, to find the $\\gamma$ value,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = S.symbols('g',positive=True) # define gamma\n",
    "v=S.integrate(stats.density(n)(x),\n",
    "             (x,S.Rational(1,2)+S.log(g),S.oo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "Providing additional information regarding the Sympy variable by using the\n",
    "keyword argument `positive=True` helps the internal simplification algorithms\n",
    "work faster and better. This is especially useful when dealing with complicated\n",
    "integrals that involve special functions. Furthermore, note that we used the\n",
    "`Rational` function to define the `1/2` fraction, which is another way of\n",
    "providing hints to Sympy. Otherwise, it's possible that the floating-point\n",
    "representation of the fraction could  disguise the simple fraction and\n",
    "thereby miss internal simplification opportunities.\n",
    "\n",
    "\n",
    "\n",
    " We want to solve for `g` in the above expression. Sympy has some\n",
    "built-in numerical solvers as in the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (S.nsolve(v-0.01,3.0)) # approx 6.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that in this situation it is better to use the numerical\n",
    "solvers because Sympy `solve` may grind along for a long time to\n",
    "resolve this.\n",
    "\n",
    "### Generalized Likelihood Ratio Test\n",
    "\n",
    "The likelihood ratio test can be generalized using the following statistic,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Lambda(\\mathbf{x})= \\frac{\\sup_{\\theta\\in\\Theta_0} L(\\theta)}{\\sup_{\\theta\\in\\Theta} L(\\theta)}=\\frac{L(\\hat{\\theta}_0)}{L(\\hat{\\theta})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\hat{\\theta}_0$ maximizes $L(\\theta)$ subject to\n",
    "$\\theta\\in\\Theta_0$ and $\\hat{\\theta}$ is the maximum likelihood estimator.\n",
    "The intuition behind this generalization of the Likelihood Ratio Test is that\n",
    "the denomimator is the usual maximum likelihood estimator and the numerator is\n",
    "the maximum likelihood estimator, but over a restricted domain ($\\Theta_0$).\n",
    "This means that the ratio is always less than unity because the maximum\n",
    "likelihood estimator over the entire space will always be at least as maximal\n",
    "as that over the more restricted space. When this $\\Lambda$ ratio gets small\n",
    "enough, it means that the maximum likelihood estimator over the entire domain\n",
    "($\\Theta$) is larger which means that it is safe to reject the null hypothesis\n",
    "$H_0$.  The tricky part is that the statistical distribution of $\\Lambda$ is\n",
    "usually eye-wateringly difficult.  Fortunately, Wilks Theorem says that with\n",
    "sufficiently large $n$, the distribution of $-2\\log\\Lambda$ is approximately\n",
    "chi-square with $r-r_0$ degrees of freedom, where $r$ is the number of free\n",
    "parameters for $\\Theta$ and $r_0$ is the number of free parameters in\n",
    "$\\Theta_0$.  With this result, if we want an approximate test at level\n",
    "$\\alpha$, we can reject $H_0$ when $-2\\log\\Lambda \\ge \\chi^2_{r-r_0}(\\alpha)$\n",
    "where $\\chi^2_{r-r_0}(\\alpha)$ denotes the $1-\\alpha$ quantile of the\n",
    "$\\chi^2_{r-r_0}$ chi-square distribution.  However, the problem with this\n",
    "result is that there is no definite way of knowing how big $n$ should be. The\n",
    "advantage of this generalized likelihood ratio test is that it \n",
    "can test multiple hypotheses simultaneously, as illustrated\n",
    "in the following example.\n",
    "\n",
    "**Example.** Let's return to our coin-flipping example, except now we have\n",
    "three different coins. The likelihood function is then,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(p_1,p_2,p_3) = \\texttt{binom}(k_1;n_1,p_1)\\texttt{binom}(k_2;n_2,p_2)\\texttt{binom}(k_3;n_3,p_3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\texttt{binom}$ is the binomial distribution with \n",
    "the given parameters. For example,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\texttt{binom}(k;n,p) =\\sum_{k=0}^n \\binom{n}{k} p^k(1-p)^{n-k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The null hypothesis is that all three coins have the\n",
    "same probability of heads, $H_0:p=p_1=p_2=p_3$. The alternative hypothesis is\n",
    "that at least one of these probabilites is different. Let's consider the\n",
    "numerator of the $\\Lambda$ first, which will give us the maximum likelihood\n",
    "estimator of $p$. Because the null hypothesis is that all the $p$ values are\n",
    "equal, we can just treat this as one big binomial distribution with\n",
    "$n=n_1+n_2+n_3$ and $k=k_1+k_2+k_3$ is the total number of heads observed for\n",
    "any coin.  Thus, under the null hypothesis, the distribution of $k$ is binomial\n",
    "with parameters $n$ and $p$. Now, what is the maximum likelihood estimator for\n",
    "this distribution? We have worked this problem before and have the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{p}_0= \\frac{k}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In other words, the maximum likelihood estimator under the null\n",
    "hypothesis is the proportion of ones observed in the sequence of $n$ trials\n",
    "total. Now, we have to substitute this in for the likelihood under the null\n",
    "hypothesis to finish the numerator of $\\Lambda$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\hat{p}_0,\\hat{p}_0,\\hat{p}_0) = \\texttt{binom}(k_1;n_1,\\hat{p}_0)\\texttt{binom}(k_2;n_2,\\hat{p}_0)\\texttt{binom}(k_3;n_3,\\hat{p}_0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the denomimator of $\\Lambda$, which represents the case of maximizing over\n",
    "the entire space, the maximum likelihood estimator for each separate binomial\n",
    "distribution is likewise,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{p}_i= \\frac{k_i}{n_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which makes the likelihood in the denominator the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\hat{p}_1,\\hat{p}_2,\\hat{p}_3) = \\texttt{binom}(k_1;n_1,\\hat{p}_1)\\texttt{binom}(k_2;n_2,\\hat{p}_2)\\texttt{binom}(k_3;n_3,\\hat{p}_3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " for each of the $i\\in \\lbrace 1,2,3 \\rbrace$ binomial distributions. Then, the\n",
    "$\\Lambda$ statistic is then the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Lambda(k_1,k_2,k_3) = \\frac{L(\\hat{p}_0,\\hat{p}_0,\\hat{p}_0)}{L(\\hat{p}_1,\\hat{p}_2,\\hat{p}_3)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Wilks theorems states that $-2\\log\\Lambda$  is chi-square\n",
    "distributed. We can compute this example with the statistics tools in  Sympy and\n",
    "Scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binom, chi2\n",
    "import numpy as np\n",
    "# some sample parameters\n",
    "p0,p1,p2 = 0.3,0.4,0.5\n",
    "n0,n1,n2 = 50,180,200\n",
    "brvs= [ binom(i,j) for i,j in zip((n0,n1,n2),(p0,p1,p2))]\n",
    "def gen_sample(n=1):\n",
    "    'generate samples from separate binomial distributions'\n",
    "    if n==1:\n",
    "        return [i.rvs() for i in brvs]\n",
    "    else:\n",
    "        return [gen_sample() for k in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "Note the recursion in the definition of the `gen_sample` function where a\n",
    "conditional clause of the function calls itself. This is a quick way to reusing\n",
    "code and generating vectorized output. Using `np.vectorize` is another way, but\n",
    "the code is simple enough in this case to use the conditional clause. In\n",
    "Python, it is generally bad for performance to have code with nested recursion\n",
    "because of how the stack frames are managed.  However,  here we are only\n",
    "recursing once so this is not an issue.\n",
    "\n",
    "\n",
    "\n",
    " Next, we compute the logarithm of the numerator of the $\\Lambda$\n",
    "statistic,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k0,k1,k2 = gen_sample()\n",
    "print (k0,k1,k2)\n",
    "pH0 = sum((k0,k1,k2))/sum((n0,n1,n2))\n",
    "numer = np.sum([np.log(binom(ni,pH0).pmf(ki)) \n",
    "                  for ni,ki in \n",
    "                      zip((n0,n1,n2),(k0,k1,k2))])\n",
    "print (numer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that we used the null hypothesis estimate for the $\\hat{p}_0$.\n",
    "Likewise, for the logarithm of the denominator we have the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "denom = np.sum([np.log(binom(ni,pi).pmf(ki)) \n",
    "                  for ni,ki,pi in \n",
    "                     zip((n0,n1,n2),(k0,k1,k2),(p0,p1,p2))])\n",
    "print (denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, we can compute the logarithm of the $\\Lambda$ statistic as\n",
    "follows and see what the corresponding value is according to Wilks theorem,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chsq=chi2(2)\n",
    "logLambda =-2*(numer-denom)\n",
    "print (logLambda)\n",
    "print (1- chsq.cdf(logLambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Because the value reported above is less than the 5% significance\n",
    "level, we reject the null  hypothesis that all the coins have the same\n",
    "probability of heads. Note that there are two degrees of freedom because the\n",
    "difference in the number of parameters between the null hypothesis ($p$) and\n",
    "the alternative ($p_1,p_2,p_3$) is two. We can build a quick Monte\n",
    "Carlo simulation to check the probability of detection for this example using\n",
    "the following code, which is just a combination of the last few code blocks,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c= chsq.isf(.05) # 5% significance level\n",
    "out = []\n",
    "for k0,k1,k2 in gen_sample(100):\n",
    "   pH0 = sum((k0,k1,k2))/sum((n0,n1,n2))\n",
    "   numer = np.sum([np.log(binom(ni,pH0).pmf(ki)) \n",
    "                     for ni,ki in \n",
    "                         zip((n0,n1,n2),(k0,k1,k2))])\n",
    "   denom = np.sum([np.log(binom(ni,pi).pmf(ki)) \n",
    "                     for ni,ki,pi in \n",
    "                        zip((n0,n1,n2),(k0,k1,k2),(p0,p1,p2))])\n",
    "   out.append(-2*(numer-denom)>c)\n",
    "\n",
    "print (np.mean(out)) # estimated probability of detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The above simulation shows the estimated probability of\n",
    "detection, for this set of example parameters. This relative low\n",
    "probability of detection means that while the test is unlikely (i.e.,\n",
    "at the 5% significance level) to mistakenly pick the null hypothesis,\n",
    "it is likewise missing many of the $H_1$ cases (i.e., low probability\n",
    "of detection). The trade-off between which is more important is up to\n",
    "the particular context of the problem. In some situations, we may\n",
    "prefer additional false alarms in exchange for missing fewer $H_1$\n",
    "cases.\n",
    "\n",
    "\n",
    "### Permutation Test\n",
    "\n",
    "<!-- p 475, Essential_Statistical_Inference_Boos.pdf -->\n",
    "<!-- p. 35, Applied_adaptive_statistical_methods_OGorman.pdf -->\n",
    "<!-- p. 80, Introduction_to_Statistics_Through_Resampling_Methods_and_R_Good.pdf -->\n",
    "<!-- p. 104, Statistical_inference_for_data_science_Caffo.pdf -->\n",
    "<!-- p. 178, All of statistics -->\n",
    "\n",
    "The Permutation Test is good way to test whether or not\n",
    "samples  samples come from the same distribution. For example, suppose that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "X_1, X_2, \\ldots, X_m \\sim F\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and also,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Y_1, Y_2, \\ldots, Y_n \\sim G\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " That is, $Y_i$ and $X_i$ come from different distributions. Suppose\n",
    "we have some test statistic, for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "T(X_1,\\ldots,X_m,Y_1,\\ldots,Y_n)  = \\vert\\overline{X}-\\overline{Y}\\vert\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Under the null hypothesis for which $F=G$, any of the\n",
    "$(n+m)!$ permutations are equally likely. Thus, suppose for\n",
    "each of the $(n+m)!$ permutations, we have the computed\n",
    "statistic,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\lbrace T_1,T_2,\\ldots,T_{(n+m)!} \\rbrace\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then, under the null hypothesis, each of these values is equally\n",
    "likely. The distribution of $T$ under the null hypothesis is the *permutation\n",
    "distribution* that puts weight $1/(n+m)!$ on each $T$-value. Suppose $t_o$ is\n",
    "the observed value of the test statistic and assume that large $T$ rejects the\n",
    "null hypothesis, then the p-value for the permutation test is the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(T>t_o)= \\frac{1}{(n+m)!} \\sum_{j=1}^{(n+m)!} I(T_j>t_o)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $I()$ is the indicator function. For large $(n+m)!$, we can\n",
    "sample randomly from the set of all permutations to estimate this p-value.\n",
    "\n",
    "**Example.** Let's return to our coin-flipping example from last time, but\n",
    "now we have only two coins. The hypothesis is that both coins\n",
    "have the same probability of heads. We can use the built-in\n",
    "function in Numpy to compute the random permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=binom(10,0.3).rvs(5) # p=0.3\n",
    "y=binom(10,0.5).rvs(3) # p=0.5\n",
    "z = np.hstack([x,y]) # combine into one array\n",
    "t_o = abs(x.mean()-y.mean()) \n",
    "out = [] # output container\n",
    "for k in range(1000):\n",
    "   perm = np.random.permutation(z)\n",
    "   T=abs(perm[:len(x)].mean()-perm[len(x):].mean())\n",
    "   out.append((T>t_o))\n",
    "\n",
    "print ('p-value = ', np.mean(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that the size of total permutation space is\n",
    "$8!=40320$ so we are taking relatively few (i.e., 100) random\n",
    "permutations from this space.\n",
    "\n",
    "### Wald Test\n",
    "\n",
    "The Wald Test is an asympotic test. Suppose we have $H_0:\\theta=\\theta_0$ and\n",
    "otherwise $H_1:\\theta\\ne\\theta_0$, the corresponding statistic is defined as\n",
    "the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "W=\\frac{\\hat{\\theta}_n-\\theta_0}{\\texttt{se}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\hat{\\theta}$ is the maximum likelihood estimator and\n",
    "$\\texttt{se}$ is the standard error,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\texttt{se} = \\sqrt{\\mathbb{V}(\\hat{\\theta}_n)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Under general conditions, $W\\overset{d}{\\to} \\mathcal{N}(0,1)$.\n",
    "Thus, an asympotic test at level $\\alpha$ rejects when $\\vert W\\vert>\n",
    "z_{\\alpha/2}$ where $z_{\\alpha/2}$ corresponds to $\\mathbb{P}(\\vert\n",
    "Z\\vert>z_{\\alpha/2})=\\alpha$ with $Z \\sim \\mathcal{N}(0,1)$.  For our favorite\n",
    "coin-flipping example, if $H_0:\\theta=\\theta_0$, then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "W = \\frac{\\hat{\\theta}-\\theta_0}{\\sqrt{\\hat{\\theta}(1-\\hat{\\theta})/n}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can simulate this using the following code at the usual\n",
    "5% significance level,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "theta0 = 0.5 # H0\n",
    "k=np.random.binomial(1000,0.3)\n",
    "theta_hat = k/1000. # MLE\n",
    "W = (theta_hat-theta0)/np.sqrt(theta_hat*(1-theta_hat)/1000)\n",
    "c = stats.norm().isf(0.05/2) # z_{alpha/2}\n",
    "print (abs(W)>c) # if true, reject H0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This rejects $H_0$ because the true $\\theta=0.3$ and the null hypothesis\n",
    "is that $\\theta=0.5$.  Note that $n=1000$ in this case which puts us well inside the\n",
    "asympotic range of the result. We can re-do this example to estimate\n",
    "the detection probability for this example as in the following code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta0 = 0.5 # H0\n",
    "c = stats.norm().isf(0.05/2.) # z_{alpha/2}\n",
    "out = []\n",
    "for i in range(100):\n",
    "   k=np.random.binomial(1000,0.3)\n",
    "   theta_hat = k/1000. # MLE\n",
    "   W = (theta_hat-theta0)/np.sqrt(theta_hat*(1-theta_hat)/1000.)\n",
    "   out.append(abs(W)>c) # if true, reject H0\n",
    "\n",
    "print (np.mean(out)) # detection probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Multiple Hypotheses\n",
    "\n",
    "Thus far, we have focused primarily on two competing hypotheses. Now, we\n",
    "consider multiple comparisons. The general situation is the following.  We test\n",
    "the null hypothesis against  a sequence of $n$ competing hypotheses $H_k$.  We\n",
    "obtain p-values for each hypothesis so now we have multiple p-values to\n",
    "consider $\\lbrace p_k \\rbrace$. To boil this sequence down to a single\n",
    "criterion, we can make the following argument. Given $n$ independent hypotheses\n",
    "that are all untrue, the probability of getting at least one false alarm is the\n",
    "following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P_{FA} = 1-(1-p_0)^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $p_0$ is the individual p-value threshold (say, 0.05). The\n",
    "problem here is that $P_{FA}\\rightarrow 1$ as $n\\rightarrow\\infty$.  If we want\n",
    "to make many comparisons at once and control the overall false alarm rate the\n",
    "overall p-value should be computed under the assumption that none of the\n",
    "competing hypotheses is valid. The most common way to address this is with the\n",
    "Bonferroni correction which says that the individual significance level should\n",
    "be reduced to $p/n$. Obviously, this  makes it much harder to declare\n",
    "significance for any particular hypothesis. The natural consequence of this\n",
    "conservative restriction is to reduce the statistical power of the experiment,\n",
    "thus making it more likely the true effects will be missed.\n",
    "\n",
    "In 1995, Benjamini and Hochberg devised a simple method that tells which\n",
    "p-values are statistically significant. The procedure is to sort the list of\n",
    "p-values in ascending order, choose a false-discovery rate (say, $q$), and then\n",
    "find the largest p-value in the sorted list such that $p_k \\le k q/n$, where\n",
    "$k$ is the p-value's position in the sorted list. Finally, declare that $p_k$\n",
    "value and all the others less than it statistically significant. This procedure\n",
    "guarantees that the proportion of false-positives is less than $q$  (on\n",
    "average). The Benjamini-Hochberg procedure (and its derivatives) is fast and\n",
    "effective and is widely used for testing hundreds of primarily false hypotheses\n",
    "when studying genetics or diseases. Additionally, this\n",
    "procedure provides better statistical power than the Bonferroni correction.\n",
    "\n",
    "<!-- TODO: Fisher transformation -->\n",
    "<!-- TODO: Cohen's D test for effect size -->\n",
    "<!-- TODO: add log-linear transform -->\n",
    "<!-- TODO: add Fisher transform -->\n",
    "<!-- TODO: Log-Linear Models -->\n",
    "\n",
    "## Fisher Exact Test\n",
    "\n",
    "<!-- # #ifdef SINGLE -->\n",
    "<!-- TITLE:  Fisher Exact Test -->\n",
    "<!-- AUTHOR: Jose Unpingco -->\n",
    "<!-- DATE: today -->\n",
    "<!-- # #endif -->\n",
    "\n",
    "<!-- References -->\n",
    "<!-- ------------ -->\n",
    "<!-- .. [freeman-halton] Freeman, G. H., and John H. Halton. \"Note on an exact -->\n",
    "<!-- treatment of contingency, goodness of fit and other problems of -->\n",
    "<!-- significance.\" Biometrika (1951): 141-149. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"tab:contingencyTable\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{table}[]\n",
    "\\centering\n",
    "\\caption{Example Contingency Table}\n",
    "\\label{tab:contingencyTable} \\tag{2}\n",
    "\\begin{tabular}{lllll}\n",
    "\\cline{1-4}\n",
    "\\multicolumn{1}{|l|}{}       & \\multicolumn{1}{l|}{Infection} & \\multicolumn{1}{l|}{No infection} & \\multicolumn{1}{l|}{Total} &  \\\\ \\cline{1-4}\n",
    "\\multicolumn{1}{|l|}{Male}   & \\multicolumn{1}{c|}{13}        & \\multicolumn{1}{c|}{11}           & \\multicolumn{1}{c|}{24}    &  \\\\ \\cline{1-4}\n",
    "\\multicolumn{1}{|l|}{Female} & \\multicolumn{1}{c|}{12}        & \\multicolumn{1}{c|}{1}            & \\multicolumn{1}{c|}{13}    &  \\\\ \\cline{1-4}\n",
    "\\multicolumn{1}{|l|}{Total}  & \\multicolumn{1}{c|}{25}        & \\multicolumn{1}{c|}{12}           & \\multicolumn{1}{c|}{37}    &  \\\\ \\cline{1-4}\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contingency tables represent the partitioning of a sample population of two\n",
    "categories between two different classifications as shown in the following\n",
    "Table [2](#tab:contingencyTable). The question is whether or not the observed\n",
    "table corresponds to a random partition of the sample population, constrained\n",
    "by the marginal sums. Note that because this is a two-by-two table, a change in\n",
    "any of the table entries automatically affects all of the other terms because\n",
    "of the row and column sum constraints. This means that equivalent questions\n",
    "like \"Under a random partition, what is the probability that a particular\n",
    "table entry is at least as large as a given value?\" can be meaningfully posed. \n",
    "\n",
    "<!-- #`` -->\n",
    "\n",
    "\n",
    "The Fisher Exact Test addresses this question. The idea is to compute the\n",
    "probability of a particular entry of the table, conditioned upon the marginal\n",
    "row and column sums,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(X_{i,j}\\vert r_1,r_2,c_1,c_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $X_{i,j}$ is $(i,j)$ table entry,  $r_1$ represents the sum of\n",
    "the first row, $r_2$ represents the sum of the second row, $c_1$ represents the\n",
    "sum of the first column, and $c_2$ is the sum of the second column.  This\n",
    "probability is given by the *hypergeometric distribution*. Recall that the\n",
    "hypergeometric distribution gives the probability of sampling (without\n",
    "replacement) $k$ items from a population of $N$ items consisting of exactly two\n",
    "different kinds of items,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(X=k) = \\frac{\\binom{K}{k}\\binom{N-K}{n-k}}{\\binom{N}{n}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $N$ is the population size, $K$ is the total number of possible\n",
    "favorable draws, $n$ is the number of draws, and $k$ is the number of observed\n",
    "favorable draws.  With the corresponding identification of variables, the\n",
    "hypergeometric distribution gives the desired conditional probability: $K=r_1,\n",
    "k=x, n= c_1, N=r_1+r_2$. \n",
    "\n",
    "In the example of the Table [2](#tab:contingencyTable), the probability for\n",
    "$x=13$ male infections among a population of $r_1=24$ males in a total\n",
    "population of $c_1=25$ infected persons, including $r_2=13$ females. The\n",
    "`scipy.stats` module has the Fisher Exact Test implemented as shown below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "table = [[13,11],[12,1]]\n",
    "odds_ratio, p_value=scipy.stats.fisher_exact(table)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The default for `scipy.stats.fisher_exact` is the two-sided\n",
    "test. The following result is for the `less` option,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "odds_ratio, p_value=scipy.stats.fisher_exact(table,alternative='less')\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This means that the p-value is computed by summing  over the\n",
    "probabilities of contingency tables that are *less* extreme than the \n",
    "given table. To undertand what this means, we can use \n",
    "the `scipy.stats.hypergeom` function to compute the probabilities of\n",
    "these with the number of infected men is less than or equal to 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hg = scipy.stats.hypergeom(37, 24, 25)\n",
    "probs = [(hg.pmf(i)) for i in range(14)]\n",
    "print (probs)\n",
    "print(sum(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is the same as the prior p-value result we obtained from\n",
    "`scipy.stats.fisher_exact`. Another option is `greater` which derives from the\n",
    "following analogous summation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "odds_ratio, p_value=scipy.stats.fisher_exact(table,alternative='greater')\n",
    "probs = [hg.pmf(i) for i in range(13,25)]\n",
    "print(probs)\n",
    "print(p_value)\n",
    "print(sum(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Finally, the two-sided version excludes those individual\n",
    "table probabilities that are less that of the given table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_,p_value=scipy.stats.fisher_exact(table)\n",
    "probs = [ hg.pmf(i) for i in range(25) ]\n",
    "print(sum(i for i in probs if i<= hg.pmf(13)))\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Thus, for this particular contingency table, we \n",
    "could reasonably conclude that 13 infected males in this total\n",
    "population is statistically significant with a p-value less than\n",
    "five percent.\n",
    "\n",
    "Performing this kind of analysis for tables larger than `2x2` easily becomes\n",
    "computationally challenging due to the nature of the underlying combinatorics and \n",
    "usually requires specialized approximations.\n",
    "\n",
    "In this section, we discussed the structure of statistical hypothesis testing\n",
    "and defined the various  terms that are commonly used for this process, along\n",
    "with the illustrations of what they mean in our running coin-flipping example.\n",
    "From an engineering standpoint, hypothesis testing is not as common as\n",
    "confidence-intervals and point estimates. On the other hand, hypothesis testing\n",
    "is very common in social and medical science, where one must deal with\n",
    "practical constraints that may limit the sample size or other aspects of the\n",
    "hypothesis testing rubric. In engineering, we can usually have much more\n",
    "control over the samples and models we employ because they are typically\n",
    "inanimate objects that can be measured repeatedly and consistently. This is\n",
    "obviously not so with human studies, which generally have other ethical and\n",
    "legal considerations."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
