{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('../../Python_probability_statistics_machine_learning_2E.png',width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we  consider the famous Gauss-Markov problem which will give\n",
    "us\n",
    "an opportunity to use all the material we have so far developed. The\n",
    "Gauss-\n",
    "Markov model is the fundamental model for noisy parameter estimation because it\n",
    "estimates unobservable parameters given a noisy indirect measurement.\n",
    "Incarnations of the same model appear in all studies of Gaussian models. This\n",
    "case is an excellent opportunity to use everything we have so far learned about\n",
    "projection and conditional expectation.\n",
    "\n",
    "Following Luenberger\n",
    "[[luenberger1968optimization]](#luenberger1968optimization) let's  consider the\n",
    "following problem:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{W} \\boldsymbol{\\upbeta} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    " where $\\mathbf{W}$ is a $ n \\times m $ matrix, and $\\mathbf{y}$ is a\n",
    "$n \\times\n",
    "1$ vector. Also, $\\boldsymbol{\\epsilon}$ is a $n$-dimensional normally\n",
    "distributed random vector with zero-mean and covariance,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}( \\boldsymbol{\\epsilon} \\boldsymbol{\\epsilon}^T) = \\mathbf{Q}\n",
    "$$\n",
    "\n",
    " Note that engineering systems usually provide a *calibration mode*\n",
    "where you\n",
    "can estimate $\\mathbf{Q}$ so it's not fantastical to assume you have\n",
    "some\n",
    "knowledge of the noise statistics. The problem is to find a matrix\n",
    "$\\mathbf{K}$\n",
    "so that $ \\boldsymbol{\\hat{\\upbeta}}=\\mathbf{K}^T \\mathbf{y}$\n",
    "approximates $\n",
    "\\boldsymbol{\\upbeta}$.  Note that we only have knowledge of\n",
    "$\\boldsymbol{\\upbeta}$ via $ \\mathbf{y}$ so we can't measure it directly.\n",
    "Further, note that $\\mathbf{K} $ is a matrix, not a vector, so there are $m\n",
    "\\times n$ entries to compute. \n",
    "\n",
    "We can approach this problem the usual way by\n",
    "trying to solve the MMSE\n",
    "problem:\n",
    "\n",
    "$$\n",
    "\\min_K\\mathbb{E}(\\Vert\\boldsymbol{\\hat{\\upbeta}}-\\boldsymbol{\\upbeta}\n",
    "\\Vert^2)\n",
    "$$\n",
    "\n",
    " which we can write out as\n",
    "\n",
    "$$\n",
    "\\min_K\\mathbb{E}(\\Vert\\boldsymbol{\\hat{\\upbeta}}-\\boldsymbol{\\upbeta}\n",
    "\\Vert^2) =  \\min_K\\mathbb{E}(\\Vert \\mathbf{K}^T\\mathbf{y}- \\boldsymbol{\\upbeta}\n",
    "\\Vert^2) =  \\min_K\\mathbb{E}(\\Vert\n",
    "\\mathbf{K}^T\\mathbf{W}\\mathbf{\\boldsymbol{\\upbeta}}+\\mathbf{K}^T\\boldsymbol{\\epsilon}-\n",
    "\\boldsymbol{\\upbeta} \\Vert^2)\n",
    "$$\n",
    "\n",
    " and since $\\boldsymbol{\\epsilon}$ is the only random variable here,\n",
    "this\n",
    "simplifies to\n",
    "\n",
    "$$\n",
    "\\min_K \\Vert\n",
    "\\mathbf{K}^T\\mathbf{W}\\mathbf{\\boldsymbol{\\upbeta}}-\\boldsymbol{\\upbeta} \\Vert^2\n",
    "+ \\mathbb{E}(\\Vert\\mathbf{K}^T\\boldsymbol{\\epsilon} \\Vert^2)\n",
    "$$\n",
    "\n",
    " The next step is to compute\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\Vert\\mathbf{K}^T\\boldsymbol{\\epsilon} \\Vert^2) =\\Tr\n",
    "\\mathbb{E}(\\mathbf{K}^T\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{K})=\\Tr(\\mathbf{K^T\n",
    "Q K})\n",
    "$$\n",
    "\n",
    " using the properties of the trace of  a matrix. We can assemble\n",
    "everything as\n",
    "\n",
    "$$\n",
    "\\min_K\\Vert\\mathbf{K^T\n",
    "W}\\boldsymbol{\\upbeta}-\\boldsymbol{\\upbeta}\\Vert^2+\\Tr(\\mathbf{K^T Q K})\n",
    "$$\n",
    "\n",
    " Now, if we were to solve this for $\\mathbf{K}$, it would be a\n",
    "function of $\n",
    "\\boldsymbol{\\upbeta}$, which is the same thing as saying that the\n",
    "estimator, $\n",
    "\\boldsymbol{\\hat{\\upbeta}}$, is a function of what we are trying to\n",
    "estimate,\n",
    "$\\boldsymbol{\\upbeta}$, which makes no sense. However, writing this out\n",
    "tells us\n",
    "that if we had $\\mathbf{K^T W}= \\mathbf{I}$, then the first term\n",
    "vanishes and\n",
    "the problem simplifies to\n",
    "\n",
    "$$\n",
    "\\min_K \\Tr(\\mathbf{K^T Q K})\n",
    "$$\n",
    "\n",
    " with the contraint,\n",
    "\n",
    "$$\n",
    "\\mathbf{K^T W} = \\mathbf{I}\n",
    "$$\n",
    "\n",
    " This requirement is the same as asserting that the estimator is unbiased,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\hat{\\upbeta}})=\\mathbf{K^T W}  \\boldsymbol{\\upbeta} =\n",
    "\\boldsymbol{\\upbeta}\n",
    "$$\n",
    "\n",
    " To line this problem up with our earlier work, let's consider  the\n",
    "$i^{th}$\n",
    "column of $\\mathbf{K}$, $\\mathbf{k}_i$. Now, we can re-write the\n",
    "problem as\n",
    "\n",
    "$$\n",
    "\\min_k (\\mathbf{k}_i^T \\mathbf{Q} \\mathbf{k}_i)\n",
    "$$\n",
    "\n",
    " with\n",
    "\n",
    "$$\n",
    "\\mathbf{W}^T \\mathbf{k}_i = \\mathbf{e}_i\n",
    "$$\n",
    "\n",
    " and we know how to solve this from our previous work on contrained\n",
    "optimization,\n",
    "\n",
    "$$\n",
    "\\mathbf{k}_i = \\mathbf{Q}^{-1}\\mathbf{W} (\\mathbf{W^T Q^{-1} W})^{-1}\n",
    "\\mathbf{e}_i\n",
    "$$\n",
    "\n",
    " Now all we have to do is stack these together for the general solution:\n",
    "\n",
    "$$\n",
    "\\mathbf{K} =  \\mathbf{Q}^{-1}\\mathbf{W} (\\mathbf{W^T Q^{-1} W})^{-1}\n",
    "$$\n",
    "\n",
    " It's easy when you have all of the concepts lined up! For completeness, the\n",
    "covariance of the error is\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\hat{\\boldsymbol{\\upbeta}}-\\boldsymbol{\\upbeta})\n",
    "(\\hat{\\boldsymbol{\\upbeta}}-\\boldsymbol{\\upbeta})^T\n",
    "=\\mathbf{K}^T\\mathbf{Q}\\mathbf{K} =(\\mathbf{W}^T \\mathbf{Q}^{-1}\n",
    "\\mathbf{W})^{-1}\n",
    "$$\n",
    "\n",
    "<!-- # !bc pycod -->\n",
    "<!-- # from mpl_toolkits.mplot3d import proj3d -->\n",
    "<!-- #\n",
    "from numpy.linalg import inv -->\n",
    "<!-- # import matplotlib.pyplot as plt -->\n",
    "<!--\n",
    "# import numpy as np -->\n",
    "<!-- # from numpy import matrix, linalg, ones, array\n",
    "-->\n",
    "<!-- # Q = np.eye(3)*.1 # error covariance matrix -->\n",
    "<!-- # beta =\n",
    "matrix(ones((2,1))) # this is what we are trying estimate -->\n",
    "<!-- # W =\n",
    "matrix([[1,2], -->\n",
    "<!-- #             [2,3], -->\n",
    "<!-- #             [1,1]]) -->\n",
    "<!-- # ntrials = 50 -->\n",
    "<!-- # epsilon =\n",
    "np.random.multivariate_normal((0,0,0),Q,ntrials).T -->\n",
    "<!-- # y=W*beta+epsilon\n",
    "-->\n",
    "<!-- # -->\n",
    "<!-- # K=inv(W.T*inv(Q)*W)*matrix(W.T)*inv(Q) -->\n",
    "<!-- # b=K*y\n",
    "#estimated beta from data -->\n",
    "<!-- # -->\n",
    "<!-- # fig = plt.figure() -->\n",
    "<!-- #\n",
    "fig.set_size_inches([6,6]) -->\n",
    "<!-- # -->\n",
    "<!-- # # some convenience definitions\n",
    "for plotting -->\n",
    "<!-- # bb = array(b) -->\n",
    "<!-- # bm = bb.mean(1) -->\n",
    "<!-- # yy =\n",
    "array(y) -->\n",
    "<!-- # ax = fig.add_subplot(111, projection='3d') -->\n",
    "<!-- # -->\n",
    "<!-- # ax.plot3D(yy[0,:],yy[1,:],yy[2,:],'mo',label='y',alpha=0.3) -->\n",
    "<!-- #\n",
    "ax.plot3D([beta[0,0],0],[beta[1,0],0],[0,0],'r-',label=r'$\\upbeta$') -->\n",
    "<!-- #\n",
    "ax.plot3D([bm[0],0],[bm[1],0],[0,0],'g-',lw=1,label=r'$\\widehat{\\upbeta}_m$')\n",
    "-->\n",
    "<!-- #\n",
    "ax.plot3D(bb[0,:],bb[1,:],0*bb[1,:],'.g',alpha=0.5,lw=3,label=r'$\\hat{\\upbeta}$')\n",
    "-->\n",
    "<!-- # ax.legend(loc=0,fontsize=18) -->\n",
    "<!-- # plt.show() -->\n",
    "<!-- # !ec -->\n",
    "<!-- dom:FIGURE: [fig-statistics/Gauss_Markov_001.png, width=500 frac=0.85] The\n",
    "red circles show the points to be estimated in the *xy*-plane by the black\n",
    "points. <div id=\"fig:Gauss_Markov_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div\n",
    "id=\"fig:Gauss_Markov_001\"></div>\n",
    "\n",
    "<p>The red circles show the points to be\n",
    "estimated in the <em>xy</em>-plane by the black points.</p>\n",
    "<img src=\"fig-\n",
    "statistics/Gauss_Markov_001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "[Figure](#fig:Gauss_Markov_001) shows the simulated $\\mathbf{y}$ data as red\n",
    "circles. The black dots show the corresponding estimates,\n",
    "$\\boldsymbol{\\hat{\\upbeta}}$ for each sample. The black lines show the true\n",
    "value\n",
    "of $\\boldsymbol{\\upbeta}$ versus the average of the estimated\n",
    "$\\boldsymbol{\\upbeta}$-values, $\\widehat{\\boldsymbol{\\upbeta}_m}$. The matrix\n",
    "$\\mathbf{K}$ maps the red circles in the corresponding dots. Note there are\n",
    "many\n",
    "possible ways to map the red circles to the plane, but the $\\mathbf{K}$ is\n",
    "the\n",
    "one that minimizes the MSE for $\\boldsymbol{\\upbeta}$. \n",
    "\n",
    "**Programming Tip.**\n",
    "The following snippets provide a quick code walkthrough.\n",
    "To simulate the target\n",
    "data, we define the relevant \n",
    "matrices below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.eye(3)*0.1 # error covariance matrix\n",
    "# this is what we are trying estimate\n",
    "beta = matrix(ones((2,1))) \n",
    "W = matrix([[1,2],\n",
    "            [2,3],\n",
    "            [1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we generate the noise terms and create\n",
    "the simulated data, $y$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrials = 50 \n",
    "epsilon = np.random.multivariate_normal((0,0,0),Q,ntrials).T \n",
    "y=W*beta+epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-statistics/Gauss_Markov_002.png, width=500 frac=0.85]\n",
    "Focusing on the *xy*-plane in [Figure](#fig:Gauss_Markov_001), the dashed line\n",
    "shows the true value for $\\boldsymbol{\\upbeta}$ versus the mean of the estimated\n",
    "values $\\widehat{\\boldsymbol{\\upbeta}}_m$. <div id=\"fig:Gauss_Markov_002\"></div>\n",
    "-->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Gauss_Markov_002\"></div>\n",
    "\n",
    "<p>Focusing on\n",
    "the <em>xy</em>-plane in [Figure](#fig:Gauss_Markov_001), the dashed line shows\n",
    "the true value for $\\boldsymbol{\\upbeta}$ versus the mean of the estimated\n",
    "values $\\widehat{\\boldsymbol{\\upbeta}}_m$.</p>\n",
    "<img src=\"fig-\n",
    "statistics/Gauss_Markov_002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "[Figure](#fig:Gauss_Markov_002) shows more detail in the horizontal *xy*-plane\n",
    "of [Figure](#fig:Gauss_Markov_001).  [Figure](#fig:Gauss_Markov_002) shows\n",
    "the\n",
    "dots, which are individual estimates of $\\boldsymbol{\\hat{\\upbeta}}$ from the\n",
    "corresponding simulated $\\mathbf{y}$ data. The dashed line is the true value\n",
    "for\n",
    "$\\boldsymbol{\\upbeta}$ and the filled line ($\\widehat{\\boldsymbol{\\upbeta}}_m$)\n",
    "is the average of all the dots.  The gray ellipse provides an error ellipse\n",
    "for\n",
    "the covariance of the estimated $\\boldsymbol{\\upbeta}$ values.  \n",
    "\n",
    "**Programming\n",
    "Tip.**\n",
    "\n",
    "The following snippets provide a quick walkthrough of\n",
    "the construction\n",
    "of [Figure](#fig:Gauss_Markov_002). To draw\n",
    "the ellipse, we need to import the\n",
    "patch primitive,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from  matplotlib.patches import Ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the parameters of the error ellipse based on the\n",
    "covariance matrix\n",
    "of the individual estimates of $\\boldsymbol{\\upbeta}$\n",
    "in the `bm_cov` variable\n",
    "below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,S,V = linalg.svd(bm_cov) \n",
    "err = np.sqrt((matrix(bm))*(bm_cov)*(matrix(bm).T))\n",
    "theta = np.arccos(U[0,1])/np.pi*180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we draw the add the scaled ellipse\n",
    "in the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.add_patch(Ellipse(bm,err*2/np.sqrt(S[0]),\n",
    "                     err*2/np.sqrt(S[1]),\n",
    "                     angle=theta,color='gray'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- References -->\n",
    "<!-- --------------- -->\n",
    "<!--  -->\n",
    "<!-- * Luenberger, David\n",
    "G. *Optimization by vector space methods*. Wiley-Interscience, 1997. -->"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
