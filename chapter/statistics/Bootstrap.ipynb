{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # TODO: Do bootstrap regression example from Statistical_Models_Freedman.pdf -->\n",
    "\n",
    "\n",
    "As we have seen, it can be very difficult  or impossible to determine\n",
    "the probability density distribution of the estimator of some\n",
    "quantity. The idea behind the bootstrap is that we can use computation\n",
    "to approximate  these functions which would otherwise be impossible to\n",
    "solve for analytically. \n",
    "\n",
    "Let's start with a simple example. Suppose we have the following set of random\n",
    "variables, $\\lbrace X_1, X_2, \\ldots, X_n \\rbrace$ where each $X_k \\sim F$. In\n",
    "other words the samples are all drawn from the same unknown distribution $F$.\n",
    "Having run the experiment, we thereby obtain the following sample set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\lbrace x_1, x_2, \\ldots, x_n \\rbrace\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The sample mean is computed from this set as,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The next question is how close is the sample mean to the true mean,\n",
    "$\\theta = \\mathbb{E}_F(X)$. Note that the second central moment of $X$ is as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu_2(F) := \\mathbb{E}_F (X^2)  - (\\mathbb{E}_F (X))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The standard deviation of the sample mean, $\\bar{x}$, given $n$\n",
    "samples from an underlying distribution $F$, is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(F) = (\\mu_2(F)/n)^{1/2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Unfortunately, because we have only the set of samples $\\lbrace x_1,\n",
    "x_2, \\ldots, x_n \\rbrace$ and not $F$ itself, we cannot compute this and\n",
    "instead must use the estimated standard error,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bar{\\sigma} = (\\bar{\\mu}_2/n)^{1/2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\bar{\\mu}_2 = \\sum (x_i -\\bar{x})^2/(n-1)$, which is the\n",
    "unbiased estimate of $\\mu_2(F)$. However, this is not the only way to proceed.\n",
    "Instead, we could replace $F$ by some estimate, $\\hat{F}$ obtained as a\n",
    "piecewise function of $\\lbrace x_1, x_2, \\ldots, x_n \\rbrace$ by placing\n",
    "probability mass $1/n$ on each $x_i$. With that in place, we can compute the\n",
    "estimated standard error as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\sigma}_B  = (\\mu_2(\\hat{F})/n)^{1/2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which is called the *bootstrap estimate* of the standard error.\n",
    "Unfortunately, the story effectively ends here. In even a slightly more general\n",
    "setting, there is no clean formula $\\sigma(F)$ within which $F$ can be swapped\n",
    "for $\\hat{F}$.\n",
    "\n",
    "This is where the computer saves the day. We actually do not need to know the\n",
    "formula $\\sigma(F)$ because we can compute it using a resampling method. The\n",
    "key idea is to sample with replacement from $\\lbrace x_1, x_2, \\ldots, x_n\n",
    "\\rbrace$. The new set of $n$ independent draws (with replacement) from this set\n",
    "is the *bootstrap sample*,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y^* = \\lbrace x_1^*, x_2^*, \\ldots, x_n^* \\rbrace\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Monte Carlo algorithm proceeds by first by selecting a large number  of\n",
    "bootstrap samples, $\\lbrace y^*_k\\rbrace$, then computing the statistic on each\n",
    "of these samples, and then computing the sample standard deviation of the\n",
    "results in the usual way. Thus, the bootstrap estimate of the statistic\n",
    "$\\theta$ is the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\theta}^*_B = \\frac{1}{B} \\sum_k \\hat{\\theta}^*(k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " with the corresponding square of the sample standard deviation as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\sigma}_B^2 = \\frac{1}{B-1} \\sum_k (\\hat{\\theta}^*(k)-\\hat{\\theta}^*_B )^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The process is much simpler than  the notation implies.\n",
    "Let's explore this with a simple example using Python. The next block\n",
    "of code sets up some samples from a $\\beta(3,2)$ distribution,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_=np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "rv = stats.beta(3,2)\n",
    "xsamples = rv.rvs(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Because this is simulation data, we already know that the\n",
    "mean is $\\mu_1 = 3/5$ and the standard deviation of the sample mean\n",
    "for $n=50$ is $\\bar{\\sigma} =\\sqrt{2}/50$, which we will verify\n",
    "later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import subplots\n",
    "fig,ax = subplots()\n",
    "fig.set_size_inches(8,4)\n",
    "_=ax.hist(xsamples,density=True,color='gray')\n",
    "ax2 = ax.twinx()\n",
    "_=ax2.plot(np.linspace(0,1,100),rv.pdf(np.linspace(0,1,100)),lw=3,color='k')\n",
    "_=ax.set_xlabel('$x$',fontsize=28)\n",
    "_=ax2.set_ylabel('  $y$',fontsize=28,rotation='horizontal')\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-statistics/Bootstrap_001.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-statistics/Bootstrap_001.png, width=500 frac=0.85] The $\\beta(3,2)$ distribution and the histogram that approximates it. <div id=\"fig:Bootstrap_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Bootstrap_001\"></div>\n",
    "\n",
    "<p>The $\\beta(3,2)$ distribution and the histogram that approximates it.</p>\n",
    "<img src=\"fig-statistics/Bootstrap_001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "[Figure](#fig:Bootstrap_001) shows the $\\beta(3,2)$ distribution and\n",
    "the corresponding histogram of the samples. The histogram represents\n",
    "$\\hat{F}$ and is the distribution we sample from to obtain the\n",
    "bootstrap samples. As shown, the $\\hat{F}$ is a pretty crude estimate\n",
    "for the $F$ density (smooth solid line), but that's not a serious\n",
    "problem insofar as the following bootstrap estimates are concerned.\n",
    "In fact, the approximation $\\hat{F}$ has a naturally tendency to\n",
    "pull towards the bulk of probability mass. This is a\n",
    "feature, not a bug; and is the underlying mechanism that explains\n",
    "bootstrapping, but the formal proofs that exploit this basic\n",
    "idea are far out of our scope here.  The next block generates the\n",
    "bootstrap samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yboot = np.random.choice(xsamples,(100,50))\n",
    "yboot_mn = yboot.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and the bootstrap estimate is therefore,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.std(yboot.mean(axis=1)) # approx sqrt(1/1250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Figure](#fig:Bootstrap_002) shows the distribution of computed\n",
    "sample means from the bootstrap samples.  As promised, the next block\n",
    "shows how to use `sympy.stats` to compute the $\\beta(3,2)$ parameters we quoted\n",
    "earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax = subplots()\n",
    "fig.set_size_inches(8,4)\n",
    "_=ax.hist(yboot.mean(axis=1),density=True,color='gray',rwidth=.8)\n",
    "_=ax.set_title('Bootstrap std of sample mean %3.3f vs actual %3.3f'%\n",
    "             (np.std(yboot.mean(axis=1)),np.sqrt(1/1250.)))\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-statistics/Bootstrap_002.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-statistics/Bootstrap_002.png, width=500 frac=0.85] For each bootstrap draw, we compute the sample mean. This is the histogram of those sample means that will be used to compute the bootstrap estimate of the standard deviation. <div id=\"fig:Bootstrap_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Bootstrap_002\"></div>\n",
    "\n",
    "<p>For each bootstrap draw, we compute the sample mean. This is the histogram of those sample means that will be used to compute the bootstrap estimate of the standard deviation.</p>\n",
    "<img src=\"fig-statistics/Bootstrap_002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sympy as S\n",
    "import sympy.stats\n",
    "for i in range(50): # 50 samples\n",
    "    # load sympy.stats Beta random variables\n",
    "    # into global namespace using exec\n",
    "    execstring = \"x%d = S.stats.Beta('x'+str(%d),3,2)\"%(i,i)\n",
    "    exec(execstring) \n",
    "\n",
    "# populate xlist with the sympy.stats random variables\n",
    "# from above\n",
    "xlist = [eval('x%d'%(i)) for i in range(50) ]\n",
    "# compute sample mean\n",
    "sample_mean = sum(xlist)/len(xlist)\n",
    "# compute expectation of sample mean\n",
    "sample_mean_1 = S.stats.E(sample_mean).evalf()\n",
    "# compute 2nd moment of sample mean\n",
    "sample_mean_2 = S.stats.E(S.expand(sample_mean**2)).evalf()\n",
    "# standard deviation of sample mean\n",
    "# use sympy sqrt function\n",
    "sigma_smn = S.sqrt(sample_mean_2-sample_mean_1**2) # sqrt(2)/50\n",
    "print(sigma_smn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "Using the `exec` function enables the creation of a sequence of Sympy\n",
    "random variables. Sympy has the `var` function which can automatically\n",
    "create a sequence of Sympy symbols, but there is no corresponding\n",
    "function in the statistics module to do this for random variables.\n",
    "\n",
    "\n",
    "\n",
    "<!-- @@@CODE src-statistics/Bootstrap.py from-to:^import sympy as S@^print (sigma_smn) -->\n",
    "\n",
    "<!-- p.505 casella -->\n",
    "\n",
    "**Example.** Recall the delta method from the section [sec:delta_method](#sec:delta_method).  Suppose we have a set of Bernoulli coin-flips\n",
    "($X_i$) with probability of head $p$. Our maximum likelihood estimator\n",
    "of $p$ is $\\hat{p}=\\sum X_i/n$ for $n$ flips.  We know this estimator\n",
    "is unbiased with $\\mathbb{E}(\\hat{p})=p$ and $\\mathbb{V}(\\hat{p}) =\n",
    "p(1-p)/n$. Suppose we want to use the data to estimate the variance of\n",
    "the Bernoulli trials ($\\mathbb{V}(X)=p(1-p)$). By the notation the\n",
    "delta method, $g(x) = x(1-x)$. By the plug-in principle, our maximum\n",
    "likelihood estimator of this variance is then $\\hat{p}(1-\\hat{p})$. We\n",
    "want the variance of this quantity.  Using the results of the delta\n",
    "method, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{V}(g(\\hat{p})) &=(1-2\\hat{p})^2\\mathbb{V}(\\hat{p})  \\\\\\\n",
    "\\mathbb{V}(g(\\hat{p})) &=(1-2\\hat{p})^2\\frac{\\hat{p}(1-\\hat{p})}{n} \\\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's see how useful this is with a short simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "p= 0.25 # true head-up probability\n",
    "x = stats.bernoulli(p).rvs(10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The maximum likelihood estimator of $p$ is $\\hat{p}=\\sum X_i/n$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phat = x.mean()\n",
    "print(phat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then, plugging this into the delta method approximant above,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print((1-2*phat)**2*(phat)**2/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, let's try this using the bootstrap estimate of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phat_b=np.random.choice(x,(50,10)).mean(1)\n",
    "print(np.var(phat_b*(1-phat_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This shows that the delta method's estimated variance\n",
    "is different from the bootstrap method, but which one is better?\n",
    "For this situation we can solve for this directly using Sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sympy as S\n",
    "from sympy.stats import E, Bernoulli\n",
    "xdata =[Bernoulli(i,p) for i in S.symbols('x:10')]\n",
    "ph = sum(xdata)/float(len(xdata))\n",
    "g = ph*(1-ph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The argument in the `S.symbols('x:10')` function returns a sequence of Sympy\n",
    "symbols named `x1,x2` and so on. This is shorthand for creating and naming each\n",
    "symbol sequentially.\n",
    "\n",
    "\n",
    "\n",
    " Note that `g` is the $g(\\hat{p})=\\hat{p}(1- \\hat{p})$ \n",
    "whose variance we are trying to estimate. Then,\n",
    "we can plug in for the estimated $\\hat{p}$ and get the correct\n",
    "value for the variance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(E(g**2) - E(g)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This case is generally representative --- the delta method tends\n",
    "to underestimate the variance and the bootstrap estimate is better here.\n",
    "\n",
    "\n",
    "## Parametric Bootstrap\n",
    "\n",
    "In the previous example, we used the $\\lbrace x_1, x_2, \\ldots, x_n \\rbrace $\n",
    "samples themselves as the basis for $\\hat{F}$ by weighting each with $1/n$. An\n",
    "alternative is to *assume* that the samples come from a particular\n",
    "distribution, estimate the parameters of that distribution from the sample set,\n",
    "and then use the bootstrap mechanism to draw samples from the assumed\n",
    "distribution, using the so-derived parameters. For example, the next code block\n",
    "does this for a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rv = stats.norm(0,2)\n",
    "xsamples = rv.rvs(45)\n",
    "# estimate mean and var from xsamples\n",
    "mn_ = np.mean(xsamples)\n",
    "std_ = np.std(xsamples)\n",
    "# bootstrap from assumed normal distribution with\n",
    "# mn_,std_ as parameters\n",
    "rvb = stats.norm(mn_,std_) #plug-in distribution\n",
    "yboot = rvb.rvs(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @@@CODE src-statistics/Bootstrap.py from-to:^# In\\[7\\]:@^yboot -->\n",
    "\n",
    " Recall the sample variance estimator is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "S^2 = \\frac{1}{n-1} \\sum (X_i-\\bar{X})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Assuming that the samples are normally distributed, this\n",
    "means that $(n-1)S^2/\\sigma^2$ has a chi-squared distribution with\n",
    "$n-1$ degrees of freedom. Thus, the variance, $\\mathbb{V}(S^2) = 2\n",
    "\\sigma^4/(n-1) $. Likewise, the MLE plug-in estimate for this is\n",
    "$\\mathbb{V}(S^2) = 2 \\hat{\\sigma}^4/(n-1)$ The following code computes\n",
    "the variance of the sample variance, $S^2$ using the MLE and bootstrap\n",
    "methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MLE-Plugin Variance of the sample mean \n",
    "print(2*(std_**2)**2/9.)         # MLE plugin\n",
    "# Bootstrap variance of the sample mean \n",
    "print(yboot.var())\n",
    "# True variance of sample mean \n",
    "print(2*(2**2)**2/9.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @@@CODE src-statistics/Bootstrap.py from-to:^# In\\[8\\]:@^# end8 -->\n",
    "\n",
    " This shows that the bootstrap estimate is better here than the MLE\n",
    "plugin estimate.\n",
    "\n",
    "Note that this technique becomes even more powerful with multivariate\n",
    "distributions with many parameters because all the mechanics are the same.\n",
    "Thus, the bootstrap is a great all-purpose method for computing standard\n",
    "errors, but, in the limit,  is it converging to the correct value? This is the\n",
    "question of *consistency*. Unfortunately, to answer this question requires more\n",
    "and deeper mathematics than we can get into here.  The short answer is that for\n",
    "estimating standard errors, the bootstrap is a consistent estimator in a wide\n",
    "range of cases and so it definitely belongs in your toolkit."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
