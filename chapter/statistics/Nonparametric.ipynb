{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import textwrap\n",
    "import sys, re\n",
    "old_displayhook = sys.displayhook\n",
    "def displ(x):\n",
    "   if x is None: return\n",
    "   print (\"\\n\".join(textwrap.wrap(repr(x).replace(' ',''),width=80)))\n",
    "\n",
    "sys.displayhook=displ\n",
    "sys.path.append('src-statistics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have considered parametric methods that reduce inference\n",
    "or prediction to parameter-fitting. However, for these to work, we had to\n",
    "assume a specific functional form for the unknown probability distribution of\n",
    "the data. Nonparametric methods eliminate the need to assume a specific\n",
    "functional form by generalizing to classes of functions.\n",
    "\n",
    "## Kernel Density Estimation\n",
    "\n",
    "We have already made heavy use of this method with the histogram, which is a\n",
    "special case of kernel density estimation.  The histogram can be considered the\n",
    "crudest and most useful nonparametric method, that estimates the underlying\n",
    "probability distribution of the data.\n",
    "\n",
    "To be formal and place the histogram on the same footing as our earlier\n",
    "estimations, suppose that $\\mathscr{X}=[0,1]^d$ is the $d$ dimensional unit\n",
    "cube and that $h$ is the *bandwidth* or size of a *bin* or sub-cube. Then,\n",
    "there are $N\\approx(1/h)^d$ such bins, each with volume $h^d$, $\\lbrace\n",
    "B_1,B_2,\\ldots,B_N \\rbrace$. With all this in place, we can write the histogram\n",
    "has a probability density estimator of the form,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{p}_h(x) = \\sum_{k=1}^N \\frac{\\hat{\\theta}_k}{h} I(x\\in B_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\theta}_k=\\frac{1}{n} \\sum_{j=1}^n I(X_j\\in B_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " is the fraction of data points ($X_k$) in each bin, $B_k$. We want to\n",
    "bound the bias and variance of $\\hat{p}_h(x)$. Keep in mind that we are trying\n",
    "to estimate a function of $x$, but the set of all possible probability\n",
    "distribution functions is extremely large and hard to manage. Thus, we need\n",
    "to restrict our attention to the following class of probability distribution of\n",
    "so-called Lipschitz functions,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathscr{P}(L) = \\lbrace p\\colon \\vert p(x)-p(y)\\vert \\le L \\Vert x-y\\Vert, \\forall \\: x,y \\rbrace\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Roughly speaking, these are the density\n",
    "functions whose slopes (i.e., growth rates)  are bounded by $L$.\n",
    "It turns out that the bias of the histogram estimator is bounded in the\n",
    "following way,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\int\\vert p(x)-\\mathbb{E}(\\hat{p}_h(x))\\vert dx \\le L h\\sqrt{d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Similarly, the variance is bounded by the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{V}(\\hat{p}_h(x)) \\le \\frac{C}{n h^d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " for some constant $C$. Putting these two facts together means that the\n",
    "risk is bounded by,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "R(p,\\hat{p}) = \\int \\mathbb{E}(p(x) -\\hat{p}_h(x))^2 dx \\le L^2 h^2 d + \\frac{C}{n h^d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This upper bound is minimized by choosing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h = \\left(\\frac{C}{L^2 n d}\\right)^\\frac{1}{d+2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In particular, this means that,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sup_{p\\in\\mathscr{P}(L)} R(p,\\hat{p}) \\le C_0 \\left(\\frac{1}{n}\\right)^{\\frac{2}{d+2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where the constant $C_0$ is a function of $L$. There is a theorem\n",
    "[[wasserman2004all]](#wasserman2004all) that shows this bound in tight, which basically means\n",
    "that the histogram is a really powerful probability density estimator for\n",
    "Lipschitz functions with risk that goes as\n",
    "$\\left(\\frac{1}{n}\\right)^{\\frac{2}{d+2}}$.  Note that this class of functions\n",
    "is not necessarily smooth because the Lipschitz condition admits \n",
    "non-smooth functions. While this is a reassuring result, we typically do\n",
    "not know which function class (Lipschitz or not) a particular probability\n",
    "belongs to ahead of time.  Nonetheless, the rate at which the risk changes with\n",
    "both dimension $d$ and $n$ samples would be hard to understand without this\n",
    "result.  [Figure](#fig:nonparametric_001) shows the probability distribution\n",
    "function of the $\\beta(2,2)$ distribution compared to computed histograms for\n",
    "different values of $n$. The box plots on each of the points show how the\n",
    "variation in each bin of the histogram reduces with increasing $n$. The risk\n",
    "function $R(p,\\hat{p})$ above is based upon integrating the squared difference\n",
    "between the histogram (as a piecewise function of $x$) and the probability\n",
    "distribution function. \n",
    "\n",
    "**Programming Tip.**\n",
    "\n",
    "The following snippet is the main element of the code for [Figure](#fig:nonparametric_001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_samples(n,ntrials=500):\n",
    "    phat = np.zeros((nbins,ntrials))\n",
    "    for k in range(ntrials):\n",
    "        d = rv.rvs(n)       \n",
    "        phat[:,k],_=histogram(d,bins,density=True)   \n",
    "    return phat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code uses the `histogram` function from Numpy.\n",
    "To be consistent with the risk function $R(p,\\hat{p})$, we have to make sure\n",
    "the `bins` keyword argument is formatted correctly using a sequence of\n",
    "bin-edges instead of just a single integer. Also, the `density=True` keyword\n",
    "argument normalizes the histogram appropriately so that the comparison between\n",
    "it and the probability distribution function of the simulated beta distribution\n",
    "is correctly scaled.\n",
    "\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig-statistics/nonparametric_001.png, width=800 frac=0.95]  The box plots on each of the points show how the variation in each bin of the histogram reduces with increasing $n$. <div id=\"fig:nonparametric_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nonparametric_001\"></div>\n",
    "\n",
    "<p>The box plots on each of the points show how the variation in each bin of the histogram reduces with increasing $n$.</p>\n",
    "<img src=\"fig-statistics/nonparametric_001.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "## Kernel Smoothing\n",
    "\n",
    "We can extend our methods to other function classes using kernel functions.\n",
    "A one-dimensional smoothing kernel is a smooth function $K$ with \n",
    "the following properties,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\int K(x) dx &= 1 \\\\\\\n",
    "\\int x K(x) dx &= 0 \\\\\\\n",
    "0< \\int x^2 K(x) dx &< \\infty \\\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For example, $K(x)=I(x)/2$ is the boxcar kernel, where $I(x)=1$\n",
    "when $\\vert x\\vert\\le 1$ and zero otherwise. The kernel density estimator is\n",
    "very similar to the histogram, except now we put a kernel function on every\n",
    "point as in the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{p}(x)=\\frac{1}{n}\\sum_{i=1}^n \\frac{1}{h^d} K\\left(\\frac{\\Vert x-X_i\\Vert}{h}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $X\\in \\mathbb{R}^d$. [Figure](#fig:nonparametric_002) shows an\n",
    "example of a kernel density estimate using a Gaussian kernel function,\n",
    "$K(x)=e^{-x^2/2}/\\sqrt{2\\pi}$. There are  five data points shown by the\n",
    "vertical lines in the upper panel. The dotted lines show the individual $K(x)$\n",
    "function at each of the data points. The lower panel shows the overall kernel\n",
    "density estimate, which is the scaled sum of the upper panel.\n",
    "\n",
    "There is an important technical result in [[wasserman2004all]](#wasserman2004all) that\n",
    "states that kernel density estimators are minimax in the sense we\n",
    "discussed in the maximum likelihood the section [ch:stats:sec:mle](#ch:stats:sec:mle). In\n",
    "broad strokes, this means that the analogous risk for the kernel\n",
    "density estimator is approximately bounded by the following factor,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "R(p,\\hat{p}) \\lesssim n^{-\\frac{2 m}{2 m+d}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " for some constant $C$ where $m$ is a factor related to bounding\n",
    "the derivatives of the probability density function. For example, if the second\n",
    "derivative of the density function is bounded, then $m=2$. This means that\n",
    "the convergence rate for this estimator decreases with increasing dimension\n",
    "$d$.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-statistics/nonparametric_002.png, width=800 frac=0.95]  The upper panel shows the individual kernel functions placed at each of the data points. The lower panel shows the composite kernel density estimate which is the sum of the individual functions in the upper panel. <div id=\"fig:nonparametric_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nonparametric_002\"></div>\n",
    "\n",
    "<p>The upper panel shows the individual kernel functions placed at each of the data points. The lower panel shows the composite kernel density estimate which is the sum of the individual functions in the upper panel.</p>\n",
    "<img src=\"fig-statistics/nonparametric_002.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "As a practical matter, the tricky part of the kernel density estimator (which\n",
    "includes the histogram as a special case) is that we need to somehow compute\n",
    "the bandwidth $h$ term using data. There are several rule-of-thumb methods that\n",
    "for some common kernels, including Silverman's rule and Scott's rule for\n",
    "Gaussian kernels. For example, Scott's factor is to simply compute $h=n^{\n",
    "-1/(d+4) }$ and Silverman's is $h=(n (d+2)/4)^{ (-1/(d+4)) }$. Rules of\n",
    "this kind are derived by assuming the underlying probability density\n",
    "function is of a certain family (e.g., Gaussian), and then deriving the\n",
    "best $h$ for a certain type of kernel density estimator, usually equipped\n",
    "with extra functional properties (say, continuous derivatives of a\n",
    "certain order). In practice, these rules seem to work pretty well,\n",
    "especially for uni-modal probability density functions.  Avoiding these\n",
    "kinds of assumptions means computing the bandwidth from data directly and that is where\n",
    "cross validation comes in.\n",
    "\n",
    "Cross-validation is a method to estimate the bandwidth from the data itself.\n",
    "The idea is to write out the following Integrated Squared Error (ISE),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\textnormal{ISE}(\\hat{p}_h,p)&=\\int (p(x)-\\hat{p}_h(x))^2 dx\\\\\\\n",
    "                             &= \\int \\hat{p}_h(x)^2 dx - 2\\int p(x) \\hat{p}_h dx + \\int p(x)^2 dx \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The problem with this expression is the middle term [^last_term],\n",
    "\n",
    "[^last_term]: The last term is of no interest because we are\n",
    "only interested in relative changes in the ISE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\int p(x)\\hat{p}_h dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $p(x)$ is what we are trying to estimate with $\\hat{p}_h$. The\n",
    "form of the last expression looks like an expectation of $\\hat{p}_h$ over the\n",
    "density of $p(x)$, $\\mathbb{E}(\\hat{p}_h)$. The approach is to\n",
    "approximate this with the mean,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(\\hat{p}_h) \\approx \\frac{1}{n}\\sum_{i=1}^n \\hat{p}_h(X_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The problem with this approach is that $\\hat{p}_h$ is computed using\n",
    "the same data that the approximation utilizes.  The way to get around this is\n",
    "to split the data into two equally sized chunks $D_1$, $D_2$; and then compute\n",
    "$\\hat{p}_h$ for a sequence of different $h$ values over the $D_1$ set. Then,\n",
    "when we apply the above approximation for the data ($Z_i$) in the $D_2$ set,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(\\hat{p}_h) \\approx \\frac{1}{\\vert D_2\\vert}\\sum_{Z_i\\in D_2} \\hat{p}_h(Z_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Plugging this approximation back into the integrated squared error\n",
    "provides the objective function,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textnormal{ISE}\\approx \\int \\hat{p}_h(x)^2 dx-\\frac{2}{\\vert D_2\\vert}\\sum_{Z_i\\in D_2} \\hat{p}_h(Z_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Some code will make these steps concrete. We will need some tools from\n",
    "Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors.kde import KernelDensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `train_test_split` function makes it easy to split and\n",
    "keep track of the $D_1$ and $D_2$ sets we need for cross validation. Scikit-learn\n",
    "already has a powerful and flexible implementation of kernel density estimators.\n",
    "To compute the objective function, we need some\n",
    "basic numerical integration tools from Scipy. For this example, we\n",
    "will generate samples from a $\\beta(2,2)$ distribution, which is\n",
    "implemented in the `stats` submodule in Scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "from scipy import stats\n",
    "rv= stats.beta(2,2)\n",
    "n=100                 # number of samples to generate\n",
    "d = rv.rvs(n)[:,None] # generate samples as column-vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The use of the `[:,None]` in the last line formats the Numpy array returned by\n",
    "the `rvs` function into a Numpy vector with a column dimension of one. This is\n",
    "required by the `KernelDensity` constructor because the column dimension is\n",
    "used for different features (in general) for Scikit-learn. Thus, even though we\n",
    "only have one feature, we still need to comply with the structured input that\n",
    "Scikit-learn relies upon. There are many ways to inject the additional\n",
    "dimension other than using `None`. For example, the more cryptic, `np.c_`, or\n",
    "the less cryptic `[:,np.newaxis]` can do the same, as can the `np.reshape`\n",
    "function.\n",
    "\n",
    "\n",
    "\n",
    " The next step is to split the data into two  halves and loop over\n",
    "each of the $h_i$ bandwidths to create a separate kernel density estimator\n",
    "based on the $D_1$ data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train,test,_,_=train_test_split(d,d,test_size=0.5)\n",
    "kdes=[KernelDensity(bandwidth=i).fit(train) \n",
    "        for i in [.05,0.1,0.2,0.3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "Note that the single underscore symbol in Python refers to the last evaluated\n",
    "result.  the above code unpacks the tuple returned by `train_test_split` into\n",
    "four elements.  Because we are only interested in the first two, we assign the\n",
    "last two to the underscore symbol. This is a stylistic usage to make it clear\n",
    "to the reader that the last two elements of the tuple are unused.\n",
    "Alternatively, we could assign the last two elements to a pair of dummy\n",
    "variables that we do not use later, but then  the reader skimming the code may\n",
    "think that those dummy variables are relevant.\n",
    "\n",
    "\n",
    "\n",
    " The last step is to loop over the so-created kernel density estimators\n",
    "and compute the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for i in kdes:\n",
    "    f = lambda x: np.exp(i.score_samples(x))\n",
    "    f2 = lambda x: f([[x]])**2\n",
    "    print('h=%3.2f\\t %3.4f'%(i.bandwidth,quad(f2,0,1)[0]\n",
    "          -2*np.mean(f(test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The lambda functions defined in the last block are necessary because\n",
    "Scikit-learn implements the return value of the kernel density estimator as a\n",
    "logarithm via the `score_samples` function. The numerical quadrature function\n",
    "`quad` from Scipy computes the $\\int \\hat{p}_h(x)^2 dx$ part of the objective\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division\n",
    "from matplotlib.pylab import subplots\n",
    "fig,ax=subplots()\n",
    "xi = np.linspace(0,1,100)[:,None]\n",
    "for i in kdes:\n",
    "    f=lambda x: np.exp(i.score_samples(x))\n",
    "    f2 = lambda x: f(x)**2\n",
    "    _=ax.plot(xi,f(xi),label='$h$='+str(i.bandwidth))\n",
    "\n",
    "_=ax.set_xlabel('$x$',fontsize=28)\n",
    "_=ax.set_ylabel('$y$',fontsize=28)\n",
    "_=ax.plot(xi,rv.pdf(xi),'k:',lw=3,label='true')\n",
    "_=ax.legend(loc=0)\n",
    "ax2 = ax.twinx()\n",
    "_=ax2.hist(d,20,alpha=.3,color='gray')\n",
    "_=ax2.axis(ymax=50)\n",
    "_=ax2.set_ylabel('count',fontsize=28)\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-statistics/nonparametric_003.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-statistics/nonparametric_003.png, width=800 frac=0.85]  Each line above is a different kernel density estimator for the given bandwidth as an approximation to the true density function. A plain histogram is imprinted on the bottom for reference.  <div id=\"fig:nonparametric_003\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nonparametric_003\"></div>\n",
    "\n",
    "<p>Each line above is a different kernel density estimator for the given bandwidth as an approximation to the true density function. A plain histogram is imprinted on the bottom for reference.</p>\n",
    "<img src=\"fig-statistics/nonparametric_003.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "Scikit-learn has many more advanced tools to automate this kind of\n",
    "hyper-parameter (i.e., kernel density bandwidth) search. To utilize these\n",
    "advanced tools, we need to format the current problem slightly differently by\n",
    "defining the following wrapper class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class KernelDensityWrapper(KernelDensity):\n",
    "    def predict(self,x):\n",
    "        return np.exp(self.score_samples(x))\n",
    "    def score(self,test):\n",
    "        f = lambda x: self.predict(x)\n",
    "        f2 = lambda x: f([[x]])**2\n",
    "        return -(quad(f2,0,1)[0]-2*np.mean(f(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is tantamount to reorganizing the above previous code \n",
    "into functions that Scikit-learn requires. Next, we create the\n",
    "dictionary of parameters we want to search over (`params`) below\n",
    "and then start the grid search with the `fit` function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'bandwidth':np.linspace(0.01,0.5,10)}\n",
    "clf = GridSearchCV(KernelDensityWrapper(), param_grid=params,cv=2)\n",
    "clf.fit(d)\n",
    "print (clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The grid search iterates over all the elements in the `params`\n",
    "dictionary and reports the best bandwidth over that list of parameter values.\n",
    "The `cv` keyword argument above specifies that we want to split the data\n",
    "into two equally-sized sets for training and testing. We can\n",
    "also examine the values of the objective function for each point\n",
    "on the grid as follow,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Keep in mind that the grid search examines multiple folds for cross\n",
    "validation to compute the above means and standard deviations. Note that there\n",
    "is also a `RandomizedSearchCV` in case you would rather specify a distribution\n",
    "of parameters instead of a list. This is particularly useful for searching very\n",
    "large parameter spaces where an exhaustive grid search would be too\n",
    "computationally expensive. Although kernel density estimators are easy to\n",
    "understand and have many attractive analytical properties, they become\n",
    "practically prohibitive for large, high-dimensional data sets.\n",
    "\n",
    "## Nonparametric Regression Estimators\n",
    "\n",
    "Beyond estimating the underlying probability density, we can use nonparametric\n",
    "methods to compute estimators of the underlying function that is generating the\n",
    "data.  Nonparametric regression estimators of the following form are known as\n",
    "linear smoothers,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y}(x) =  \\sum_{i=1}^n \\ell_i(x) y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To understand the performance of these smoothers,\n",
    "we can define the risk as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "R(\\hat{y},y) = \\mathbb{E}\\left( \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}(x_i)-y(x_i))^2 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and find the best $\\hat{y}$ that minimizes this. The problem with\n",
    "this metric is that we do not know $y(x)$, which is why we are trying to\n",
    "approximate it with $\\hat{y}(x)$. We could construct an estimation by using the\n",
    "data at hand as in the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{R}(\\hat{y},y) =\\frac{1}{n} \\sum_{i=1}^n (\\hat{y}(x_i)-Y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where we have substituted the data $Y_i$ for the unknown function\n",
    "value, $y(x_i)$. The problem with this approach is that we are using the data\n",
    "to estimate the function and then using the same data to evaluate the risk of\n",
    "doing so. This kind of double-dipping leads to overly optimistic estimators.\n",
    "One way out of this conundrum is to use leave-one-out cross validation, wherein\n",
    "the $\\hat{y}$ function is estimated using all but one of the data pairs,\n",
    "$(X_i,Y_i)$. Then, this missing data element is used to estimate the above\n",
    "risk. Notationally, this is written as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{R}(\\hat{y},y) =\\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_{(-i)}(x_i)-Y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\hat{y}_{(-i)}$ denotes computing the estimator without using\n",
    "the $i^{th}$ data pair. Unfortunately, for anything other than relatively small\n",
    "data sets, it quickly becomes computationally prohibitive to use leave-one-out\n",
    "cross validation in practice. We'll get back to this issue shortly, but let's\n",
    "consider a concrete example of such a nonparametric smoother.\n",
    "\n",
    "## Nearest Neighbors Regression\n",
    "<div id=\"ch:stats:sec:nnreg\"></div>\n",
    "\n",
    "The simplest possible nonparametric regression method is the $k$-nearest\n",
    "neighbors regression. This is easier to explain in words than to write out in\n",
    "math. Given an input $x$, find the closest one of the $k$ clusters that\n",
    "contains it and then return the mean of the data values in that cluster. As a\n",
    "univariate example, let's consider the following *chirp* waveform,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y(x)=\\cos\\left(2\\pi\\left(f_o x + \\frac{BW x^2}{2\\tau}\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This waveform is important in high-resolution radar applications.\n",
    "The $f_o$ is the start frequency and $BW/\\tau$ is the frequency slope of the\n",
    "signal. For our example, the fact that it is nonuniform over its domain is\n",
    "important. We can easily create some data by sampling the\n",
    "chirp as in the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import cos, pi\n",
    "xi = np.linspace(0,1,100)[:,None]\n",
    "xin = np.linspace(0,1,12)[:,None]\n",
    "f0 = 1 # init frequency\n",
    "BW = 5\n",
    "y = np.cos(2*pi*(f0*xin+(BW/2.0)*xin**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can use this data to construct a simple nearest neighbor\n",
    "estimator using Scikit-learn,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knr=KNeighborsRegressor(2) \n",
    "knr.fit(xin,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "Scikit-learn has a fantastically consistent interface. The `fit` function above\n",
    "fits the model parameters to the data. The corresponding `predict` function\n",
    "returns the output of the model given an arbitrary input. We will spend a lot\n",
    "more time on Scikit-learn in the machine learning chapter. The `[:,None]` part\n",
    "at the end is just injecting a column dimension into the array in order to\n",
    "satisfy the dimensional requirements of Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.pylab import subplots\n",
    "fig,ax=subplots()\n",
    "yi = cos(2*pi*(f0*xi+(BW/2.0)*xi**2))\n",
    "_=ax.plot(xi,yi,'k--',lw=2,label=r'$y(x)$')\n",
    "_=ax.plot(xin,y,'ko',lw=2,ms=11,color='gray',alpha=.8,label='$y(x_i)$')\n",
    "_=ax.fill_between(xi.flat,yi.flat,knr.predict(xi).flat,color='gray',alpha=.3)\n",
    "_=ax.plot(xi,knr.predict(xi),'k-',lw=2,label='$\\hat{y}(x)$')\n",
    "_=ax.set_aspect(1/4.)\n",
    "_=ax.axis(ymax=1.05,ymin=-1.05)\n",
    "_=ax.set_xlabel(r'$x$',fontsize=24)\n",
    "_=ax.legend(loc=0)\n",
    "fig.set_tight_layout(True)\n",
    "fig.savefig('fig-statistics/nonparametric_004.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-statistics/nonparametric_004.png, width=800 frac=0.85] The dotted line shows the chirp signal and the solid line shows the nearest neighbor estimate. The gray circles are the sample points that we used to fit the nearest neighbor estimator. The shaded area shows the gaps between the estimator and the unsampled chirp.  <div id=\"fig:nonparametric_004\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nonparametric_004\"></div>\n",
    "\n",
    "<p>The dotted line shows the chirp signal and the solid line shows the nearest neighbor estimate. The gray circles are the sample points that we used to fit the nearest neighbor estimator. The shaded area shows the gaps between the estimator and the unsampled chirp.</p>\n",
    "<img src=\"fig-statistics/nonparametric_004.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    " [Figure](#fig:nonparametric_004) shows the sampled signal (gray\n",
    "circles) against the values generated by the nearest neighbor estimator (solid\n",
    "line). The dotted line is the full unsampled chirp signal, which increases in\n",
    "frequency with $x$. This is important for our example because it adds a\n",
    "non-stationary aspect to this problem in that the function gets progressively\n",
    "wigglier with increasing $x$.  The area between the estimated curve and the\n",
    "signal is shaded in gray. Because the nearest neighbor estimator uses only two\n",
    "nearest neighbors, for each new $x$, it finds the two adjacent $X_i$ that\n",
    "bracket the $x$ in the training data and then averages the corresponding $Y_i$\n",
    "values to compute the estimated value. That is, if you take every adjacent pair\n",
    "of sequential gray circles in the Figure, you find that the horizontal solid line \n",
    "splits the pair on the vertical axis. We can adjust the number of\n",
    "nearest neighbors  by changing the constructor,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knr=KNeighborsRegressor(3) \n",
    "knr.fit(xin,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "_=ax.plot(xi,yi,'k--',lw=2,label=r'$y(x)$')\n",
    "_=ax.plot(xin,y,'ko',lw=2,ms=11,color='gray',alpha=.8,label='$y(x_i)$')\n",
    "_=ax.fill_between(xi.flat,yi.flat,knr.predict(xi).flat,color='gray',alpha=.3)\n",
    "_=ax.plot(xi,knr.predict(xi),'k-',lw=2,label='$\\hat{y}(x)$')\n",
    "_=ax.set_aspect(1/4.)\n",
    "_=ax.axis(ymax=1.05,ymin=-1.05)\n",
    "_=ax.set_xlabel(r'$x$',fontsize=24)\n",
    "_=ax.legend(loc=0)\n",
    "fig.set_tight_layout(True)\n",
    "fig.savefig('fig-statistics/nonparametric_005.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which produces the following corresponding [Figure](#fig:nonparametric_005).\n",
    "\n",
    "<!-- dom:FIGURE: [fig-statistics/nonparametric_005.png, width=800 frac=0.85] This is the same as [Figure](#fig:nonparametric_004) except that here there are three nearest neighbors used to build the estimator.  <div id=\"fig:nonparametric_005\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nonparametric_005\"></div>\n",
    "\n",
    "<p>This is the same as [Figure](#fig:nonparametric_004) except that here there are three nearest neighbors used to build the estimator.</p>\n",
    "<img src=\"fig-statistics/nonparametric_005.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "For this example, [Figure](#fig:nonparametric_005) shows that with\n",
    "more nearest neighbors the fit performs poorly, especially towards the end of\n",
    "the signal, where there is increasing variation, because the chirp is not\n",
    "uniformly continuous.\n",
    "\n",
    "Scikit-learn provides many tools for cross validation. The following code\n",
    "sets up the tools for leave-one-out cross validation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo=LeaveOneOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `LeaveOneOut` object is an iterable that produces a set of\n",
    "disjoint indices of the data --- one for fitting the model (training set) and\n",
    "one for evaluating the model (testing set).  The next block loops over the\n",
    "disjoint sets of training and test indicies iterates provided by the `loo`\n",
    "variable to evaluate the estimated risk, which is accumulated in the `out`\n",
    "list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out=[]\n",
    "for train_index, test_index in loo.split(xin):\n",
    "    _=knr.fit(xin[train_index],y[train_index])\n",
    "    out.append((knr.predict(xi[test_index])-y[test_index])**2)\n",
    "\n",
    "print( 'Leave-one-out Estimated Risk: ',np.mean(out),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The last line in the code above reports leave-one-out's estimated\n",
    "risk.  \n",
    "\n",
    "Linear smoothers of this type can be rewritten in using the following matrix,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathscr{S} = \\left[ \\ell_i(x_j) \\right]_{i,j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " so that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\mathbf{y}} = \\mathscr{S} \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   where $\\mathbf{y}=\\left[Y_1,Y_2,\\ldots,Y_n\\right]\\in \\mathbb{R}^n$\n",
    "and $\\hat{ \\mathbf{y}\n",
    "}=\\left[\\hat{y}(x_1),\\hat{y}(x_2),\\ldots,\\hat{y}(x_n)\\right]\\in \\mathbb{R}^n$.\n",
    "This leads to a quick way to approximate leave-one-out cross validation as the\n",
    "following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{R}=\\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-\\hat{y}(x_i)}{1-\\mathscr{S}_{i,i}}\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " However, this does not reproduce the approach in the code above\n",
    "because it assumes that each $\\hat{y}_{(-i)}(x_i)$ is consuming one fewer\n",
    "nearest neighbor than $\\hat{y}(x)$.\n",
    "\n",
    "We can get this $\\mathscr{S}$ matrix from the `knr` object as in the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_= knr.fit(xin,y) # fit on all data\n",
    "S=(knr.kneighbors_graph(xin)).todense()/float(knr.n_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `todense` part reformats the sparse matrix that is\n",
    "returned into a regular Numpy `matrix`. The following shows a subsection\n",
    "of this $\\mathcal{S}$ matrix,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(S[:5,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The sub-blocks show the windows of the the `y` data that are being\n",
    "processed by the nearest neighbor estimator. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.hstack([knr.predict(xin[:5]),(S*y)[:5]]))#columns match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Or, more concisely checking all entries for approximate equality,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.allclose(knr.predict(xin),S*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which shows that the results from the nearest neighbor\n",
    "object and the matrix multiply match.\n",
    "\n",
    "**Programming Tip.**\n",
    "\n",
    "Note that because we formatted the returned $\\mathscr{S}$ as a Numpy matrix, we\n",
    "automatically get the matrix multiplication instead of default element-wise\n",
    "multiplication in the `S*y` term.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## Kernel Regression\n",
    "\n",
    "For estimating the probability density, we started with the histogram and moved\n",
    "to the more general kernel density estimate. Likewise, we can also extend\n",
    "regression from nearest neighbors to kernel-based regression using the\n",
    "*Nadaraya-Watson* kernel regression estimator.  Given a bandwidth $h>0$, the\n",
    "kernel regression estimator is defined as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y}(x)=\\frac{\\sum_{i=1}^n K\\left(\\frac{x-x_i}{h}\\right) Y_i}{\\sum_{i=1}^n K \\left( \\frac{x-x_i}{h} \\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Unfortunately, Scikit-learn does not implement this\n",
    "regression estimator; however, Jan Hendrik Metzen  makes a compatible\n",
    "version available on `github.com`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src-statistics')\n",
    "xin = np.linspace(0,1,20)[:,None]\n",
    "y = cos(2*pi*(f0*xin+(BW/2.0)*xin**2)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kernel_regression import KernelRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This code makes it possible to internally optimize over the bandwidth\n",
    "parameter using leave-one-out cross validation by specifying a grid of\n",
    "potential bandwidth values (`gamma`), as in the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kr = KernelRegression(gamma=np.linspace(6e3,7e3,500))\n",
    "kr.fit(xin,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Figure](#fig:nonparametric_006) shows the kernel estimator (heavy\n",
    "black line) using the Gaussian kernel compared to the nearest neighbor\n",
    "estimator (solid light black line). As before, the data points are shown as\n",
    "circles. [Figure](#fig:nonparametric_006) shows that the kernel estimator can\n",
    "pick out the sharp peaks that are missed by the nearest neighbor estimator.  \n",
    "\n",
    "<!-- dom:FIGURE: [fig-statistics/nonparametric_006.png, width=800 frac=0.85] The heavy black line is the Gaussian kernel estimator. The light black line is the nearest neighbor estimator. The data points are shown as gray circles. Note that unlike the nearest neighbor estimator, the Gaussian kernel estimator is able to pick out the sharp peaks in the training data.  <div id=\"fig:nonparametric_006\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nonparametric_006\"></div>\n",
    "\n",
    "<p>The heavy black line is the Gaussian kernel estimator. The light black line is the nearest neighbor estimator. The data points are shown as gray circles. Note that unlike the nearest neighbor estimator, the Gaussian kernel estimator is able to pick out the sharp peaks in the training data.</p>\n",
    "<img src=\"fig-statistics/nonparametric_006.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "Thus, the difference between nearest neighbor and kernel estimation is that the\n",
    "latter provides a smooth moving averaging of points whereas the former provides\n",
    "a discontinuous averaging. Note that kernel estimates suffer near the\n",
    "boundaries where there is mismatch between the edges and the kernel\n",
    "function. This problem gets worse in higher dimensions because the data\n",
    "naturally drift towards the boundaries (this is a consequence of the *curse of\n",
    "dimensionality*). Indeed, it is not possible to simultaneously maintain local\n",
    "accuracy (i.e., low bias) and a generous neighborhood (i.e., low variance). One\n",
    "way to address this problem is to create a local polynomial regression using\n",
    "the kernel function as a window to localize a region of interest. For example,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y}(x)=\\sum_{i=1}^n K\\left(\\frac{x-x_i}{h}\\right) (Y_i-\\alpha - \\beta x_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and now we have to optimize over the two linear parameters $\\alpha$\n",
    "and $\\beta$. This method is known as *local linear regression*\n",
    "[[loader2006local]](#loader2006local), [[hastie2013elements]](#hastie2013elements). Naturally, this can be\n",
    "extended to higher-order polynomials. Note that these methods are not yet\n",
    "implemented in Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "#fig.set_size_inches((12,4))\n",
    "_=ax.plot(xi,kr.predict(xi),'k-',label='kernel',lw=3)\n",
    "_=ax.plot(xin,y,'o',lw=3,color='gray',ms=12)\n",
    "_=ax.plot(xi,yi,'--',color='gray',label='chirp')\n",
    "_=ax.plot(xi,knr.predict(xi),'k-',label='nearest')\n",
    "_=ax.axis(ymax=1.1,ymin=-1.1)\n",
    "_=ax.set_aspect(1/4.)\n",
    "_=ax.axis(ymax=1.05,ymin=-1.05)\n",
    "_=ax.set_xlabel(r'$x$',fontsize=24)\n",
    "_=ax.set_ylabel(r'$y$',fontsize=24)\n",
    "_=ax.legend(loc=0)\n",
    "fig.savefig('fig-statistics/nonparametric_006.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.displayhook= old_displayhook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # #ifdef SINGLE -->\n",
    "<!-- TITLE:  Curse of Dimensionality -->\n",
    "<!-- AUTHOR: Jose Unpingco -->\n",
    "<!-- DATE: today -->\n",
    "<!-- # #endif -->\n",
    "\n",
    "The so-called curse of dimensionality occurs as we move into higher and higher\n",
    "dimensions. The term was coined by Bellman in 1961 while he was studying\n",
    "adaptive control processes. Nowadays, the term is vaguely refers to anything\n",
    "that becomes more complicated as the number of dimensions increases\n",
    "substantially. Nevertheless, the concept is useful for recognizing\n",
    "and characterizing the practical difficulties of high-dimensional analysis and\n",
    "estimation.\n",
    "\n",
    "Consider the volume of an $d$-dimensional sphere of radius $r$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "V_s(d,r)=\\frac{\\pi ^{d/2} r^d}{\\Gamma \\left(\\frac{d}{2}+1\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Further, consider the sphere $V_s(d,1/2)$ enclosed by an $d$\n",
    "dimensional unit cube.  The volume of the cube is always equal to one, but\n",
    "$\\lim_{d\\rightarrow\\infty} V_s(d,1/2) = 0$. What does this mean? It means that\n",
    "the volume of the cube is pushed away from its center, where the embedded\n",
    "hypersphere lives.  Specifically, the distance from the center of the cube to\n",
    "its vertices in $d$ dimensions is $\\sqrt{d}/2$, whereas the distance from the\n",
    "center of the inscribing sphere is $1/2$. This diagonal distance goes to\n",
    "infinity as $d$ does. For a fixed $d$, the tiny spherical region at the center\n",
    "of the cube has many long spines attached to it, like a hyper-dimensional sea\n",
    "urchin or porcupine.\n",
    "\n",
    "Another way to think about this is to consider the $\\epsilon>0$ thick peel of the\n",
    "hypersphere,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{P}_{\\epsilon} =V_s(d,r) - V_s(d,r-\\epsilon)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then, we consider the following limit,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\lim_{d\\rightarrow\\infty}\\mathcal{P}_{\\epsilon}  =\\lim_{d\\rightarrow\\infty} V_s(d,r)\\left(1 - \\frac{V_s(d,r-\\epsilon)}{V_s(d,r)}\\right)  \n",
    "\\label{_auto1} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\\n",
    "                                                 =\\lim_{d\\rightarrow\\infty} V_s(d,r)\\left(1 -\\lim_{d\\rightarrow\\infty} \\left(\\frac{r-\\epsilon}{r}\\right)^d\\right)  \n",
    "\\label{_auto2} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\\n",
    "                                                 =\\lim_{d\\rightarrow\\infty} V_s(d,r)\n",
    "\\label{_auto3} \\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " So, in the limit, the volume of the $\\epsilon$-thick peel \n",
    "consumes the volume of the hypersphere.\n",
    "\n",
    "What are the consequences of this? For methods that rely on nearest\n",
    "neighbors, exploiting locality to lower bias becomes intractable. For\n",
    "example, suppose we have an $d$ dimensional space and a point near the\n",
    "origin we want to localize around. To estimate behavior around this\n",
    "point, we need to average the unknown function about this point, but\n",
    "in a high-dimensional space, the chances of finding neighbors to\n",
    "average are slim. Looked at from the opposing point of view, suppose\n",
    "we have a binary variable, as in the coin-flipping problem. If we have\n",
    "1000 trials, then, based on our earlier work, we can be confident\n",
    "about estimating the probability of heads.  Now, suppose we have 10\n",
    "binary variables.  Now we have $2^{ 10 }=1024$ vertices to estimate.\n",
    "If we had the same 1000 points, then at least 24 vertices would not\n",
    "get any data.  To keep the same resolution, we would need 1000 samples\n",
    "at each vertex for a grand total of $1000\\times 1024 \\approx 10^6$\n",
    "data points. So, for a ten fold increase in the number of variables,\n",
    "we now have about 1000 more data points to collect to maintain the\n",
    "same statistical resolution. This is the curse of dimensionality.\n",
    "\n",
    "Perhaps some code will clarify this. The following code generates samples in\n",
    "two dimensions that are plotted as points in [Figure](#fig:curse_of_dimensionality_001) with the inscribed circle in two\n",
    "dimensions. Note that for $d=2$ dimensions,  most of the points are contained\n",
    "in the circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "v=np.random.rand(1000,2)-1/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle\n",
    "from matplotlib.pylab import subplots\n",
    "fig,ax=subplots()\n",
    "fig.set_size_inches((5,5))\n",
    "_=ax.set_aspect(1)\n",
    "_=ax.scatter(v[:,0],v[:,1],color='gray',alpha=.3)\n",
    "_=ax.add_patch(Circle((0,0),0.5,alpha=.8,lw=3.,fill=False))\n",
    "fig.savefig('fig-statistics/curse_of_dimensionality_001.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-statistics/curse_of_dimensionality_001.pdf, width=800 frac=0.65] Two dimensional scatter of points randomly and independently uniformly distributed in the unit square. Note that most of the points are contained in the circle. Counter to intuition, this does not persist as the number of dimensions increases. <div id=\"fig:curse_of_dimensionality_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:curse_of_dimensionality_001\"></div>\n",
    "\n",
    "<p>Two dimensional scatter of points randomly and independently uniformly distributed in the unit square. Note that most of the points are contained in the circle. Counter to intuition, this does not persist as the number of dimensions increases.</p>\n",
    "<img src=\"fig-statistics/curse_of_dimensionality_001.pdf\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    " The next code block describes the core computation in\n",
    "[Figure](#fig:curse_of_dimensionality_002). For each of the dimensions, we\n",
    "create a set of uniformly distributed random variates along each dimension\n",
    "and then compute how close each $d$ dimensional vector is to the origin.\n",
    "Those that measure one half are those contained in the hypersphere. The\n",
    "histogram of each measurment is shown in the corresponding panel in the\n",
    "[Figure](#fig:curse_of_dimensionality_002). The dark vertical line shows the threshold value. Values to the left\n",
    "of this indicate the population that are contained in the hypersphere. Thus,\n",
    "[Figure](#fig:curse_of_dimensionality_002) shows that as $d$ increases,\n",
    "fewer points are contained in the inscribed hypersphere. The following\n",
    "code paraphrases  the content of [Figure](#fig:curse_of_dimensionality_002)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "for d in [2,3,5,10,20,50]:\n",
    "    v=np.random.rand(5000,d)-1/2.\n",
    "    ax.hist([np.linalg.norm(i) for i in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "siz = [ 2,3,5,10,20,50 ]\n",
    "fig,axs=subplots(3,2,sharex=True)\n",
    "fig.set_size_inches((10,6))\n",
    "for ax,k in zip(axs.flatten(),siz):\n",
    "    v=np.random.rand(5000,k)-1/2.\n",
    "    _=ax.hist([np.linalg.norm(i) for i in v],color='gray',density=True);\n",
    "    _=ax.vlines(0.5,0,ax.axis()[-1]*1.1,lw=3)\n",
    "    _=ax.set_title('$d=%d$'%k,fontsize=20)\n",
    "    _=ax.tick_params(labelsize='small',top=False,right=False)\n",
    "    _=ax.spines['top'].set_visible(False)\n",
    "    _=ax.spines['right'].set_visible(False)\n",
    "    _=ax.spines['left'].set_visible(False)\n",
    "    _=ax.yaxis.set_visible(False)\n",
    "    _=ax.axis(ymax=3.5)\n",
    "\n",
    "fig.set_tight_layout(True)\n",
    "fig.savefig('fig-statistics/curse_of_dimensionality_002.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-statistics/curse_of_dimensionality_002.pdf, width=800 frac=0.95] Each panel shows the histogram of lengths of uniformly distributed $d$ dimensional random vectors. The population to the left of the dark vertical line are those that are contained in the inscribed hypersphere. This shows that fewer points are contained in the hypersphere with increasing dimension. <div id=\"fig:curse_of_dimensionality_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:curse_of_dimensionality_002\"></div>\n",
    "\n",
    "<p>Each panel shows the histogram of lengths of uniformly distributed $d$ dimensional random vectors. The population to the left of the dark vertical line are those that are contained in the inscribed hypersphere. This shows that fewer points are contained in the hypersphere with increasing dimension.</p>\n",
    "<img src=\"fig-statistics/curse_of_dimensionality_002.pdf\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "## Nonparametric Tests\n",
    "\n",
    "\n",
    "Determining whether or not two sets of observations derive from the same\n",
    "underlying probability distribution is an important problem. The most popular\n",
    "way to do this is with a standard t-test, but that requires assumptions about\n",
    "normality that may be hard to justify, which leads to nonparametric methods can\n",
    "get at this questions without such assumptions.\n",
    "\n",
    "Let $V$ and $W$ be continuous random variables. The variable \n",
    "$V$ is *stochastically larger* than $W$ if,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(V\\ge x) \\ge \\mathbb{P}(W\\ge x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " for all $x\\in \\mathbb{R}$ with strict inequality for at least one\n",
    "$x$. The term *stochastically smaller* means the obverse of this. For example,\n",
    "the black line density function shown in [Figure](#fig:nonparametric_tests_001) is stochastically larger than the gray one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from matplotlib.pylab import subplots\n",
    "\n",
    "fig,ax=subplots()\n",
    "xi = np.linspace(0,2,100)\n",
    "_=ax.plot(xi,stats.norm(1,0.25).pdf(xi),lw=3,color='k')\n",
    "_=ax.plot(xi,stats.beta(2,4).pdf(xi),lw=3,color='gray')\n",
    "_=ax.spines['top'].set_visible(0)\n",
    "_=ax.spines['right'].set_visible(0)\n",
    "_=ax.tick_params(labelsize='medium',top=False,right=False)\n",
    "ax.set_aspect(1/2.5)\n",
    "\n",
    "fig.savefig('fig-statistics/nonparametric_tests_001.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-statistics/nonparametric_tests_001.png, width=800 frac=0.65] The black line density function is stochastically larger than the gray one. <div id=\"fig:nonparametric_tests_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nonparametric_tests_001\"></div>\n",
    "\n",
    "<p>The black line density function is stochastically larger than the gray one.</p>\n",
    "<img src=\"fig-statistics/nonparametric_tests_001.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "### The Mann-Whitney-Wilcoxon Test\n",
    "\n",
    "The Mann-Whitney-Wilcoxon Test approaches the following alternative hypotheses\n",
    "\n",
    "   * $H_0$ : $F(x) = G(x)$ for all $x$ versus\n",
    "\n",
    "   * $H_a$ : $F(x) \\ge G(x)$, $F$ stochastically greater than $G$.\n",
    "\n",
    "Suppose we have two data sets $X$ and $Y$ and we want to know if they are drawn\n",
    "from the same underlying probability distribution or if one is stochastically\n",
    "greater than the other. There are $n_x$ elements in $X$ and $n_y$ elements in\n",
    "$Y$. If we combine these two data sets and rank them, then, under the null\n",
    "hypothesis, any data element should be as likely as any other to be assigned\n",
    "any particular rank.  that is, the combined set $Z$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Z = \\lbrace X_1,\\ldots,X_{n_x}, Y_1,\\ldots,Y_{n_y} \\rbrace\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " contains $n=n_x+n_y$ elements. Thus, any assignment of $n_y$ ranks\n",
    "from the integers $\\lbrace 1,\\ldots,n \\rbrace$  to $\\lbrace Y_1,\\ldots,Y_{n_y}\n",
    "\\rbrace$ should be equally likely (i.e., $\\mathbb{P}={ \\binom{n}{n_y} }^{-1}$).\n",
    "Importantly, this property is independent of the $F$ distribution.\n",
    "\n",
    "That is, we can define the $U$ statistic as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "U_X =\\sum_{i=1}^{n_x}\\sum_{j=1}^{n_y}\\mathbb{I}(X_i\\ge Y_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\mathbb{I}(\\cdot)$ is the usual indicator function. For an\n",
    "interpretation, this counts the number of times that elements of $Y$ outrank\n",
    "elements of $X$.   For example, let us suppose that $X=\\lbrace 1,3,4,5,6\n",
    "\\rbrace$ and $Y=\\lbrace 2,7,8,10,11 \\rbrace$.  We can get a this in one move\n",
    "using Numpy broadcasting,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([ 1,3,4,5,6 ])\n",
    "y = np.array([2,7,8,10,11])\n",
    "U_X = (y <= x[:,None]).sum()\n",
    "U_Y = (x <= y[:,None]).sum()\n",
    "print (U_X, U_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "U_X+U_Y =\\sum_{i=1}^{n_x}\\sum_{j=1}^{n_y} \\mathbb{I}(Y_i\\ge X_j)+\\mathbb{I}(X_i\\ge Y_j)= n_x n_y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " because $\\mathbb{I}(Y_i\\ge X_j)+\\mathbb{I}(X_i\\ge Y_j)=1$.  We \n",
    "can verify this in Python,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ((U_X+U_Y) == len(x)*len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now that we can compute the $U_X$ statistic, we have to characterize it. Let us\n",
    "consider $U_X$. If $H_0$ is true, then $X$ and $Y$ are identically distributed\n",
    "random variables. Thus all $\\binom{n_x+n_y}{n_x}$ allocations of the\n",
    "$X$-variables in the ordered combined sample  are equally likely. Among these, \n",
    "there are $\\binom{n_x+n_y-1}{n_x}$ allocations have a $Y$ variable \n",
    "as the largest observation in the combined sample. For these, omitting this \n",
    "largest observation does not affect $U_X$ because it would not have been\n",
    "counted anyway. The other $\\binom{n_x+n_y-1}{n_x-1}$ allocations have \n",
    "an element of $X$ as the largest observation. Omitting this observation \n",
    "reduces $U_X$ by $n_y$.\n",
    "\n",
    "With all that, suppose $N_{n_x,n_y}(u)$ be the number of allocations of \n",
    "$X$ and $Y$ elements that result in $U_X=u$. Under $H_0$ situation \n",
    "of equally likely outcomes, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p_{n_x, n_y}(u) = \\mathbb{P}(U_X=u)=\\frac{N_{n_x,n_y}(u)}{\\binom{n_x+n_y}{n_x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From our previous discussion, we have the recursive relationship,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N_{n_x,n_y}(u) = N_{n_x,n_y-1}(u) + N_{n_x-1,n_y}(u-n_y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " After dividing all of this by $\\binom{n_x+n_y}{n_x}$ and using the\n",
    "$p_{n_x, n_y}(u)$ notation above, we obtain the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p_{n_x, n_y}(u) = \\frac{n_y}{n_x+n_y} p_{n_x,n_y-1}(u)+\\frac{n_x}{n_x+n_y} p_{n_x-1,n_y}(u-n_y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $0\\le u\\le n_x n_y$. To start this recursion, we need the\n",
    "following initial conditions,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray*}\n",
    "p_{0,n_y}(u_x=0) & = & 1 \\\\ \n",
    "p_{0,n_y}(u_x>0) & = & 0 \\\\ \n",
    "p_{n_x,0}(u_x=0) & = & 1 \\\\ \n",
    "p_{n_x,0}(u_x>0) & = & 0 \n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To see how this works in Python,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def prob(n,m,u):\n",
    "    if u<0: return 0\n",
    "    if n==0 or m==0:\n",
    "        return int(u==0)\n",
    "    else:\n",
    "        f = m/float(m+n)\n",
    "        return (f*prob(n,m-1,u) + \n",
    "                (1-f)*prob(n-1,m,u-m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " These are shown in [Figure](#fig:nonparametric_tests_002) and\n",
    "approach a normal distribution for large $n_x,n_y$, with the following \n",
    "mean and variance,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:ustatmv\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}(U) & = & \\frac{n_x n_y}{2} \\\\\n",
    "\\mathbb{V}(U) & = & \\frac{n_x n_y (n_x+n_y+1)}{12}\n",
    "\\end{eqnarray}\n",
    "\\label{eq:ustatmv} \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The variance becomes more complicated when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,axs=subplots(2,2)\n",
    "fig.tight_layout()\n",
    "ax=axs[0,0]\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "_=ax.stem([prob(2,2,i) for i in range(2*2+1)],linefmt='k-',markerfmt='ko',basefmt='k-')\n",
    "_=ax.set_title(r'$n_x=%d,n_y=%d$'%(2,2),fontsize=14)\n",
    "ax=axs[0,1]\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "_=ax.stem([prob(4,2,i) for i in range(4*2+1)],linefmt='k-',markerfmt='ko',basefmt='k-')\n",
    "_=ax.set_title(r'$n_x=%d,n_y=%d$'%(4,2),fontsize=14)\n",
    "ax=axs[1,0]\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "_=ax.stem([prob(6,7,i) for i in range(6*7+1)],linefmt='k-',markerfmt='ko',basefmt='k-')\n",
    "_=ax.set_title(r'$n_x=%d,n_y=%d$'%(6,7),fontsize=14)\n",
    "ax=axs[1,1]\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "_=ax.stem([prob(8,12,i) for i in range(8*12+1)],linefmt='k-',markerfmt='ko',basefmt='k-')\n",
    "_=ax.set_title(r'$n_x=%d,n_y=%d$'%(8,12),fontsize=14)\n",
    "fig.savefig('fig-statistics/nonparametric_tests_002.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-statistics/nonparametric_tests_002.png, width=800 frac=0.75] The normal approximation to the distribution improves with increasing $n_x, n_y$. <div id=\"fig:nonparametric_tests_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nonparametric_tests_002\"></div>\n",
    "\n",
    "<p>The normal approximation to the distribution improves with increasing $n_x, n_y$.</p>\n",
    "<img src=\"fig-statistics/nonparametric_tests_002.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "We are trying to determine whether or not one network configuration is faster\n",
    "than another. We obtain the following round-trip times for each of the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X=np.array([ 50.6,31.9,40.5,38.1,39.4,35.1,33.1,36.5,38.7,42.3 ])\n",
    "Y=np.array([ 28.8,30.1,18.2,38.5,44.2,28.2,32.9,48.8,39.5,30.7 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Because there are too few elements to use the\n",
    "`scipy.stats.mannwhitneyu` function (which internally uses the normal\n",
    "approximation to the U-statistic), we can use our custom function above, but\n",
    "first we need to compute the $U_X$ statistic using Numpy,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U_X = (Y <= X[:,None]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For the p-value, we want to compute the probability that the observed \n",
    "$U_X$ statistic at least as great as what was observed,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sum(prob(10,10,i) for i in range(U_X,101)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is close to the usual five percent p-value threshold so it is\n",
    "possible at a slightly higher threshold to conclude that the two sets of\n",
    "samples do *not* originate from the same underlying distribution. Keep in mind\n",
    "that the usual five percent threshold is just a guideline. Ultimately, it is up\n",
    "to the analyst to make the call.\n",
    "\n",
    "### Proving Mean and Variance for U-Statistic\n",
    "\n",
    "To prove Equation [4](#eq:ustatmv), we assume there are no ties.\n",
    "One way to get at the result $\\mathbb{E}(U)= n_x n_y/2$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(U_Y) = \\sum_j\\sum_i\\mathbb{P}(X_i \\leq Y_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " because $\\mathbb{E}(\\mathbb{I}(X_i\\leq Y_j))=\\mathbb{P}(X_i \\leq\n",
    "Y_j)$. Further, because all the subscripted $X$ and $Y$ variables are drawn\n",
    "independently from the same distribution, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(U_Y) = n_x n_y \\mathbb{P}(X \\leq Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and also,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(X \\leq Y) + \\mathbb{P}(X \\ge Y) =1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " because those are the two mutually exclusive conditions.  Because the\n",
    "$X$ variables and $Y$ variables are drawn from the same distribution, we have\n",
    "$\\mathbb{P}(X \\leq Y) = \\mathbb{P}(X \\ge Y)$, which means $ \\mathbb{P}(X \\leq\n",
    "Y)=1/2$ and therefore $\\mathbb{E}(U_Y)= n_x n_y /2$.  Another way to get the\n",
    "same result, is to note that, as we showed earlier,  $U_X+U_Y = n_x n_y $.\n",
    "Then, taking the expectation of both sides noting that\n",
    "$\\mathbb{E}(U_X)=\\mathbb{E}(U_Y)=\\mathbb{E}(U)$, gives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "2 \\mathbb{E}(U) = n_x n_y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which gives $\\mathbb{E}(U)=n_x n_y /2$. \n",
    "\n",
    "Getting the variance \n",
    "is trickier. To start, we compute the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(U_X U_Y) = \\sum_i \\sum_j \\sum_k \\sum_l \\mathbb{P}( X_i\\ge Y_j \\land X_k \\le Y_l )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Of these terms, we have $\\mathbb{P}( Y_j \\le X_i\\le Y_j)=0$ because these \n",
    "are continuous random variables. Let's consider the terms of the following type,\n",
    "$\\mathbb{P}( Y_i \\le X_k \\le Y_l)$. To reduce the notational noise, let's rewrite this as\n",
    "$\\mathbb{P}( Z \\le X \\le Y)$. Writing this out gives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}( Z \\le X \\le Y) = \\int_{\\mathbb{R}} \\int_Z^\\infty (F(Y)-F(Z))f(y)f(z)dy dz\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $F$ is the cumulative density function and $f$ is the\n",
    "probability density function ($dF(x)/dx = f(x)$).  Let's break this up term by\n",
    "term.  Using some calculus for the term,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\int_Z^\\infty F(Y)f(y)dy = \\int_{F(Z)}^1 F dF\\ = \\frac{1}{2}\\left(1-F(Z) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then, integrating out the $Z$ variable from this result, we obtain the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\int_{\\mathbb{R}} \\frac{1}{2}\\left(1-\\frac{F(Z)^2}{2}\\right) f(z) dz = \\frac{1}{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next, we  compute,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\int_{\\mathbb{R}} F(Z) \\int_Z^\\infty f(y) dy f(z) dz &=&\\int_{\\mathbb{R}} (1-F(Z)) F(Z) f(z) dz \\\\\n",
    "                                                     &=&\\int_{\\mathbb{R}} (1-F) F dF =\\frac{1}{6}\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Finally, assembling the result, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}( Z \\le X \\le Y) = \\frac{1}{3}- \\frac{1}{6} = \\frac{1}{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Also, terms like $\\mathbb{P}(X_k\\ge Y_i \\land X_m \\le Y_i) =\n",
    "\\mathbb{P}(X_m\\le Y_i \\le X_k)=1/6$ by the same reasoning. That leaves the\n",
    "terms like $\\mathbb{P}(X_k\\ge Y_i\\land X_m\\le Y_l)=1/4$ because of mutual\n",
    "independence  and $\\mathbb{P}(X_k\\ge Y_i)=1/2$. Now that we have all the\n",
    "terms, we have to assemble the combinatorics to get the final answer.\n",
    "\n",
    "There are $ n_y (n_y -1) n_x  + n_x (n_x -1) n_y $ terms of type $\\mathbb{P}(\n",
    "Y_i \\le X_k \\le Y_l)$. There are $ n_y (n_y -1) n_x (n_x -1)$ terms like\n",
    "$\\mathbb{P}(X_k\\ge Y_i\\land X_m\\le Y_l)$. Putting this all together, \n",
    "this means that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(U_X U_Y) = \\frac{n_x n_y(n_x+n_y-2)}{6}+\\frac{n_x n_y(n_x-1)(n_y-1)}{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To assemble the $\\mathbb{E}(U^2)$ result, we need to appeal to our earlier result,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "U_X+U_Y = n_x n_y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Squaring both sides of this and taking the expectation gives,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(U_X^2) + 2 \\mathbb{E}(U_X U_Y)+\\mathbb{E}(U_Y^2) = n_x^2 n_y^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Because $\\mathbb{E}(U_X^2)=\\mathbb{E}(U_X^2)=\\mathbb{E}(U)$, we \n",
    "can simplify this as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\mathbb{E}(U^2) &=& \\frac{n_x^2 n_y^2 - 2 \\mathbb{E}(U_X U_Y)}{2}\\\\\n",
    "\\mathbb{E}(U^2) &=& \\frac{n_x n_y (1+n_x +n_y +3 n_x n_y )}{12}\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then, since $\\mathbb{V}(U) = \\mathbb{E}(U^2)- \\mathbb{E}(U)^2$, we\n",
    "finally have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{V}(U) = \\frac{n_x n_y (1+ n_x +n_y )}{12}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TODO: Additive models, \"\" -->\n",
    "<!-- TODO: Local Regression Methods, p. 32 -->\n",
    "<!-- TODO: Spline Methods, p. 32 -->\n",
    "<!-- TODO: Rank-sum test Mathematica_Laboratories_for_Mathematical_Statistics_Baglivo.txt -->\n",
    "<!-- TODO: Rank-sum test Mathematica_Laboratories_for_Mat.. 11.2 Paired sample analysis -->"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
