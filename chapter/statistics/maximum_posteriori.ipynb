{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('../../Python_probability_statistics_machine_learning_2E.png',width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw with maximum likelihood estimation how we could use the\n",
    "principle of\n",
    "maximum likelihood to derive a formula of the data that\n",
    "would estimate the\n",
    "underlying parameters (say, $\\theta$). Under that\n",
    "method, the parameter was\n",
    "fixed, but unknown. If we change our\n",
    "perspective slightly and consider the\n",
    "underlying parameter as a random\n",
    "variable in its own right, this leads to\n",
    "additional flexibility in\n",
    "estimation.  This method is the simplest of the family\n",
    "of Bayesian\n",
    "statistical methods and is most closely related to maximum\n",
    "likelihood\n",
    "estimation. It is very popular in communications and signal\n",
    "processing\n",
    "and is the backbone of many important algorithms in those areas.\n",
    "Given that the parameter $\\theta$ is also a random variable, it has a\n",
    "joint\n",
    "distribution with the other random variables, say,\n",
    "$f(x,\\theta)$.  Bayes'\n",
    "theorem gives the following:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\theta|x) =\n",
    "\\frac{\\mathbb{P}(x|\\theta)\\mathbb{P}(\\theta)}{\\mathbb{P}(x)}\n",
    "$$\n",
    "\n",
    " The $\\mathbb{P}(x|\\theta)$ term is the usual likelihood term we have\n",
    "seen\n",
    "before. The term in the denominator is *prior* probability of the data $x$\n",
    "and\n",
    "it explicitly makes a very powerful claim: even before collecting or\n",
    "processing\n",
    "any data,  we know what the probability of that data is.  The\n",
    "$\\mathbb{P}(\\theta)$ is the prior probability of the\n",
    "parameter. In other words,\n",
    "regardless of the data that is collected, this is\n",
    "the probability of the\n",
    "parameter itself.\n",
    "\n",
    "In a particular application, whether or not you feel\n",
    "justified making these\n",
    "claims is something that you have to reconcile for\n",
    "yourself and the problem at\n",
    "hand.  There are many persuasive  philosophical\n",
    "arguments one way or the other,\n",
    "but the main thing to keep in mind when applying\n",
    "any method is whether or not\n",
    "the assumptions are reasonable for the problem at\n",
    "hand. \n",
    "\n",
    "However, for now, let's just assume that we somehow have\n",
    "$\\mathbb{P}(\\theta)$\n",
    "and the next step is the maximizing of this expression over\n",
    "the $\\theta$.\n",
    "Whatever results from that maximization is the maximum\n",
    "a-posteriori (MAP)\n",
    "estimator for $\\theta$. Because the maximization takes place\n",
    "with respect to\n",
    "$\\theta$ and not $x$, we can ignore the $\\mathbb{P}(x)$ part. To\n",
    "make things\n",
    "concrete, let us return to our original coin flipping problem.  From\n",
    "our\n",
    "earlier analysis, we know that the likelihood function for this problem is\n",
    "the following:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) := \\theta^k (1-\\theta)^{ (n-k) }\n",
    "$$\n",
    "\n",
    " where the probability of the coin coming up heads is $\\theta$. The\n",
    "next step is\n",
    "the prior probability, $\\mathbb{P}(\\theta)$. For this example, we\n",
    "will choose\n",
    "the $\\beta(6,6)$ distribution (shown in the top left panel of\n",
    "[Figure](#fig:MAP_001)). The $\\beta$ family of distributions is a gold mine\n",
    "because it allows for a wide variety of distributions using few input\n",
    "parameters. Now that we have all the ingredients, we turn to maximizing the\n",
    "posterior function, $\\mathbb{P}(\\theta|x)$. Because the logarithm is convex, we\n",
    "can use it to make the maximization process easier by converting the product to\n",
    "a sum without changing the extrema that we are looking for. Thus, we prefer the\n",
    "to work with the logarithm of $\\mathbb{P}(\\theta|x)$ as in the following.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} := \\log \\mathbb{P}(\\theta|x) =  \\log \\ell(\\theta) +\n",
    "\\log\\mathbb{P}(\\theta) - \\log\\mathbb{P}(x)\n",
    "$$\n",
    "\n",
    " This is tedious to do by hand and therefore an excellent job\n",
    "for Sympy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import sympy\n",
    ">>> from sympy import stats as st\n",
    ">>> from sympy.abc import p,k,n\n",
    "# setup objective function using sympy.log\n",
    ">>> obj=sympy.expand_log(sympy.log(p**k*(1-p)**(n-k)*\n",
    "                         st.density(st.Beta('p',6,6))(p)))\n",
    "# use calculus to maximize objective\n",
    ">>> sol=sympy.solve(sympy.simplify(sympy.diff(obj,p)),p)[0]\n",
    ">>> sol\n",
    "(k + 5)/(n + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means that our MAP estimator of $\\theta$ is the following:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{MAP} = \\frac{k+5}{n+10}\n",
    "$$\n",
    "\n",
    " where $k$ is the number of heads in the sample. This is obviously a\n",
    "biased\n",
    "estimator of $\\theta$,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\hat{\\theta}_{MAP}) = \\frac{5+n \\theta}{10 +n} \\neq \\theta\n",
    "$$\n",
    "\n",
    " But is this bias *bad*? Why would anyone want a biased estimator?\n",
    "Remember that\n",
    "we constructed this entire estimator using the idea of the prior\n",
    "probability of\n",
    "$\\mathbb{P}(\\theta)$ which *favors* (biases!) the estimate\n",
    "according to the\n",
    "prior.  For example, if $\\theta=1/2$, the MAP estimator\n",
    "evaluates to\n",
    "$\\hat{\\theta}_{MAP}=1/2$. No bias there! This is because the peak\n",
    "of the prior\n",
    "probability is at $\\theta=1/2$. \n",
    "\n",
    "To compute the corresponding variance for this\n",
    "estimator, we need this\n",
    "intermediate result,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\hat{\\theta}_{MAP}^2) =\\frac{25 +10 n \\theta + n \\theta((n-1)\n",
    "p+1)}{(10+n)^2}\n",
    "$$\n",
    "\n",
    " which gives the following variance,\n",
    "\n",
    "$$\n",
    "\\mathbb{V}(\\hat{\\theta}_{MAP}) = \\frac{n (1-\\theta) \\theta}{(n+10)^2}\n",
    "$$\n",
    "\n",
    "Let's pause and compare this to our previous maximum likelihood (ML) estimator\n",
    "shown below:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{ML} = \\frac{1}{n} \\sum_{i=1}^n X_i =  \\frac{k}{n}\n",
    "$$\n",
    "\n",
    " As we discussed before, the ML-estimator is unbiased with the\n",
    "following\n",
    "variance.\n",
    "\n",
    "$$\n",
    "\\mathbb{V}(\\hat{\\theta}_{ML}) = \\frac{\\theta(1-\\theta)}{n}\n",
    "$$\n",
    "\n",
    " How does this variance compare to that of the MAP? The ratio of the\n",
    "two is the\n",
    "following:\n",
    "\n",
    "$$\n",
    "\\frac{\\mathbb{V}(\\hat{\\theta}_{MAP})}{\\mathbb{V}(\\hat{\\theta}_{ML})}=\\frac{n^2}{(n+10)^2}\n",
    "$$\n",
    "\n",
    "  This ratio shows that the variance for the MAP-estimator is smaller\n",
    "than that\n",
    "of the the ML-estimator. This is payoff for having a biased\n",
    "MAP-estimator --- it\n",
    "requires fewer samples to estimate if the underlying\n",
    "parameter is consistent\n",
    "with the prior probability. If not, then it will take\n",
    "more samples to pull the\n",
    "estimator away from the bias. In the limit as $n\n",
    "\\rightarrow \\infty$, the ratio\n",
    "goes to one.  This means that the benefit of the\n",
    "reduced variance vanishes with\n",
    "enough samples. \n",
    "\n",
    "The above discussion admits a level of arbitrariness via the\n",
    "prior\n",
    "distribution. We don't have to choose just one prior, however. The\n",
    "following shows how we can use the previous posterior distribution as the\n",
    "prior\n",
    "for the next posterior distribution,\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\theta|x_{k+1}) =\n",
    "\\frac{\\mathbb{P}(x_{k+1}|\\theta)\\mathbb{P}(\\theta|x_k)}{\\mathbb{P}(x_{k+1})}\n",
    "$$\n",
    "\n",
    " This is a very different strategy because we are using every data\n",
    "sample $x_k$\n",
    "as a parameter for the posterior distribution instead of lumping\n",
    "all the samples\n",
    "together in a summation (this is where we got the $k$\n",
    "term in the prior case).\n",
    "This case is much harder to analyze because now\n",
    "every incremental posterior\n",
    "distribution is itself a random function because of\n",
    "the injection of the $x$\n",
    "random variable. On the other hand, this is more in\n",
    "line with more general\n",
    "Bayesian methods because it is clear that the output of\n",
    "this estimation process\n",
    "is a posterior distribution function, not just a single\n",
    "parameter estimate.\n",
    "[Figure](#fig:MAP_001) illustrates this method. The graph\n",
    "in the top row, far\n",
    "left shows the prior probability ($\\beta(6,6)$) and the dot\n",
    "on the top shows the\n",
    "most recent MAP-estimate for $\\theta$. Thus, before we\n",
    "obtain any data, the peak\n",
    "of the prior probability  is the estimate. The next\n",
    "graph to right shows the\n",
    "effect of $x_0=0$ on the incremental prior\n",
    "probability.  Note that the estimate\n",
    "has barely moved to the left. This is\n",
    "because the influence of the data has not\n",
    "caused the prior probability to drift\n",
    "away from the original\n",
    "$\\beta(6,6)$-distribution.  The first two rows of the\n",
    "figure all have $x_k=0$\n",
    "just to illustrate how far left the original prior\n",
    "probability can be moved by\n",
    "those data. The dots on the tops of the sub-graphs\n",
    "show how the MAP estimate\n",
    "changes frame-by-frame as more data is incorporated.\n",
    "The remaining graphs,\n",
    "proceeding top-down, left-to-right show the incremental\n",
    "change in the prior\n",
    "probability for $x_k=1$.  Again, this shows how far to the right \n",
    "the estimate\n",
    "can be pulled from where it started. For this example, there\n",
    "are an equal number\n",
    "of $x_k=0$ and $x_k=1$ data, which corresponds \n",
    "to $\\theta=1/2$.  \n",
    "\n",
    "<!--\n",
    "dom:FIGURE: [fig-statistics/MAP_001.png, width=500 frac=0.95] The prior\n",
    "probability is the $\\beta(6,6)$ distribution shown in the top left panel. The\n",
    "dots near the peaks of each of the subgraphs indicate the MAP estimate at that\n",
    "frame <div id=\"fig:MAP_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div\n",
    "id=\"fig:MAP_001\"></div>\n",
    "\n",
    "<p>The prior probability is the $\\beta(6,6)$\n",
    "distribution shown in the top left panel. The dots near the peaks of each of the\n",
    "subgraphs indicate the MAP estimate at that frame</p>\n",
    "<img src=\"fig-\n",
    "statistics/MAP_001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "**Programming Tip.**\n",
    "The following is a quick paraphrase of how [Figure](#fig:MAP_001) was\n",
    "constructed. The first step is to recursively create the\n",
    "posteriors from the\n",
    "data. Note the example data is sorted\n",
    "to make the progression easy to see as a\n",
    "sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy.abc import p,x\n",
    "from scipy.stats import density, Beta, Bernoulli\n",
    "prior = density(Beta('p',6,6))(p)\n",
    "likelihood=density(Bernoulli('x',p))(x)\n",
    "data = (0,0,0,0,0,0,0,1,1,1,1,1,1,1,1)\n",
    "posteriors = [prior]\n",
    "for i in data:\n",
    "    posteriors.append(posteriors[-1]*likelihood.subs(x,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the posteriors in hand, the next step \n",
    "is to compute the peak values at\n",
    "each frame using the\n",
    "`fminbound` function from Scipy's  `optimize` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = linspace(0,1,100)\n",
    "mxvals = []\n",
    "for i,j in zip(ax.flat,posteriors):\n",
    "    i.plot(pvals,sympy.lambdify(p,j)(pvals),color='k')\n",
    "    mxval = fminbound(sympy.lambdify(p,-j),0,1)\n",
    "    mxvals.append(mxval)\n",
    "    h = i.axis()[-1]\n",
    "    i.axis(ymax=h*1.3)\n",
    "    i.plot(mxvals[-1],h*1.2,'ok')\n",
    "    i.plot(mxvals[:-1],[h*1.2]*len(mxvals[:-1]),'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Figure](#fig:MAP_002) is the same as [Figure](#fig:MAP_001) except that\n",
    "the\n",
    "initial prior probability is the $\\beta(1.3,1.3)$-distribution, which has a\n",
    "wider lobe that the $\\beta(6,6)$-distribution. As shown in the figure, this\n",
    "prior has the ability to be swayed more violently one way or the other based on\n",
    "the $x_k$ data that is incorporated. This means that it can more quickly adapt\n",
    "to data that is not so consistent with the initial prior and thus does not\n",
    "require a large amount of data in order to *unlearn* the prior probability.\n",
    "Depending on the application, the ability to unlearn the prior probability or\n",
    "stick with it is a design problem for the analyst. In this example, because the\n",
    "data are representative of a $\\theta=1/2$ parameter, both priors eventually\n",
    "settle on an estimated posterior that is about the same. However, if this had\n",
    "not been the case ($\\theta \\neq 1/2$), then the second prior \n",
    "would have\n",
    "produced a better estimate for the same amount of data. \n",
    "\n",
    "<!-- dom:FIGURE: [fig-\n",
    "statistics/MAP_002.png, width=500 frac=0.95] For this example, the prior\n",
    "probability is the $\\beta(1.3,1.3)$ distribution, which has a wider main lobe\n",
    "than the $\\beta(6,6)$ distribution. The dots near the peaks of each of the\n",
    "subgraphs indicate the MAP estimate at that frame. <div id=\"fig:MAP_002\"></div>\n",
    "-->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:MAP_002\"></div>\n",
    "\n",
    "<p>For this example, the\n",
    "prior probability is the $\\beta(1.3,1.3)$ distribution, which has a wider main\n",
    "lobe than the $\\beta(6,6)$ distribution. The dots near the peaks of each of the\n",
    "subgraphs indicate the MAP estimate at that frame.</p>\n",
    "<img src=\"fig-\n",
    "statistics/MAP_002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "Because we have the\n",
    "entire posterior density available, we can compute\n",
    "something that is closely\n",
    "related to the confidence interval we discussed\n",
    "earlier, except in this\n",
    "situation, given the Bayesian interpretation, it is\n",
    "called a *credible interval*\n",
    "or *credible set*.  The idea is that we want to\n",
    "find a symmetric interval around\n",
    "the peak that accounts for 95% (say) of the\n",
    "posterior density. This means that\n",
    "we can then say the probability that the\n",
    "estimated parameter is within the\n",
    "credible interval is 95%. The computation\n",
    "requires significant numerical\n",
    "processing because even though we have the\n",
    "posterior density in hand, it is hard\n",
    "to integrate analytically and requires\n",
    "numerical quadrature (see Scipy's\n",
    "`integrate` module).  [Figure](#fig:MAP_003)\n",
    "shows extent of the interval and\n",
    "the shaded region under the posterior density\n",
    "that accounts for 95%.\n",
    "\n",
    "<!--\n",
    "dom:FIGURE: [fig-statistics/MAP_003.png, width=500 frac=0.75] The *credible\n",
    "interval* in Bayesian maximum a-posteriori is the interval corresponding to the\n",
    "shaded region in the posterior density.  <div id=\"fig:MAP_003\"></div> -->\n",
    "<!--\n",
    "begin figure -->\n",
    "<div id=\"fig:MAP_003\"></div>\n",
    "\n",
    "<p>The <em>credible interval</em>\n",
    "in Bayesian maximum a-posteriori is the interval corresponding to the shaded\n",
    "region in the posterior density.</p>\n",
    "<img src=\"fig-statistics/MAP_003.png\"\n",
    "width=500>\n",
    "\n",
    "<!-- end figure -->"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
