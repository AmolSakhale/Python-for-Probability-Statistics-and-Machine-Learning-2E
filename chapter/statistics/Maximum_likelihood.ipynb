{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('../../Python_probability_statistics_machine_learning_2E.png',width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimation problem starts with the desire to infer something meaningful\n",
    "from\n",
    "data.  For parametric estimation, the strategy is to postulate a model for\n",
    "the\n",
    "data and then use the data to fit model parameters.  This leads to two\n",
    "fundamental questions: where to get the model and how to estimate the\n",
    "parameters? The first question is best answered by the maxim: *all models are\n",
    "wrong, some are useful*. In other words, choosing a model depends as much on\n",
    "the\n",
    "application as on the model itself. Think about models as building\n",
    "different\n",
    "telescopes to view the sky. No one would ever claim that the\n",
    "telescope generates\n",
    "the sky! It is same with data models. Models give us\n",
    "multiple perspectives on\n",
    "the data that themselves are proxies for some deeper\n",
    "underlying phenomenon.\n",
    "Some\n",
    "categories of data may be more commonly studied using certain types of\n",
    "models,\n",
    "but this is usually very domain-specific and ultimately depends on the\n",
    "aims of\n",
    "the analysis. In some cases, there may be strong physical reasons\n",
    "behind\n",
    "choosing a model.  For example, one could postulate that the model is\n",
    "linear\n",
    "with some noise as in the following:\n",
    "\n",
    "$$\n",
    "Y = a X + \\epsilon\n",
    "$$\n",
    "\n",
    " which basically\n",
    "says that you, as the experimenter, dial in some\n",
    "value for $X$\n",
    "and then read off\n",
    "something directly proportional to $X$ as the\n",
    "measurement,\n",
    "$Y$, plus some\n",
    "additive noise that you attribute to jitter in the\n",
    "apparatus.\n",
    "Then, the next\n",
    "step is to estimate the paramater $a$ in the model,\n",
    "given some\n",
    "postulated claim\n",
    "about the nature of $\\epsilon$. How to compute the\n",
    "model\n",
    "parameters depends on\n",
    "the particular methodology. The two broad rubrics\n",
    "are\n",
    "parametric and non-\n",
    "parametric estimation. In the former, we assume we know\n",
    "the\n",
    "density function of\n",
    "the data and then try to derive the embedded parameters\n",
    "for\n",
    "it. In the latter,\n",
    "we claim only to know that the density function is a\n",
    "member\n",
    "of a broad class of\n",
    "density functions and then use the data\n",
    "to characterize a\n",
    "member of that class.\n",
    "Broadly speaking, the former consumes\n",
    "less data than the\n",
    "latter, because there\n",
    "are fewer unknowns to compute from\n",
    "the data.\n",
    "\n",
    "Let's\n",
    "concentrate on parametric\n",
    "estimation for now. The tradition is to denote\n",
    "the\n",
    "unknown parameter to be\n",
    "estimated as $\\theta$ which is a member of a large\n",
    "space\n",
    "of alternates,\n",
    "$\\Theta$. To judge between potential $\\theta$ values, we\n",
    "need an\n",
    "objective\n",
    "function, known as a *risk* function,\n",
    "$L(\\theta,\\hat{\\theta})$, where\n",
    "$\\hat{\\theta}(\\mathbf{x})$ is an\n",
    "estimate for the unknown $\\theta$ that is\n",
    "derived from the available\n",
    "data $\\mathbf{x}$. The most common and useful risk\n",
    "function is the\n",
    "squared error loss,\n",
    "\n",
    "$$\n",
    "L(\\theta,\\hat{\\theta}) =\n",
    "(\\theta-\\hat{\\theta})^2\n",
    "$$\n",
    "\n",
    " Although neat, this is not practical because we\n",
    "need to know the\n",
    "unknown\n",
    "$\\theta$ to compute it. The other problem is because\n",
    "$\\hat{\\theta}$ is\n",
    "a\n",
    "function of the observed data, it is also a random variable\n",
    "with its own\n",
    "probability density function.  This leads to the notion of the\n",
    "*expected risk*\n",
    "function,\n",
    "\n",
    "$$\n",
    "R(\\theta,\\hat{\\theta}) =\n",
    "\\mathbb{E}_\\theta(L(\\theta,\\hat{\\theta})) = \\int\n",
    "L(\\theta,\\hat{\\theta}(\\mathbf{x})) f(\\mathbf{x};\\theta) d \\mathbf{x}\n",
    "$$\n",
    "\n",
    " In\n",
    "other words, given a fixed $\\theta$, integrate over the\n",
    "probability density\n",
    "function of the data, $f(\\mathbf{x})$,  to compute the\n",
    "risk. Plugging in for the\n",
    "squared error loss,  we compute the\n",
    "mean squared error,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_\\theta(\\theta-\\hat{\\theta})^2 =\\int (\\theta-\\hat{\\theta})^2\n",
    "f(\\mathbf{x};\\theta) d \\mathbf{x}\n",
    "$$\n",
    "\n",
    " This has the important factorization into\n",
    "the *bias*,\n",
    "\n",
    "$$\n",
    "\\textnormal{bias} = \\mathbb{E}_\\theta(\\hat{\\theta})-\\theta\n",
    "$$\n",
    "with the corresponding variance, $\\mathbb{V}_\\theta(\\hat{\\theta})$ as\n",
    "in the\n",
    "following *mean squared error* (MSE):\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_\\theta(\\theta-\\hat{\\theta})^2=\n",
    "\\textnormal{bias}^2+\\mathbb{V}_\\theta(\\hat{\\theta})\n",
    "$$\n",
    "\n",
    " This is an important\n",
    "trade-off that we will return to repeatedly. The\n",
    "idea is\n",
    "the bias is nonzero\n",
    "when the estimator $\\hat{\\theta}$, integrated\n",
    "over all\n",
    "possible data,\n",
    "$f(\\mathbf{x})$, does not equal the underlying target\n",
    "parameter\n",
    "$\\theta$. In\n",
    "some sense, the estimator misses the target, no matter\n",
    "how much\n",
    "data is used.\n",
    "When the bias equals zero, the estimated is *unbiased*.\n",
    "For fixed\n",
    "MSE, low bias\n",
    "implies high variance and vice-versa. This trade-off\n",
    "was once not\n",
    "emphasized and\n",
    "instead much attention was paid to the smallest\n",
    "variance of\n",
    "unbiased estimators\n",
    "(see Cramer-Rao bounds). In practice,\n",
    "understanding and\n",
    "exploiting the trade-off\n",
    "between bias and variance and\n",
    "reducing the MSE is more\n",
    "important.\n",
    "\n",
    "With all this\n",
    "set up, we can now ask how bad can bad get by\n",
    "examining *minimax* risk,\n",
    "\n",
    "$$\n",
    "R_{\\textnormal{mmx}} = \\inf_{\\hat{\\theta}} \\sup_\\theta R(\\theta,\\hat{\\theta})\n",
    "$$\n",
    "where the $\\inf$ is take over all estimators.  Intuitively, this\n",
    "means if we\n",
    "found the worst possible $\\theta$ and swept over all possible\n",
    "parameter\n",
    "estimators $\\hat{\\theta}$, and then took the smallest possible risk\n",
    "we could\n",
    "find, we would have the minimax risk. Thus, an estimator,\n",
    "$\\hat{\\theta}_{\\textnormal{mmx}}$, is a *minimax estimator* if it achieves this\n",
    "feat,\n",
    "\n",
    "$$\n",
    "\\sup_\\theta R(\\theta,\\hat{\\theta}_{\\textnormal{mmx}})\n",
    "=\\inf_{\\hat{\\theta}}\n",
    "\\sup_\\theta R(\\theta,\\hat{\\theta})\n",
    "$$\n",
    "\n",
    " In other words,\n",
    "even in the face of the worst $\\theta$ (i.e., the\n",
    "$\\sup_\\theta$),\n",
    "$\\hat{\\theta}_{\\textnormal{mmx}}$ still achieves the minimax\n",
    "risk. There is a\n",
    "greater theory that revolves around minimax estimators of\n",
    "various kinds, but\n",
    "this is far beyond our scope here. The main thing to focus\n",
    "on\n",
    "is that under\n",
    "certain technical but easily satisfiable conditions, the\n",
    "maximum\n",
    "likelihood\n",
    "estimator is approximately minimax. Maximum likelihood is\n",
    "the subject\n",
    "of the\n",
    "next section.  Let's get started with the simplest\n",
    "application: coin-\n",
    "flipping.\n",
    "## Setting up the Coin Flipping Experiment\n",
    "\n",
    "Suppose we have coin and\n",
    "want to\n",
    "estimate the probability of heads ($p$) for\n",
    "it. We model the\n",
    "distribution of\n",
    "heads and tails as a Bernoulli distribution\n",
    "with the following\n",
    "probability mass\n",
    "function:\n",
    "\n",
    "$$\n",
    "\\phi(x)= p^x (1-p)^{(1-x)}\n",
    "$$\n",
    "\n",
    " where $x$ is the outcome, *1* for\n",
    "heads and *0* for tails. Note that\n",
    "maximum\n",
    "likelihood is a parametric method\n",
    "that requires the specification of a\n",
    "particular model for which we will compute\n",
    "embedded parameters. For $n$\n",
    "independent flips, we have the joint density as the\n",
    "product of $n$ of\n",
    "these\n",
    "functions as in,\n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf{x})=\\prod_{i=1}^n\n",
    "p^x_i (1-p)^{(1-x_i)}\n",
    "$$\n",
    "\n",
    " The following is the *likelihood function*,\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(p ; \\mathbf{x})= \\prod_{i=1}^n p^{ x_i }(1-p)^{1-x_i}\n",
    "$$\n",
    "\n",
    " This is\n",
    "basically notation. We have just renamed the\n",
    "previous equation to\n",
    "emphasize the\n",
    "$p$ parameter, which is what\n",
    "we want to estimate.\n",
    "\n",
    "The principle\n",
    "of *maximum\n",
    "likelihood* is to maximize the likelihood as the\n",
    "function of $p$\n",
    "after plugging\n",
    "in all of the $x_i$ data. We then call this\n",
    "maximizer $\\hat{p}$\n",
    "which is a\n",
    "function of the observed $x_i$ data, and as\n",
    "such, is a random\n",
    "variable with its\n",
    "own distribution. This method therefore\n",
    "ingests data and an\n",
    "assumed model for\n",
    "the probability density, and produces a\n",
    "function that\n",
    "estimates the embedded\n",
    "parameter in the assumed probability\n",
    "density.  Thus,\n",
    "maximum likelihood\n",
    "generates the *functions* of data that we\n",
    "need in order to\n",
    "get at the underlying\n",
    "parameters of the model. Note that there\n",
    "is no limit to\n",
    "the ways we can\n",
    "functionally manipulate the data we have\n",
    "collected. The maximum\n",
    "likelihood\n",
    "principle gives us a systematic method for\n",
    "constructing these\n",
    "functions subject\n",
    "to the assumed model. This is a point\n",
    "worth emphasizing: the\n",
    "maximum likelihood\n",
    "principle yields functions as\n",
    "solutions the same way solving\n",
    "differential\n",
    "equations yields functions as\n",
    "solutions. It is very, very much\n",
    "harder to produce\n",
    "a function than to produce a\n",
    "value as a solution, even with\n",
    "the assumption of a\n",
    "convenient probability\n",
    "density.  Thus, the power of the\n",
    "principle is that you\n",
    "can construct such\n",
    "functions subject to the model\n",
    "assumptions.\n",
    "\n",
    "### Simulating\n",
    "the Experiment\n",
    "\n",
    "We need the following code to\n",
    "simulate coin flipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli \n",
    "p_true=1/2.0         # estimate this!\n",
    "fp=bernoulli(p_true) # create bernoulli random variate\n",
    "xs = fp.rvs(100)     # generate some samples\n",
    "print (xs[:30])        # see first 30 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can write out the likelihood function using Sympy.  Note\n",
    "that we give\n",
    "the Sympy variables the `positive=True` attribute upon\n",
    "construction because this\n",
    "eases Sympy's internal simplification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "x,p,z=sympy.symbols('x p z', positive=True)\n",
    "phi=p**x*(1-p)**(1-x) # distribution function\n",
    "L=np.prod([phi.subs(x,i) for i in xs]) # likelihood function \n",
    "print (L) # approx 0.5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, once we plug in the data, the likelihood function is\n",
    "solely a\n",
    "function of the unknown parameter ($p$ in this case).  The following\n",
    "code uses\n",
    "calculus to find the extrema of the likelihood function.  Note that\n",
    "taking the\n",
    "`log` of $L$ makes the maximization problem tractable but doesn't\n",
    "change the\n",
    "extrema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logL=sympy.expand_log(sympy.log(L))\n",
    "sol,=sympy.solve(sympy.diff(logL,p),p)\n",
    "print (sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "Note that `sol,=sympy.solve` statement includes\n",
    "a comma\n",
    "after the `sol` variable. This is because the `solve`\n",
    "function returns a list\n",
    "containing a single element. Using\n",
    "this assignment unpacks that single element\n",
    "into the `sol` variable\n",
    "directly. This is another one of the many small\n",
    "elegancies of Python.\n",
    "\n",
    " \n",
    "\n",
    "The following code generates\n",
    "[Figure](#fig:Maximum_likelihood_10_2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.pylab import subplots\n",
    "fig,ax=subplots()\n",
    "x=np.linspace(0,1,100)\n",
    "ax.plot(x,map(sympy.lambdify(p,logJ,'numpy'),x),'k-',lw=3)\n",
    "ax.plot(sol,logJ.subs(p,sol),'o',\n",
    "        color='gray',ms=15,label='Estimated')\n",
    "ax.plot(p_true,logJ.subs(p,p_true),'s',\n",
    "        color='k',ms=15,label='Actual')\n",
    "ax.set_xlabel('$p$',fontsize=18)\n",
    "ax.set_ylabel('Likelihood',fontsize=18)\n",
    "ax.set_title('Estimate not equal to true value',fontsize=18)\n",
    "ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "In the prior code, we use the `lambdify` function in\n",
    "`lambdify(p,logJ,'numpy')` to\n",
    "take a Sympy expression and convert it into a\n",
    "Numpy version that is easier to\n",
    "compute.  The `lambdify` function has an extra\n",
    "argument where you can specify\n",
    "the function space that it should use to convert\n",
    "the expression. In the above\n",
    "this is set to Numpy.\n",
    "\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig-\n",
    "statistics/Maximum_likelihood_10_2.png, width=500 frac=0.75] Maximum likelihood\n",
    "estimate vs. true parameter. Note that the estimate is slightly off from the\n",
    "true value. This is a consequence of the fact that the estimator is a function\n",
    "of the data and lacks knowledge of the true underlying value.  <div\n",
    "id=\"fig:Maximum_likelihood_10_2\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div\n",
    "id=\"fig:Maximum_likelihood_10_2\"></div>\n",
    "\n",
    "<p>Maximum likelihood estimate vs. true\n",
    "parameter. Note that the estimate is slightly off from the true value. This is a\n",
    "consequence of the fact that the estimator is a function of the data and lacks\n",
    "knowledge of the true underlying value.</p>\n",
    "<img src=\"fig-\n",
    "statistics/Maximum_likelihood_10_2.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "[Figure](#fig:Maximum_likelihood_10_2) shows  that our estimator $\\hat{p}$\n",
    "(circle) is not equal to the true value of $p$ (square), despite being\n",
    "the\n",
    "maximum of the likelihood function. This may sound disturbing, but keep in\n",
    "mind\n",
    "this estimate is a function of the random data; and since that data can\n",
    "change,\n",
    "the ultimate estimate can likewise change. \n",
    "Remember that the estimator is a\n",
    "*function* of the data and is thus also a\n",
    "*random variable*, just like the data\n",
    "is. This means it has its own probability\n",
    "distribution with corresponding mean\n",
    "and variance. So, what we are observing is\n",
    "a consequence of that variance.\n",
    "\n",
    "<!--\n",
    "!bc pycod -->\n",
    "<!-- def estimator_gen(niter=10,ns=100): -->\n",
    "<!-- 'generate data\n",
    "to estimate distribution of maximum likelihood estimator' -->\n",
    "<!-- out=[] -->\n",
    "<!-- # make sympy variable real-valued -->\n",
    "<!-- x=sympy.symbols('x',real=True)\n",
    "-->\n",
    "<!-- # likelihood function -->\n",
    "<!-- L= p**x*(1-p)**(1-x) -->\n",
    "<!-- for i in\n",
    "range(niter): -->\n",
    "<!-- # generate some samples from the experiment -->\n",
    "<!-- xs =\n",
    "sample(ns) -->\n",
    "<!-- # objective function to maximize -->\n",
    "<!--\n",
    "J=np.prod([L.subs(x,i) for i in xs]) -->\n",
    "<!-- # log is easier to work with -->\n",
    "<!-- logJ=sympy.expand_log(sympy.log(J)) -->\n",
    "<!-- # use basic calculus to find\n",
    "extrema -->\n",
    "<!-- sol=sympy.solve(sympy.diff(logJ,p),p)[0] -->\n",
    "<!-- # convert\n",
    "output to numeric float from sympy -->\n",
    "<!-- out.append(float(sol.evalf())) -->\n",
    "<!-- # return scalar if list contains only 1 term -->\n",
    "<!-- return out if\n",
    "len(out)>1 else out[0] -->\n",
    "<!-- !ec -->\n",
    "\n",
    "<!-- dom:FIGURE: [fig-\n",
    "statistics/Maximum_likelihood_30_2.png, width=500 frac=0.85] Histogram of\n",
    "maximum likelihood estimates. The title shows the estimated mean and standard\n",
    "deviation of the samples. <div id=\"fig:Maximum_likelihood_30_2\"></div>  -->\n",
    "<!--\n",
    "begin figure -->\n",
    "<div id=\"fig:Maximum_likelihood_30_2\"></div>\n",
    "\n",
    "<p>Histogram of\n",
    "maximum likelihood estimates. The title shows the estimated mean and standard\n",
    "deviation of the samples.</p>\n",
    "<img src=\"fig-\n",
    "statistics/Maximum_likelihood_30_2.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "[Figure](#fig:Maximum_likelihood_30_2) shows what happens when you run many\n",
    "thousands of coin experiments and compute the maximum likelihood\n",
    "estimate for\n",
    "each experiment, given a particular number of samples \n",
    "per experiment. This\n",
    "simulation gives us a histogram of the maximum likelihood\n",
    "estimates, which is an\n",
    "approximation of the probability distribution of the\n",
    "$\\hat{p}$ estimator itself.\n",
    "This figure shows that the sample mean\n",
    "of the estimator ($\\mu = \\frac{1}{n}\\sum\n",
    "\\hat{p}_i$) is pretty close to the\n",
    "true value, but looks can be deceiving. The\n",
    "only way to know for sure is to\n",
    "check if the estimator is unbiased, namely, if\n",
    "$$\n",
    "\\mathbb{E}(\\hat{p}) = p\n",
    "$$\n",
    "\n",
    " Because this problem is simple, we can solve for\n",
    "this in general\n",
    "noting that\n",
    "the terms above are either $p$, if $x_i=1$ or $1-p$\n",
    "if $x_i=0$.\n",
    "This means that\n",
    "we can write\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(p\\vert \\mathbf{x})=\n",
    "p^{\\sum_{i=1}^n x_i}(1-p)^{n-\\sum_{i=1}^n\n",
    "x_i}\n",
    "$$\n",
    "\n",
    " with corresponding logarithm\n",
    "as\n",
    "\n",
    "$$\n",
    "J=\\log(\\mathcal{L}(p\\vert \\mathbf{x})) =  \\log(p)  \\sum_{i=1}^n x_i +\n",
    "\\log(1-p) \\left(n-\\sum_{i=1}^n x_i\\right)\n",
    "$$\n",
    "\n",
    " Taking the derivative of this\n",
    "gives:\n",
    "\n",
    "$$\n",
    "\\frac{dJ}{dp} = \\frac{1}{p}\\sum_{i=1}^n x_i + \\frac{(n-\\sum_{i=1}^n\n",
    "x_i)}{p-1}\n",
    "$$\n",
    "\n",
    " and solving this for $p$ leads to\n",
    "\n",
    "$$\n",
    "\\hat{p} = \\frac{1}{ n}\n",
    "\\sum_{i=1}^n x_i\n",
    "$$\n",
    "\n",
    "This is our *estimator* for $p$. Up until now, we have been\n",
    "using Sympy to\n",
    "solve\n",
    "for this based on the data $x_i$ but now that we have it\n",
    "analytically we\n",
    "don't\n",
    "have to solve for it each time. To check if this estimator\n",
    "is biased, we\n",
    "compute\n",
    "its expectation:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left(\\hat{p}\\right)\n",
    "=\\frac{1}{n}\\sum_i^n \\mathbb{E}(x_i) =\n",
    "\\frac{1}{n} n \\mathbb{E}(x_i)\n",
    "$$\n",
    "\n",
    " by\n",
    "linearity of the expectation and where\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(x_i)  = p\n",
    "$$\n",
    "\n",
    " Therefore,\n",
    "$$\n",
    "\\mathbb{E}\\left(\\hat{p}\\right) =p\n",
    "$$\n",
    "\n",
    " This means that the estimator is\n",
    "*unbiased*. Similarly,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left(\\hat{p}^2\\right) = \\frac{1}{n^2}\n",
    "\\mathbb{E}\\left[\\left(\n",
    "\\sum_{i=1}^n x_i \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    " and where\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left(x_i^2\\right) =p\n",
    "$$\n",
    "\n",
    " and by the independence assumption,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left(x_i x_j\\right) =\\mathbb{E}(x_i)\\mathbb{E}(x_j) =p^2\n",
    "$$\n",
    "\n",
    " Thus,\n",
    "$$\n",
    "\\mathbb{E}\\left(\\hat{p}^2\\right) =\\left(\\frac{1}{n^2}\\right) n \\left[\n",
    "p+(n-1)p^2 \\right]\n",
    "$$\n",
    "\n",
    " So, the variance of the estimator, $\\hat{p}$, is the\n",
    "following:\n",
    "\n",
    "$$\n",
    "\\mathbb{V}(\\hat{p}) = \\mathbb{E}\\left(\\hat{p}^2\\right)-\n",
    "\\mathbb{E}\\left(\\hat{p}\\right)^2  = \\frac{p(1-p)}{n}\n",
    "$$\n",
    "\n",
    " Note that the $n$ in\n",
    "the denominator means that the variance\n",
    "asymptotically\n",
    "goes to zero as $n$\n",
    "increases (i.e., we consider more and\n",
    "more samples). This is\n",
    "good news because\n",
    "it means that more and\n",
    "more coin flips lead to a better\n",
    "estimate of the\n",
    "underlying $p$.\n",
    "\n",
    "Unfortunately, this formula for the variance is\n",
    "practically\n",
    "useless because we\n",
    "need $p$ to compute it and $p$ is the parameter\n",
    "we are trying\n",
    "to estimate in\n",
    "the first place! However, this is where the *plug-\n",
    "in* principle\n",
    "[^invariance-property] \n",
    "saves the day.  It turns out in this\n",
    "situation, you can\n",
    "simply substitute the maximum likelihood estimator,\n",
    "$\\hat{p}$, for the $p$ in\n",
    "the above equation to obtain the asymptotic variance\n",
    "for $\\mathbb{V}(\\hat{p})$.\n",
    "The fact that this works is guaranteed by the\n",
    "asymptotic theory of maximum\n",
    "likelihood estimators.\n",
    "\n",
    "[^invariance-property]:\n",
    "This is also known as the\n",
    "*invariance property*\n",
    "of maximum likelihood\n",
    "estimators. It basically states that\n",
    "the \n",
    "maximum likelihood estimator of any\n",
    "function, say, $h(\\theta)$, is\n",
    "the same\n",
    "$h$ with the maximum likelihood\n",
    "estimator for $\\theta$ substituted\n",
    "in for\n",
    "$\\theta$; namely, $h(\\theta_{ML})$.\n",
    "Nevertheless, looking at\n",
    "$\\mathbb{V}(\\hat{p})^2$, we can immediately notice\n",
    "that\n",
    "if $p=0$, then there is\n",
    "no estimator variance because the outcomes are\n",
    "guaranteed to be tails. Also, for\n",
    "any $n$,  the maximum of this variance\n",
    "happens\n",
    "at $p=1/2$. This is our worst\n",
    "case scenario and the only way to\n",
    "compensate is\n",
    "with larger $n$.\n",
    "\n",
    "All we have\n",
    "computed is the mean and variance of the\n",
    "estimator. In general,\n",
    "this is\n",
    "insufficient to characterize the underlying\n",
    "probability density of\n",
    "$\\hat{p}$,\n",
    "except if we somehow knew that $\\hat{p}$ were\n",
    "normally distributed.\n",
    "This is\n",
    "where the powerful *Central Limit Theorem* we\n",
    "discussed in the section\n",
    "[ch:stats:sec:limit](#ch:stats:sec:limit) comes in. The\n",
    "form of the estimator,\n",
    "which is just a\n",
    "sample mean, implies that we can apply\n",
    "this theorem and conclude\n",
    "that $\\hat{p}$\n",
    "is asymptotically normally distributed.\n",
    "However, it doesn't\n",
    "quantify how many\n",
    "samples $n$ we need. In our simulation\n",
    "this is no problem\n",
    "because we can\n",
    "generate as much data as we like, but in the\n",
    "real world, with a\n",
    "costly\n",
    "experiment, each sample may be precious [^edgeworth].\n",
    "In the following,\n",
    "we won't apply the Central Limit Theorem and instead proceed\n",
    "analytically.\n",
    "[^edgeworth]: It turns out that the central limit theorem\n",
    "augmented with an\n",
    "Edgeworth expansion tells us that convergence is regulated by\n",
    "the skewness\n",
    "of\n",
    "the distribution\n",
    "[[feller1950introduction]](#feller1950introduction). In other\n",
    "words, the \n",
    "more\n",
    "symmetric the distribution, the faster it converges to the\n",
    "normal\n",
    "distribution\n",
    "according to the central limit theorem.\n",
    "\n",
    "### Probability\n",
    "Density for the\n",
    "Estimator\n",
    "\n",
    "To write out the full density for $\\hat{p}$, we first\n",
    "have to ask\n",
    "what is\n",
    "the probability that the estimator will equal a specific\n",
    "value and the\n",
    "tally up\n",
    "all the ways that could happen with their corresponding\n",
    "probabilities.\n",
    "For\n",
    "example, what is the probability that\n",
    "\n",
    "$$\n",
    "\\hat{p} =\n",
    "\\frac{1}{n}\\sum_{i=1}^n x_i  = 0\n",
    "$$\n",
    "\n",
    "  This can only happen one way: when $x_i=0\n",
    "\\hspace{0.5em} \\forall i$. The\n",
    "probability of this happening can be computed\n",
    "from the density\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x},p)= \\prod_{i=1}^n \\left(p^{x_i} (1-p)^{1-x_i}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f\\left(\\sum_{i=1}^n x_i  = 0,p\\right)= \\left(1-p\\right)^n\n",
    "$$\n",
    "Likewise, if $\\lbrace x_i \\rbrace$ has only one nonzero element, then\n",
    "\n",
    "$$\n",
    "f\\left(\\sum_{i=1}^n x_i  = 1,p\\right)= n p \\prod_{i=1}^{n-1} \\left(1-p\\right)\n",
    "$$\n",
    "where the $n$ comes from the $n$ ways to pick one element\n",
    "from the $n$ elements\n",
    "$x_i$. Continuing this way, we can construct the\n",
    "entire density as\n",
    "\n",
    "$$\n",
    "f\\left(\\sum_{i=1}^n x_i  = k,p\\right)= \\binom{n}{k} p^k  (1-p)^{n-k}\n",
    "$$\n",
    "\n",
    " where\n",
    "the first term on the right is the binomial coefficient of $n$ things\n",
    "taken $k$\n",
    "at a time. This is the binomial distribution and it's not the\n",
    "density\n",
    "for\n",
    "$\\hat{p}$, but rather for $n\\hat{p}$. We'll leave this as-is\n",
    "because it's\n",
    "easier\n",
    "to work with below. We just have to remember to keep\n",
    "track of the $n$\n",
    "factor.\n",
    "**Confidence Intervals**\n",
    "\n",
    "Now that we have the full density for\n",
    "$\\hat{p}$, we\n",
    "are ready to ask some\n",
    "meaningful questions. For example, what is\n",
    "the probability\n",
    "the estimator is within\n",
    "$\\epsilon$ fraction of the true value\n",
    "of $p$?\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left( \\vert  \\hat{p}-p \\vert  \\le \\epsilon p \\right)\n",
    "$$\n",
    "\n",
    " More\n",
    "concretely, we want to know how often the\n",
    "estimated $\\hat{p}$ is trapped\n",
    "within\n",
    "$\\epsilon$ of the actual value.  That is,\n",
    "suppose we ran the experiment\n",
    "1000\n",
    "times to generate 1000 different estimates\n",
    "of $\\hat{p}$. What percentage of\n",
    "the\n",
    "1000 so-computed values are trapped within\n",
    "$\\epsilon$ of the underlying\n",
    "value.\n",
    "Rewriting the above equation as the\n",
    "following,\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left(p-\\epsilon p\n",
    "< \\hat{p} < p + \\epsilon p \\right) =\n",
    "\\mathbb{P}\\left(  n p - n \\epsilon p <\n",
    "\\sum_{i=1}^n x_i < n p + n \\epsilon p\n",
    "\\right)\n",
    "$$\n",
    "\n",
    " Let's plug in some live\n",
    "numbers here for our worst case\n",
    "scenario (i.e., highest\n",
    "variance scenario) where\n",
    "$p=1/2$. Then, if\n",
    "$\\epsilon = 1/100$, we have\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left( \\frac{99\n",
    "n}{100} < \\sum_{i=1}^n x_i   < \\frac{101 n}{100}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    " Since the sum in\n",
    "integer-valued, we need $n> 100$ to even compute this.\n",
    "Thus,\n",
    "if $n=101$ we have,\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\mathbb{P}\\left(\\frac{9999}{200} < \\sum_{i=1}^{101} x_i <\n",
    "\\frac{10201}{200} \\right) = f\\left(\\sum_{i=1}^{101} x_i = 50,p\\right) &  \\ldots\n",
    "\\\\\\\n",
    "= \\binom{101}{50} (1/2)^{50}  (1-1/2)^{101-50} & = & 0.079\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    " This means that in the worst-case scenario for $p=1/2$, given $n=101$\n",
    "trials,\n",
    "we will only get within 1\\% of the actual $p=1/2$ about 8\\% of the\n",
    "time.\n",
    "If you\n",
    "feel disappointed, it is because you've been paying attention.\n",
    "What if\n",
    "the coin\n",
    "was really heavy and it was hard work to repeat this 101 times?\n",
    "\n",
    "Let's\n",
    "come at\n",
    "this another way: given I could only flip the coin 100\n",
    "times, how close\n",
    "could I\n",
    "come to the true underlying value with high\n",
    "probability (say, 95\\%)? In\n",
    "this\n",
    "case, instead of picking a value for\n",
    "$\\epsilon$, we are solving for\n",
    "$\\epsilon$.\n",
    "Plugging in gives,\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left(50 - 50\\epsilon <\n",
    "\\sum_{i=1}^{100} x_i < 50 + 50 \\epsilon\n",
    "\\right) = 0.95\n",
    "$$\n",
    "\n",
    " which we have to\n",
    "solve for $\\epsilon$. Fortunately, all the tools we\n",
    "need to\n",
    "solve for this are\n",
    "already in Scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "# n=100, p = 0.5, distribution of the estimator phat\n",
    "b=binom(100,.5) \n",
    "# symmetric sum the probability around the mean\n",
    "g = lambda i:b.pmf(np.arange(-i,i)+50).sum() \n",
    "print (g(10)) # approx 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import subplots, arange\n",
    "fig,ax= subplots()\n",
    "fig.set_size_inches((10,5))\n",
    "# here is the density of the sum of x_i\n",
    "_=ax.stem(arange(0,101),b.pmf(arange(0,101)),\n",
    "        linefmt='k-', markerfmt='ko') \n",
    "_=ax.vlines( [50+10,50-10],0 ,ax.get_ylim()[1] ,color='k',lw=3.)\n",
    "_=ax.axis(xmin=30,xmax=70)\n",
    "_=ax.tick_params(labelsize=18)\n",
    "fig.savefig('fig-statistics/Maximum_likelihood_20_2.png')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-statistics/Maximum_likelihood_20_2.png, width=500\n",
    "frac=0.85] Probability mass function for $\\hat{p}$. The two vertical lines form\n",
    "the confidence interval. <div id=\"fig:Maximum_likelihood_20_2\"></div> -->\n",
    "<!--\n",
    "begin figure -->\n",
    "<div id=\"fig:Maximum_likelihood_20_2\"></div>\n",
    "\n",
    "<p>Probability\n",
    "mass function for $\\hat{p}$. The two vertical lines form the confidence\n",
    "interval.</p>\n",
    "<img src=\"fig-statistics/Maximum_likelihood_20_2.png\" width=500>\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    " The two vertical lines in\n",
    "[Figure](#fig:Maximum_likelihood_20_2)\n",
    "show how far out from the mean we have to\n",
    "go to accumulate 95\\% of the\n",
    "probability. Now, we can solve this as\n",
    "\n",
    "$$\n",
    "50+50\\epsilon=60\n",
    "$$\n",
    "\n",
    " which makes $\\epsilon=1/5$ or 20\\%. So, flipping 100 times\n",
    "means I can\n",
    "only get\n",
    "within 20\\% of the real $p$ 95\\% of the time in the worst\n",
    "case\n",
    "scenario (i.e.,\n",
    "$p=1/2$). The following code verifies the situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli \n",
    "b=bernoulli(0.5) # coin distribution\n",
    "xs = b.rvs(100) # flip it 100 times\n",
    "phat = np.mean(xs) # estimated p\n",
    "print (abs(phat-0.5) < 0.5*0.20) # make it w/in interval?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep doing this and see if we can get within this interval 95\\% of\n",
    "the\n",
    "time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=[]\n",
    "b=bernoulli(0.5) # coin distribution\n",
    "for i in range(500):   # number of tries\n",
    "    xs = b.rvs(100)    # flip it 100 times\n",
    "    phat = np.mean(xs) # estimated p\n",
    "    out.append(abs(phat-0.5) < 0.5*0.20 ) # within 20% ?\n",
    "\n",
    "# percentage of tries w/in 20% interval\n",
    "print (100*np.mean(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that seems to work! Now we have a way to get at the quality of\n",
    "the\n",
    "estimator, $\\hat{p}$.\n",
    "\n",
    "**Maximum Likelihood Estimator Without Calculus**\n",
    "\n",
    "The\n",
    "prior example showed how we can use calculus to compute the maximum\n",
    "likelihood\n",
    "estimator. It's important to emphasize that the maximum likelihood\n",
    "principle\n",
    "does *not* depend on calculus and extends to more general situations\n",
    "where\n",
    "calculus is impossible. For example, let $X$ be uniformly distributed in\n",
    "the\n",
    "interval $[0,\\theta]$.  Given $n$ measurements of $X$, the likelihood\n",
    "function\n",
    "is the following:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^n \\frac{1}{\\theta} =\n",
    "\\frac{1}{\\theta^n}\n",
    "$$\n",
    "\n",
    " where each $x_i \\in [0,\\theta]$. Note that the slope of\n",
    "this function\n",
    "is not\n",
    "zero anywhere so the usual calculus approach is not going\n",
    "to work here.\n",
    "Because\n",
    "the likelihood is the product of the individual uniform\n",
    "densities, if\n",
    "any of the\n",
    "$x_i$ values were outside of the proposed $[0,\\theta]$\n",
    "interval,\n",
    "then the\n",
    "likelihood would go to zero, because the uniform density is\n",
    "zero\n",
    "outside of the\n",
    "$[0,\\theta]$. This is no good for maximization. Thus,\n",
    "observing\n",
    "that the\n",
    "likelihood function is strictly decreasing with increasing\n",
    "$\\theta$,\n",
    "we conclude\n",
    "that the value for $\\theta$ that maximizes the likelihood\n",
    "is the\n",
    "maximum of the\n",
    "$x_i$ values. To summarize, the maximum likelihood\n",
    "estimator is\n",
    "the following:\n",
    "\n",
    "$$\n",
    "\\theta_{ML} = \\max_i x_i\n",
    "$$\n",
    "\n",
    " As always, we want\n",
    "the distribution of this estimator to judge its\n",
    "performance.\n",
    "In this case, this\n",
    "is pretty straightforward. The cumulative\n",
    "density function\n",
    "for the $\\max$\n",
    "function is the following:\n",
    "\n",
    "$$\n",
    "\\mathbb{P} \\left( \\hat{\\theta}_{ML} < v \\right) =\n",
    "\\mathbb{P}( x_0 \\leq v\n",
    "\\wedge x_1 \\leq v \\ldots \\wedge x_n \\leq v)\n",
    "$$\n",
    "\n",
    " and\n",
    "since all the $x_i$ are uniformly distributed in $[0,\\theta]$, we have\n",
    "\n",
    "$$\n",
    "\\mathbb{P} \\left( \\hat{\\theta}_{ML} < v  \\right) =\n",
    "\\left(\\frac{v}{\\theta}\\right)^n\n",
    "$$\n",
    "\n",
    " So, the probability density function is\n",
    "then,\n",
    "\n",
    "$$\n",
    "f_{\\hat{\\theta}_{ML}}(\\theta_{ML}) =  n \\theta_{ML}^{ n-1 } \\theta^{\n",
    "-n }\n",
    "$$\n",
    "\n",
    " Then, we can compute the $\\mathbb{E}(\\theta_{ML}) = (\\theta n)/(n+1)$\n",
    "with\n",
    "corresponding variance as $\\mathbb{V}(\\theta_{ML}) = (\\theta^2\n",
    "n)/(n+1)^2/(n+2)$.\n",
    "\n",
    "For a quick sanity check, we can write the following\n",
    "simulation for $\\theta =1$\n",
    "as in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from scipy import stats\n",
    ">>> rv = stats.uniform(0,1)  # define uniform random variable\n",
    ">>> mle=rv.rvs((100,500)).max(0) # max along row-dimension\n",
    ">>> print (mean(mle)) # approx n/(n+1) = 100/101 ~= 0.99\n",
    "0.989942138048\n",
    ">>> print (var(mle)) #approx n/(n+1)**2/(n+2) ~= 9.61E-5\n",
    "9.95762009884e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The `max(0)` suffix on for the `mle` computation takes\n",
    "the\n",
    "maximum of the so-computed array along the row (`axis=0`)\n",
    "dimension.\n",
    "\n",
    "\n",
    "\n",
    " You can\n",
    "also plot `hist(mle)` to see the histogram of the simulated\n",
    "maximum likelihood\n",
    "estimates and match it up against the probability density\n",
    "function we derived\n",
    "above.  \n",
    "\n",
    "\n",
    "In this section, we explored the concept of maximum\n",
    "likelihood\n",
    "estimation using a coin flipping experiment both analytically and\n",
    "numerically\n",
    "with the scientific Python stack. We also explored the case when\n",
    "calculus is not\n",
    "workable for maximum likelihood estimation.  There are two key\n",
    "points to\n",
    "remember. First, maximum likelihood estimation produces a function of\n",
    "the data\n",
    "that is itself a random variable, with its own probability\n",
    "distribution. We can\n",
    "get at the quality of the so-derived estimators by\n",
    "examining the confidence\n",
    "intervals around the estimated values using the\n",
    "probability distributions\n",
    "associated with the estimators themselves.  \n",
    "Second, maximum likelihood\n",
    "estimation applies even in situations \n",
    "where using basic calculus is not\n",
    "applicable [[wasserman2004all]](#wasserman2004all).\n",
    "\n",
    "\n",
    "## Delta Method\n",
    "<div\n",
    "id=\"sec:delta_method\"></div>\n",
    "\n",
    "Sometimes we want to characterize the distribution\n",
    "of a *function* of a random\n",
    "variable. In order to extend and generalize the\n",
    "Central Limit Theorem in this\n",
    "way, we need the Taylor series expansion. Recall\n",
    "that the Taylor series\n",
    "expansion is an approximation of a function of the\n",
    "following form,\n",
    "\n",
    "$$\n",
    "T_r(x) =\\sum_{i=0}^r \\frac{g^{(i)}(a)}{i!}(x-a)^i\n",
    "$$\n",
    "\n",
    " this\n",
    "basically says that a function $g$ can be adequately\n",
    "approximated about a\n",
    "point\n",
    "$a$ using a polynomial based on its derivatives\n",
    "evaluated at $a$. Before\n",
    "we\n",
    "state the general theorem, let's examine\n",
    "an example to understand how the\n",
    "mechanics work.\n",
    "\n",
    "**Example.**  Suppose that $X$ is a random variable with\n",
    "$\\mathbb{E}(X)=\\mu\\neq 0$.  Furthermore, supposedly have a suitable\n",
    "function $g$\n",
    "and we want the distribution of $g(X)$. Applying the\n",
    "Taylor series expansion, we\n",
    "obtain the following,\n",
    "\n",
    "$$\n",
    "g(X) \\approx g(\\mu)+ g^{\\prime}(\\mu)(X-\\mu)\n",
    "$$\n",
    "\n",
    " If we\n",
    "use $g(X)$  as an estimator for $g(\\mu)$, then we can say that\n",
    "we\n",
    "approximately\n",
    "have the following\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(g(X)) &=g(\\mu) \\\\\\\n",
    "\\mathbb{V}(g(X))\n",
    "&=(g^{\\prime}(\\mu))^2 \\mathbb{V}(X) \\\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Concretely, suppose we want to estimate the odds, $\\frac{p}{1-p}$.\n",
    "For example,\n",
    "if $p=2/3$, then we say that the odds is `2:1` meaning that the\n",
    "odds of the one\n",
    "outcome are twice as likely as the odds of the other outcome.\n",
    "Thus, we have\n",
    "$g(p)=\\frac{p}{1-p}$ and we want to find\n",
    "$\\mathbb{V}(g(\\hat{p}))$.  In our coin-\n",
    "flipping problem, we have the\n",
    "estimator $\\hat{p}=\\frac{1}{n}\\sum X_k$ from the\n",
    "Bernoulli-distributed data\n",
    "$X_k$ individual coin-flips. Thus,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(\\hat{p}) &= p  \\\\\\\n",
    "\\mathbb{V}(\\hat{p}) &=\n",
    "\\frac{p(1-p)}{n}  \\\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "  Now, $g^\\prime(p)=1/(1-p)^2$, so we have,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{V}(g(\\hat{p}))&=(g^\\prime(p))^2 \\mathbb{V}(\\hat{p})\n",
    "\\\\\\\n",
    "&=\\left(\\frac{1}{(1-p)^2}\\right)^2 \\frac{p(1-p)}{n}\n",
    "\\\\\\\n",
    "                      &=\n",
    "\\frac{p}{n(1-p)^3}  \\\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    " which is an approximation of the\n",
    "variance of the estimator\n",
    "$g(\\hat{p})$. Let's\n",
    "simulate this and see how it\n",
    "agrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# compute MLE estimates \n",
    "d=stats.bernoulli(0.1).rvs((10,5000)).mean(0)\n",
    "# avoid divide-by-zero\n",
    "d=d[np.logical_not(np.isclose(d,1))]\n",
    "# compute odds ratio\n",
    "odds = d/(1-d)\n",
    "print ('odds ratio=',np.mean(odds),'var=',np.var(odds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first number above is the mean of the simulated odds\n",
    "ratio and the second\n",
    "is\n",
    "the variance of the estimate.  According to\n",
    "the variance estimate above, we\n",
    "have\n",
    "$\\mathbb{V}(g(1/10))\\approx\n",
    "0.0137$, which is not too bad for this\n",
    "approximation. Recall we want\n",
    "to estimate the odds from $\\hat{p}$. The code\n",
    "above takes $5000$\n",
    "estimates of the $\\hat{p}$ to estimate $\\mathbb{V}(g)$. The\n",
    "odds ratio\n",
    "for $p=1/10$ is $1/9\\approx 0.111$.\n",
    "\n",
    "**Programming Tip.**\n",
    "\n",
    "The code\n",
    "above uses the `np.isclose` function to identify the ones from\n",
    "the simulation\n",
    "and the `np.logical_not` removes these elements from the\n",
    "data because the odds\n",
    "ratio has a zero in the denominator\n",
    "for these values.\n",
    "\n",
    "\n",
    "\n",
    "Let's try this again\n",
    "with a probability of heads of `0.5` instead of\n",
    "`0.3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "d=stats.bernoulli(.5).rvs((10,5000)).mean(0)\n",
    "d=d[np.logical_not(np.isclose(d,1))]\n",
    "print( 'odds ratio=',np.mean(d),'var=',np.var(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds ratio is this case is equal to one, which\n",
    "is not close to what was\n",
    "reported. According to our\n",
    "approximation, we should have $\\mathbb{V}(g)=0.4$,\n",
    "which does not\n",
    "look like what our simulation just reported.  This is\n",
    "because the\n",
    "approximation is best when the odds ratio is\n",
    "nearly linear and worse otherwise\n",
    "(see [Figure](#fig:Maximum_likelihood_0001)).\n",
    "\n",
    "<!-- dom:FIGURE: [fig-\n",
    "statistics/Maximum_likelihood_0001.png, width=500 frac=0.85] The odds ratio is\n",
    "close to linear for small values but becomes unbounded as $p$ approaches one.\n",
    "The delta method is more effective for small underlying values of $p$, where the\n",
    "linear approximation is better. <div id=\"fig:Maximum_likelihood_0001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Maximum_likelihood_0001\"></div>\n",
    "\n",
    "<p>The odds\n",
    "ratio is close to linear for small values but becomes unbounded as $p$\n",
    "approaches one. The delta method is more effective for small underlying values\n",
    "of $p$, where the linear approximation is better.</p>\n",
    "<img src=\"fig-\n",
    "statistics/Maximum_likelihood_0001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
