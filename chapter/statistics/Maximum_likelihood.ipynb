{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimation problem starts with the desire to infer something meaningful\n",
    "from data.  For parametric estimation, the strategy is to postulate a model for\n",
    "the data and then use the data to fit model parameters.  This leads to two\n",
    "fundamental questions: where to get the model and how to estimate the\n",
    "parameters? The first question is best answered by the maxim: *all models are\n",
    "wrong, some are useful*. In other words, choosing a model depends as much on\n",
    "the application as on the model itself. Think about models as building\n",
    "different telescopes to view the sky. No one would ever claim that the\n",
    "telescope generates the sky! It is same with data models. Models give us\n",
    "multiple perspectives on the data that themselves are proxies for some deeper\n",
    "underlying phenomenon.\n",
    "\n",
    "Some categories of data may be more commonly studied using certain types of\n",
    "models, but this is usually very domain-specific and ultimately depends on the\n",
    "aims of the analysis. In some cases, there may be strong physical reasons\n",
    "behind choosing a model.  For example, one could postulate that the model is\n",
    "linear with some noise as in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Y = a X + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which basically says that you, as the experimenter, dial in some\n",
    "value for $X$ and then read off something directly proportional to $X$ as the\n",
    "measurement, $Y$, plus some additive noise that you attribute to jitter in the\n",
    "apparatus. Then, the next step is to estimate the paramater $a$ in the model,\n",
    "given some postulated claim about the nature of $\\epsilon$. How to compute the\n",
    "model parameters depends on the particular methodology. The two broad rubrics\n",
    "are parametric and non-parametric estimation. In the former, we assume we know\n",
    "the density function of the data and then try to derive the embedded parameters\n",
    "for it. In the latter, we claim only to know that the density function is a\n",
    "member of a broad class of density functions and then use the data\n",
    "to characterize a member of that class. Broadly speaking, the former consumes\n",
    "less data than the latter, because there are fewer unknowns to compute from\n",
    "the data.\n",
    "\n",
    "Let's concentrate on parametric estimation for now. The tradition is to denote\n",
    "the unknown parameter to be estimated as $\\theta$ which is a member of a large\n",
    "space of alternates, $\\Theta$. To judge between potential $\\theta$ values, we\n",
    "need an objective function, known as a *risk* function,\n",
    "$L(\\theta,\\hat{\\theta})$, where $\\hat{\\theta}(\\mathbf{x})$ is an\n",
    "estimate for the unknown $\\theta$ that is derived from the available\n",
    "data $\\mathbf{x}$. The most common and useful risk function is the\n",
    "squared error loss,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\theta,\\hat{\\theta}) = (\\theta-\\hat{\\theta})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Although neat, this is not practical because we need to know the\n",
    "unknown $\\theta$ to compute it. The other problem is because $\\hat{\\theta}$ is\n",
    "a function of the observed data, it is also a random variable with its own\n",
    "probability density function.  This leads to the notion of the *expected risk*\n",
    "function,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "R(\\theta,\\hat{\\theta}) = \\mathbb{E}_\\theta(L(\\theta,\\hat{\\theta})) = \\int L(\\theta,\\hat{\\theta}(\\mathbf{x})) f(\\mathbf{x};\\theta) d \\mathbf{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In other words, given a fixed $\\theta$, integrate over the\n",
    "probability density function of the data, $f(\\mathbf{x})$,  to compute the\n",
    "risk. Plugging in for the squared error loss,  we compute the\n",
    "mean squared error,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}_\\theta(\\theta-\\hat{\\theta})^2 =\\int (\\theta-\\hat{\\theta})^2 f(\\mathbf{x};\\theta) d \\mathbf{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This has the important factorization into the *bias*,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textnormal{bias} = \\mathbb{E}_\\theta(\\hat{\\theta})-\\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " with the corresponding variance, $\\mathbb{V}_\\theta(\\hat{\\theta})$ as\n",
    "in the following *mean squared error* (MSE):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}_\\theta(\\theta-\\hat{\\theta})^2= \\textnormal{bias}^2+\\mathbb{V}_\\theta(\\hat{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is an important trade-off that we will return to repeatedly. The\n",
    "idea is the bias is nonzero when the estimator $\\hat{\\theta}$, integrated\n",
    "over all possible data, $f(\\mathbf{x})$, does not equal the underlying target\n",
    "parameter $\\theta$. In some sense, the estimator misses the target, no matter\n",
    "how much data is used.  When the bias equals zero, the estimated is *unbiased*.\n",
    "For fixed MSE, low bias implies high variance and vice-versa. This trade-off\n",
    "was once not emphasized and instead much attention was paid to the smallest\n",
    "variance of unbiased estimators (see Cramer-Rao bounds). In practice,\n",
    "understanding and exploiting the trade-off between bias and variance and\n",
    "reducing the MSE is more important.\n",
    "\n",
    "With all this set up, we can now ask how bad can bad get by\n",
    "examining *minimax* risk,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "R_{\\textnormal{mmx}} = \\inf_{\\hat{\\theta}} \\sup_\\theta R(\\theta,\\hat{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where the $\\inf$ is take over all estimators.  Intuitively, this\n",
    "means if we found the worst possible $\\theta$ and swept over all possible\n",
    "parameter estimators $\\hat{\\theta}$, and then took the smallest possible risk\n",
    "we could find, we would have the minimax risk. Thus, an estimator,\n",
    "$\\hat{\\theta}_{\\textnormal{mmx}}$, is a *minimax estimator* if it achieves this\n",
    "feat,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sup_\\theta R(\\theta,\\hat{\\theta}_{\\textnormal{mmx}}) =\\inf_{\\hat{\\theta}} \\sup_\\theta R(\\theta,\\hat{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In other words, even in the face of the worst $\\theta$ (i.e., the\n",
    "$\\sup_\\theta$), $\\hat{\\theta}_{\\textnormal{mmx}}$ still achieves the minimax\n",
    "risk. There is a greater theory that revolves around minimax estimators of\n",
    "various kinds, but this is far beyond our scope here. The main thing to focus\n",
    "on is that under certain technical but easily satisfiable conditions, the\n",
    "maximum likelihood estimator is approximately minimax. Maximum likelihood is\n",
    "the subject of the next section.  Let's get started with the simplest\n",
    "application: coin-flipping.\n",
    "\n",
    "## Setting up the Coin Flipping Experiment\n",
    "\n",
    "Suppose we have coin and want to estimate the probability of heads ($p$) for\n",
    "it. We model the distribution of heads and tails as a Bernoulli distribution\n",
    "with the following probability mass function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\phi(x)= p^x (1-p)^{(1-x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $x$ is the outcome, *1* for heads and *0* for tails. Note that\n",
    "maximum likelihood is a parametric method that requires the specification of a\n",
    "particular model for which we will compute embedded parameters. For $n$\n",
    "independent flips, we have the joint density as the product of $n$ of\n",
    "these functions as in,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\phi(\\mathbf{x})=\\prod_{i=1}^n p^x_i (1-p)^{(1-x_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following is the *likelihood function*,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}(p ; \\mathbf{x})= \\prod_{i=1}^n p^{ x_i }(1-p)^{1-x_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is basically notation. We have just renamed the\n",
    "previous equation to emphasize the $p$ parameter, which is what\n",
    "we want to estimate.\n",
    "\n",
    "The principle of *maximum likelihood* is to maximize the likelihood as the\n",
    "function of $p$ after plugging in all of the $x_i$ data. We then call this\n",
    "maximizer $\\hat{p}$ which is a function of the observed $x_i$ data, and as\n",
    "such, is a random variable with its own distribution. This method therefore\n",
    "ingests data and an assumed model for the probability density, and produces a\n",
    "function that estimates the embedded parameter in the assumed probability\n",
    "density.  Thus, maximum likelihood generates the *functions* of data that we\n",
    "need in order to get at the underlying parameters of the model. Note that there\n",
    "is no limit to the ways we can functionally manipulate the data we have\n",
    "collected. The maximum likelihood principle gives us a systematic method for\n",
    "constructing these functions subject to the assumed model. This is a point\n",
    "worth emphasizing: the maximum likelihood principle yields functions as\n",
    "solutions the same way solving differential equations yields functions as\n",
    "solutions. It is very, very much harder to produce a function than to produce a\n",
    "value as a solution, even with the assumption of a convenient probability\n",
    "density.  Thus, the power of the principle is that you can construct such\n",
    "functions subject to the model assumptions.\n",
    "\n",
    "### Simulating the Experiment\n",
    "\n",
    "We need the following code to simulate coin flipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli \n",
    "p_true=1/2.0         # estimate this!\n",
    "fp=bernoulli(p_true) # create bernoulli random variate\n",
    "xs = fp.rvs(100)     # generate some samples\n",
    "print (xs[:30])        # see first 30 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, we can write out the likelihood function using Sympy.  Note\n",
    "that we give the Sympy variables the `positive=True` attribute upon\n",
    "construction because this eases Sympy's internal simplification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sympy\n",
    "x,p,z=sympy.symbols('x p z', positive=True)\n",
    "phi=p**x*(1-p)**(1-x) # distribution function\n",
    "L=np.prod([phi.subs(x,i) for i in xs]) # likelihood function \n",
    "print (L) # approx 0.5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that, once we plug in the data, the likelihood function is\n",
    "solely a function of the unknown parameter ($p$ in this case).  The following\n",
    "code uses calculus to find the extrema of the likelihood function.  Note that\n",
    "taking the `log` of $L$ makes the maximization problem tractable but doesn't\n",
    "change the extrema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logL=sympy.expand_log(sympy.log(L))\n",
    "sol,=sympy.solve(sympy.diff(logL,p),p)\n",
    "print (sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "Note that `sol,=sympy.solve` statement includes\n",
    "a comma after the `sol` variable. This is because the `solve`\n",
    "function returns a list containing a single element. Using\n",
    "this assignment unpacks that single element into the `sol` variable\n",
    "directly. This is another one of the many small elegancies of Python.\n",
    "\n",
    " \n",
    "\n",
    "The following code generates [Figure](#fig:Maximum_likelihood_10_2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fig,ax=subplots()\n",
    "x=np.linspace(0,1,100)\n",
    "ax.plot(x,map(sympy.lambdify(p,logJ,'numpy'),x),'k-',lw=3)\n",
    "ax.plot(sol,logJ.subs(p,sol),'o',\n",
    "        color='gray',ms=15,label='Estimated')\n",
    "ax.plot(p_true,logJ.subs(p,p_true),'s',\n",
    "        color='k',ms=15,label='Actual')\n",
    "ax.set_xlabel('$p$',fontsize=18)\n",
    "ax.set_ylabel('Likelihood',fontsize=18)\n",
    "ax.set_title('Estimate not equal to true value',fontsize=18)\n",
    "ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "In the prior code, we use the `lambdify` function in `lambdify(p,logJ,'numpy')` to\n",
    "take a Sympy expression and convert it into a Numpy version that is easier to\n",
    "compute.  The `lambdify` function has an extra argument where you can specify\n",
    "the function space that it should use to convert the expression. In the above\n",
    "this is set to Numpy.\n",
    "\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig-statistics/Maximum_likelihood_10_2.png, width=500 frac=0.75] Maximum likelihood estimate vs. true parameter. Note that the estimate is slightly off from the true value. This is a consequence of the fact that the estimator is a function of the data and lacks knowledge of the true underlying value.  <div id=\"fig:Maximum_likelihood_10_2\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Maximum_likelihood_10_2\"></div>\n",
    "\n",
    "<p>Maximum likelihood estimate vs. true parameter. Note that the estimate is slightly off from the true value. This is a consequence of the fact that the estimator is a function of the data and lacks knowledge of the true underlying value.</p>\n",
    "<img src=\"fig-statistics/Maximum_likelihood_10_2.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "[Figure](#fig:Maximum_likelihood_10_2) shows  that our estimator $\\hat{p}$\n",
    "(circle) is not equal to the true value of $p$ (square), despite being\n",
    "the maximum of the likelihood function. This may sound disturbing, but keep in\n",
    "mind this estimate is a function of the random data; and since that data can\n",
    "change, the ultimate estimate can likewise change. \n",
    "Remember that the estimator is a *function* of the data and is thus also a\n",
    "*random variable*, just like the data is. This means it has its own probability\n",
    "distribution with corresponding mean and variance. So, what we are observing is\n",
    "a consequence of that variance.\n",
    "\n",
    "<!-- !bc pycod -->\n",
    "<!-- def estimator_gen(niter=10,ns=100): -->\n",
    "<!-- 'generate data to estimate distribution of maximum likelihood estimator' -->\n",
    "<!-- out=[] -->\n",
    "<!-- # make sympy variable real-valued -->\n",
    "<!-- x=sympy.symbols('x',real=True) -->\n",
    "<!-- # likelihood function -->\n",
    "<!-- L= p**x*(1-p)**(1-x) -->\n",
    "<!-- for i in range(niter): -->\n",
    "<!-- # generate some samples from the experiment -->\n",
    "<!-- xs = sample(ns) -->\n",
    "<!-- # objective function to maximize -->\n",
    "<!-- J=np.prod([L.subs(x,i) for i in xs]) -->\n",
    "<!-- # log is easier to work with -->\n",
    "<!-- logJ=sympy.expand_log(sympy.log(J)) -->\n",
    "<!-- # use basic calculus to find extrema -->\n",
    "<!-- sol=sympy.solve(sympy.diff(logJ,p),p)[0] -->\n",
    "<!-- # convert output to numeric float from sympy -->\n",
    "<!-- out.append(float(sol.evalf())) -->\n",
    "<!-- # return scalar if list contains only 1 term -->\n",
    "<!-- return out if len(out)>1 else out[0] -->\n",
    "<!-- !ec -->\n",
    "\n",
    "<!-- dom:FIGURE: [fig-statistics/Maximum_likelihood_30_2.png, width=500 frac=0.85] Histogram of maximum likelihood estimates. The title shows the estimated mean and standard deviation of the samples. <div id=\"fig:Maximum_likelihood_30_2\"></div>  -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Maximum_likelihood_30_2\"></div>\n",
    "\n",
    "<p>Histogram of maximum likelihood estimates. The title shows the estimated mean and standard deviation of the samples.</p>\n",
    "<img src=\"fig-statistics/Maximum_likelihood_30_2.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "[Figure](#fig:Maximum_likelihood_30_2) shows what happens when you run many\n",
    "thousands of coin experiments and compute the maximum likelihood\n",
    "estimate for each experiment, given a particular number of samples \n",
    "per experiment. This simulation gives us a histogram of the maximum likelihood\n",
    "estimates, which is an approximation of the probability distribution of the\n",
    "$\\hat{p}$ estimator itself.  This figure shows that the sample mean\n",
    "of the estimator ($\\mu = \\frac{1}{n}\\sum \\hat{p}_i$) is pretty close to the\n",
    "true value, but looks can be deceiving. The only way to know for sure is to\n",
    "check if the estimator is unbiased, namely, if"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(\\hat{p}) = p\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Because this problem is simple, we can solve for this in general\n",
    "noting that the terms above are either $p$, if $x_i=1$ or $1-p$ if $x_i=0$.\n",
    "This means that we can write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}(p\\vert \\mathbf{x})= p^{\\sum_{i=1}^n x_i}(1-p)^{n-\\sum_{i=1}^n x_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " with corresponding logarithm as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J=\\log(\\mathcal{L}(p\\vert \\mathbf{x})) =  \\log(p)  \\sum_{i=1}^n x_i +   \\log(1-p) \\left(n-\\sum_{i=1}^n x_i\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Taking the derivative of this gives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dJ}{dp} = \\frac{1}{p}\\sum_{i=1}^n x_i + \\frac{(n-\\sum_{i=1}^n x_i)}{p-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and solving this for $p$ leads to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{p} = \\frac{1}{ n} \\sum_{i=1}^n x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our *estimator* for $p$. Up until now, we have been using Sympy to\n",
    "solve for this based on the data $x_i$ but now that we have it analytically we\n",
    "don't have to solve for it each time. To check if this estimator is biased, we\n",
    "compute its expectation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left(\\hat{p}\\right) =\\frac{1}{n}\\sum_i^n \\mathbb{E}(x_i) = \\frac{1}{n} n \\mathbb{E}(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " by linearity of the expectation and where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(x_i)  = p\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Therefore,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left(\\hat{p}\\right) =p\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This means that the estimator is *unbiased*. Similarly,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left(\\hat{p}^2\\right) = \\frac{1}{n^2} \\mathbb{E}\\left[\\left(  \\sum_{i=1}^n x_i \\right)^2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left(x_i^2\\right) =p\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and by the independence assumption,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left(x_i x_j\\right) =\\mathbb{E}(x_i)\\mathbb{E}(x_j) =p^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Thus,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left(\\hat{p}^2\\right) =\\left(\\frac{1}{n^2}\\right) n \\left[ p+(n-1)p^2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " So, the variance of the estimator, $\\hat{p}$, is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{V}(\\hat{p}) = \\mathbb{E}\\left(\\hat{p}^2\\right)- \\mathbb{E}\\left(\\hat{p}\\right)^2  = \\frac{p(1-p)}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that the $n$ in the denominator means that the variance\n",
    "asymptotically goes to zero as $n$ increases (i.e., we consider more and\n",
    "more samples). This is good news because it means that more and\n",
    "more coin flips lead to a better estimate of the underlying $p$.\n",
    "\n",
    "Unfortunately, this formula for the variance is practically useless because we\n",
    "need $p$ to compute it and $p$ is the parameter we are trying to estimate in\n",
    "the first place! However, this is where the *plug-in* principle [^invariance-property] \n",
    "saves the day.  It turns out in this situation, you can\n",
    "simply substitute the maximum likelihood estimator, $\\hat{p}$, for the $p$ in\n",
    "the above equation to obtain the asymptotic variance for $\\mathbb{V}(\\hat{p})$.\n",
    "The fact that this works is guaranteed by the asymptotic theory of maximum\n",
    "likelihood estimators.\n",
    "\n",
    "[^invariance-property]: This is also known as the *invariance property*\n",
    "of maximum likelihood estimators. It basically states that the \n",
    "maximum likelihood estimator of any function, say, $h(\\theta)$, is\n",
    "the same $h$ with the maximum likelihood estimator for $\\theta$ substituted\n",
    "in for $\\theta$; namely, $h(\\theta_{ML})$.\n",
    "\n",
    "Nevertheless, looking at $\\mathbb{V}(\\hat{p})^2$, we can immediately notice\n",
    "that if $p=0$, then there is no estimator variance because the outcomes are\n",
    "guaranteed to be tails. Also, for any $n$,  the maximum of this variance\n",
    "happens at $p=1/2$. This is our worst case scenario and the only way to\n",
    "compensate is with larger $n$.\n",
    "\n",
    "All we have computed is the mean and variance of the estimator. In general,\n",
    "this is insufficient to characterize the underlying probability density of\n",
    "$\\hat{p}$, except if we somehow knew that $\\hat{p}$ were normally distributed.\n",
    "This is where the powerful *Central Limit Theorem* we discussed in the section [ch:stats:sec:limit](#ch:stats:sec:limit) comes in. The form of the estimator, which is just a\n",
    "sample mean, implies that we can apply this theorem and conclude that $\\hat{p}$\n",
    "is asymptotically normally distributed. However, it doesn't quantify how many\n",
    "samples $n$ we need. In our simulation this is no problem because we can\n",
    "generate as much data as we like, but in the real world, with a costly\n",
    "experiment, each sample may be precious [^edgeworth].  \n",
    "In the following, we won't apply the Central Limit Theorem and instead proceed\n",
    "analytically.\n",
    "\n",
    "[^edgeworth]: It turns out that the central limit theorem augmented with an\n",
    "Edgeworth expansion tells us that convergence is regulated by the skewness\n",
    "of the distribution [[feller1950introduction]](#feller1950introduction). In other words, the \n",
    "more symmetric the distribution, the faster it converges to the normal\n",
    "distribution according to the central limit theorem.\n",
    "\n",
    "### Probability Density for the Estimator\n",
    "\n",
    "To write out the full density for $\\hat{p}$, we first have to ask what is\n",
    "the probability that the estimator will equal a specific value and the tally up\n",
    "all the ways that could happen with their corresponding probabilities. For\n",
    "example, what is the probability that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{p} = \\frac{1}{n}\\sum_{i=1}^n x_i  = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  This can only happen one way: when $x_i=0 \\hspace{0.5em} \\forall i$. The\n",
    "probability of this happening can be computed from the density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(\\mathbf{x},p)= \\prod_{i=1}^n \\left(p^{x_i} (1-p)^{1-x_i}  \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f\\left(\\sum_{i=1}^n x_i  = 0,p\\right)= \\left(1-p\\right)^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Likewise, if $\\lbrace x_i \\rbrace$ has only one nonzero element, then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f\\left(\\sum_{i=1}^n x_i  = 1,p\\right)= n p \\prod_{i=1}^{n-1} \\left(1-p\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where the $n$ comes from the $n$ ways to pick one element\n",
    "from the $n$ elements $x_i$. Continuing this way, we can construct the\n",
    "entire density as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f\\left(\\sum_{i=1}^n x_i  = k,p\\right)= \\binom{n}{k} p^k  (1-p)^{n-k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where the first term on the right is the binomial coefficient of $n$ things\n",
    "taken $k$ at a time. This is the binomial distribution and it's not the\n",
    "density for $\\hat{p}$, but rather for $n\\hat{p}$. We'll leave this as-is\n",
    "because it's easier to work with below. We just have to remember to keep\n",
    "track of the $n$ factor.\n",
    "\n",
    "**Confidence Intervals**\n",
    "\n",
    "Now that we have the full density for $\\hat{p}$, we are ready to ask some\n",
    "meaningful questions. For example, what is the probability  the estimator is within\n",
    "$\\epsilon$ fraction of the true value of $p$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}\\left( \\vert  \\hat{p}-p \\vert  \\le \\epsilon p \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " More concretely, we want to know how often the\n",
    "estimated $\\hat{p}$ is trapped within $\\epsilon$ of the actual value.  That is,\n",
    "suppose we ran the experiment 1000 times to generate 1000 different estimates\n",
    "of $\\hat{p}$. What percentage of the 1000 so-computed values are trapped within\n",
    "$\\epsilon$ of the underlying value.  Rewriting the above equation as the\n",
    "following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}\\left(p-\\epsilon p < \\hat{p} < p + \\epsilon p \\right) = \\mathbb{P}\\left(  n p - n \\epsilon p < \\sum_{i=1}^n x_i < n p + n \\epsilon p \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's plug in some live numbers here for our worst case\n",
    "scenario (i.e., highest variance scenario) where $p=1/2$. Then, if\n",
    "$\\epsilon = 1/100$, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}\\left( \\frac{99 n}{100} < \\sum_{i=1}^n x_i   < \\frac{101 n}{100} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Since the sum in integer-valued, we need $n> 100$ to even compute this.\n",
    "Thus, if $n=101$ we have,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\mathbb{P}\\left(\\frac{9999}{200} < \\sum_{i=1}^{101} x_i < \\frac{10201}{200} \\right) = f\\left(\\sum_{i=1}^{101} x_i = 50,p\\right) &  \\ldots \\\\\\\n",
    "= \\binom{101}{50} (1/2)^{50}  (1-1/2)^{101-50} & = & 0.079\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This means that in the worst-case scenario for $p=1/2$, given $n=101$\n",
    "trials, we will only get within 1\\% of the actual $p=1/2$ about 8\\% of the\n",
    "time. If you feel disappointed, it is because you've been paying attention.\n",
    "What if the coin was really heavy and it was hard work to repeat this 101 times?\n",
    "\n",
    "Let's come at this another way: given I could only flip the coin 100\n",
    "times, how close could I come to the true underlying value with high\n",
    "probability (say, 95\\%)? In this case, instead of picking a value for\n",
    "$\\epsilon$, we are solving for $\\epsilon$. Plugging in gives,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}\\left(50 - 50\\epsilon < \\sum_{i=1}^{100} x_i < 50 + 50 \\epsilon  \\right) = 0.95\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which we have to solve for $\\epsilon$. Fortunately, all the tools we\n",
    "need to solve for this are already in Scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "# n=100, p = 0.5, distribution of the estimator phat\n",
    "b=binom(100,.5) \n",
    "# symmetric sum the probability around the mean\n",
    "g = lambda i:b.pmf(np.arange(-i,i)+50).sum() \n",
    "print (g(10)) # approx 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import subplots, arange\n",
    "fig,ax= subplots()\n",
    "fig.set_size_inches((10,5))\n",
    "# here is the density of the sum of x_i\n",
    "_=ax.stem(arange(0,101),b.pmf(arange(0,101)),\n",
    "        linefmt='k-', markerfmt='ko') \n",
    "_=ax.vlines( [50+10,50-10],0 ,ax.get_ylim()[1] ,color='k',lw=3.)\n",
    "_=ax.axis(xmin=30,xmax=70)\n",
    "_=ax.tick_params(labelsize=18)\n",
    "fig.savefig('fig-statistics/Maximum_likelihood_20_2.png')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-statistics/Maximum_likelihood_20_2.png, width=500 frac=0.85] Probability mass function for $\\hat{p}$. The two vertical lines form the confidence interval. <div id=\"fig:Maximum_likelihood_20_2\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Maximum_likelihood_20_2\"></div>\n",
    "\n",
    "<p>Probability mass function for $\\hat{p}$. The two vertical lines form the confidence interval.</p>\n",
    "<img src=\"fig-statistics/Maximum_likelihood_20_2.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    " The two vertical lines in [Figure](#fig:Maximum_likelihood_20_2)\n",
    "show how far out from the mean we have to go to accumulate 95\\% of the\n",
    "probability. Now, we can solve this as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "50+50\\epsilon=60\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which makes $\\epsilon=1/5$ or 20\\%. So, flipping 100 times means I can\n",
    "only get within 20\\% of the real $p$ 95\\% of the time in the worst case\n",
    "scenario (i.e., $p=1/2$). The following code verifies the situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli \n",
    "b=bernoulli(0.5) # coin distribution\n",
    "xs = b.rvs(100) # flip it 100 times\n",
    "phat = np.mean(xs) # estimated p\n",
    "print (abs(phat-0.5) < 0.5*0.20) # make it w/in interval?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's keep doing this and see if we can get within this interval 95\\% of\n",
    "the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out=[]\n",
    "b=bernoulli(0.5) # coin distribution\n",
    "for i in range(500):   # number of tries\n",
    "    xs = b.rvs(100)    # flip it 100 times\n",
    "    phat = np.mean(xs) # estimated p\n",
    "    out.append(abs(phat-0.5) < 0.5*0.20 ) # within 20% ?\n",
    "\n",
    "# percentage of tries w/in 20% interval\n",
    "print (100*np.mean(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Well, that seems to work! Now we have a way to get at the quality of\n",
    "the estimator, $\\hat{p}$.\n",
    "\n",
    "**Maximum Likelihood Estimator Without Calculus**\n",
    "\n",
    "The prior example showed how we can use calculus to compute the maximum\n",
    "likelihood estimator. It's important to emphasize that the maximum likelihood\n",
    "principle does *not* depend on calculus and extends to more general situations\n",
    "where calculus is impossible. For example, let $X$ be uniformly distributed in\n",
    "the interval $[0,\\theta]$.  Given $n$ measurements of $X$, the likelihood\n",
    "function is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^n \\frac{1}{\\theta} = \\frac{1}{\\theta^n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where each $x_i \\in [0,\\theta]$. Note that the slope of this function\n",
    "is not zero anywhere so the usual calculus approach is not going to work here.\n",
    "Because the likelihood is the product of the individual uniform densities, if\n",
    "any of the $x_i$ values were outside of the proposed $[0,\\theta]$ interval,\n",
    "then the likelihood would go to zero, because the uniform density is zero\n",
    "outside of the $[0,\\theta]$. This is no good for maximization. Thus, observing\n",
    "that the likelihood function is strictly decreasing with increasing $\\theta$,\n",
    "we conclude that the value for $\\theta$ that maximizes the likelihood is the\n",
    "maximum of the $x_i$ values. To summarize, the maximum likelihood estimator is\n",
    "the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_{ML} = \\max_i x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As always, we want the distribution of this estimator to judge its\n",
    "performance. In this case, this is pretty straightforward. The cumulative\n",
    "density function for the $\\max$ function is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P} \\left( \\hat{\\theta}_{ML} < v \\right) =  \\mathbb{P}( x_0 \\leq v \\wedge x_1 \\leq v \\ldots \\wedge x_n \\leq v)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and since all the $x_i$ are uniformly distributed in $[0,\\theta]$, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P} \\left( \\hat{\\theta}_{ML} < v  \\right) =  \\left(\\frac{v}{\\theta}\\right)^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " So, the probability density function is then,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f_{\\hat{\\theta}_{ML}}(\\theta_{ML}) =  n \\theta_{ML}^{ n-1 } \\theta^{ -n }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then, we can compute the $\\mathbb{E}(\\theta_{ML}) = (\\theta n)/(n+1)$ with\n",
    "corresponding variance as $\\mathbb{V}(\\theta_{ML}) = (\\theta^2 n)/(n+1)^2/(n+2)$.\n",
    "\n",
    "For a quick sanity check, we can write the following simulation for $\\theta =1$\n",
    "as in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ">>> from scipy import stats\n",
    ">>> rv = stats.uniform(0,1)  # define uniform random variable\n",
    ">>> mle=rv.rvs((100,500)).max(0) # max along row-dimension\n",
    ">>> print (mean(mle)) # approx n/(n+1) = 100/101 ~= 0.99\n",
    "0.989942138048\n",
    ">>> print (var(mle)) #approx n/(n+1)**2/(n+2) ~= 9.61E-5\n",
    "9.95762009884e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The `max(0)` suffix on for the `mle` computation takes\n",
    "the maximum of the so-computed array along the row (`axis=0`)\n",
    "dimension.\n",
    "\n",
    "\n",
    "\n",
    " You can also plot `hist(mle)` to see the histogram of the simulated\n",
    "maximum likelihood estimates and match it up against the probability density\n",
    "function we derived above.  \n",
    "\n",
    "\n",
    "In this section, we explored the concept of maximum\n",
    "likelihood estimation using a coin flipping experiment both analytically and\n",
    "numerically with the scientific Python stack. We also explored the case when\n",
    "calculus is not workable for maximum likelihood estimation.  There are two key\n",
    "points to remember. First, maximum likelihood estimation produces a function of\n",
    "the data that is itself a random variable, with its own probability\n",
    "distribution. We can get at the quality of the so-derived estimators by\n",
    "examining the confidence intervals around the estimated values using the\n",
    "probability distributions associated with the estimators themselves.  \n",
    "Second, maximum likelihood estimation applies even in situations \n",
    "where using basic calculus is not applicable [[wasserman2004all]](#wasserman2004all).\n",
    "\n",
    "\n",
    "## Delta Method\n",
    "<div id=\"sec:delta_method\"></div>\n",
    "\n",
    "Sometimes we want to characterize the distribution of a *function* of a random\n",
    "variable. In order to extend and generalize the Central Limit Theorem in this\n",
    "way, we need the Taylor series expansion. Recall that the Taylor series\n",
    "expansion is an approximation of a function of the following form,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "T_r(x) =\\sum_{i=0}^r \\frac{g^{(i)}(a)}{i!}(x-a)^i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " this basically says that a function $g$ can be adequately\n",
    "approximated about a point $a$ using a polynomial based on its derivatives\n",
    "evaluated at $a$. Before we state the general theorem, let's examine\n",
    "an example to understand how the mechanics work.\n",
    "\n",
    "**Example.**  Suppose that $X$ is a random variable with\n",
    "$\\mathbb{E}(X)=\\mu\\neq 0$.  Furthermore, supposedly have a suitable\n",
    "function $g$ and we want the distribution of $g(X)$. Applying the\n",
    "Taylor series expansion, we obtain the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "g(X) \\approx g(\\mu)+ g^{\\prime}(\\mu)(X-\\mu)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If we use $g(X)$  as an estimator for $g(\\mu)$, then we can say that\n",
    "we approximately have the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(g(X)) &=g(\\mu) \\\\\\\n",
    "\\mathbb{V}(g(X)) &=(g^{\\prime}(\\mu))^2 \\mathbb{V}(X) \\\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Concretely, suppose we want to estimate the odds, $\\frac{p}{1-p}$.\n",
    "For example, if $p=2/3$, then we say that the odds is `2:1` meaning that the\n",
    "odds of the one outcome are twice as likely as the odds of the other outcome.\n",
    "Thus, we have $g(p)=\\frac{p}{1-p}$ and we want to find\n",
    "$\\mathbb{V}(g(\\hat{p}))$.  In our coin-flipping problem, we have the\n",
    "estimator $\\hat{p}=\\frac{1}{n}\\sum X_k$ from the Bernoulli-distributed data\n",
    "$X_k$ individual coin-flips. Thus,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(\\hat{p}) &= p  \\\\\\\n",
    "\\mathbb{V}(\\hat{p}) &= \\frac{p(1-p)}{n}  \\\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Now, $g^\\prime(p)=1/(1-p)^2$, so we have,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{V}(g(\\hat{p}))&=(g^\\prime(p))^2 \\mathbb{V}(\\hat{p}) \\\\\\\n",
    "                      &=\\left(\\frac{1}{(1-p)^2}\\right)^2 \\frac{p(1-p)}{n}  \\\\\\\n",
    "                      &= \\frac{p}{n(1-p)^3}  \\\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which is an approximation of the variance of the estimator\n",
    "$g(\\hat{p})$. Let's simulate this and see how it agrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# compute MLE estimates \n",
    "d=stats.bernoulli(0.1).rvs((10,5000)).mean(0)\n",
    "# avoid divide-by-zero\n",
    "d=d[np.logical_not(np.isclose(d,1))]\n",
    "# compute odds ratio\n",
    "odds = d/(1-d)\n",
    "print ('odds ratio=',np.mean(odds),'var=',np.var(odds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The first number above is the mean of the simulated odds\n",
    "ratio and the second is the variance of the estimate.  According to\n",
    "the variance estimate above, we have $\\mathbb{V}(g(1/10))\\approx\n",
    "0.0137$, which is not too bad for this approximation. Recall we want\n",
    "to estimate the odds from $\\hat{p}$. The code above takes $5000$\n",
    "estimates of the $\\hat{p}$ to estimate $\\mathbb{V}(g)$. The odds ratio\n",
    "for $p=1/10$ is $1/9\\approx 0.111$.\n",
    "\n",
    "**Programming Tip.**\n",
    "\n",
    "The code above uses the `np.isclose` function to identify the ones from\n",
    "the simulation and the `np.logical_not` removes these elements from the\n",
    "data because the odds ratio has a zero in the denominator\n",
    "for these values.\n",
    "\n",
    "\n",
    "\n",
    "Let's try this again with a probability of heads of `0.5` instead of\n",
    "`0.3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "d=stats.bernoulli(.5).rvs((10,5000)).mean(0)\n",
    "d=d[np.logical_not(np.isclose(d,1))]\n",
    "print( 'odds ratio=',np.mean(d),'var=',np.var(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The odds ratio is this case is equal to one, which\n",
    "is not close to what was reported. According to our\n",
    "approximation, we should have $\\mathbb{V}(g)=0.4$, which does not\n",
    "look like what our simulation just reported.  This is\n",
    "because the approximation is best when the odds ratio is\n",
    "nearly linear and worse otherwise (see [Figure](#fig:Maximum_likelihood_0001)).\n",
    "\n",
    "<!-- dom:FIGURE: [fig-statistics/Maximum_likelihood_0001.png, width=500 frac=0.85] The odds ratio is close to linear for small values but becomes unbounded as $p$ approaches one. The delta method is more effective for small underlying values of $p$, where the linear approximation is better. <div id=\"fig:Maximum_likelihood_0001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Maximum_likelihood_0001\"></div>\n",
    "\n",
    "<p>The odds ratio is close to linear for small values but becomes unbounded as $p$ approaches one. The delta method is more effective for small underlying values of $p$, where the linear approximation is better.</p>\n",
    "<img src=\"fig-statistics/Maximum_likelihood_0001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
