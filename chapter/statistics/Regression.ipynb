{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('../../Python_probability_statistics_machine_learning_2E.png',width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression gets to the heart of\n",
    "statistics: Given a set of data points,\n",
    "what is the relationship of the data in-\n",
    "hand to data yet seen? \n",
    "How should information from one data set propagate to\n",
    "other data? Linear\n",
    "regression offers the following model to address this\n",
    "question:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(Y|X=x) \\approx a x + b\n",
    "$$\n",
    "\n",
    " That is, given specific values for $X$, assume that the conditional\n",
    "expectation\n",
    "is a linear function of those specific values.  However, because\n",
    "the observed\n",
    "values are not the expectations themselves, the model accommodates\n",
    "this with an\n",
    "additive noise term. In other words, the observed variable (a.k.a.\n",
    "response,\n",
    "target, dependent variable) is modeled as,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(Y| X=x_i) + \\epsilon_i \\approx a x + b+ \\epsilon_i=y\n",
    "$$\n",
    "\n",
    " where $\\mathbb{E}(\\epsilon_i)=0$ and the $\\epsilon_i$ are iid and\n",
    "where the\n",
    "distribution function of $\\epsilon_i$ depends on the problem, even\n",
    "though it is\n",
    "often assumed Gaussian. The $X=x$ values are known as independent\n",
    "variables,\n",
    "covariates, or regressors. \n",
    "\n",
    "Let's see if we can use all of the methods we have\n",
    "developed so far to\n",
    "understand this form of regression. The first task is to\n",
    "determine how to\n",
    "estimate the unknown linear parameters, $a$ and $b$. To make\n",
    "this concrete,\n",
    "let's assume that $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$.  Bear\n",
    "in mind that\n",
    "$\\mathbb{E}(Y|X=x)$ is a deterministic function of $x$.  In other\n",
    "words, the\n",
    "variable $x$ changes with each draw, but after the data have been\n",
    "collected\n",
    "these are no longer random quantities. Thus, for fixed $x$, $y$ is a\n",
    "random\n",
    "variable generated by $\\epsilon$.  Perhaps we should denote $\\epsilon$ as\n",
    "$\\epsilon_x$ to emphasize this, but because $\\epsilon$ is an independent,\n",
    "identically-distributed (iid) random variable at each fixed $x$, this would be\n",
    "excessive.  Because of Gaussian additive noise, the\n",
    "distribution of $y$ is\n",
    "completely characterized by its mean and variance.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(y) &= a x  + b  \\\\\\\n",
    "\\mathbb{V}(y) &= \\sigma^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    " Using the maximum likelihood procedure, we write\n",
    "out the log-likelihood\n",
    "function as\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(a,b)  = \\sum_{i=1}^n \\log \\mathcal{N}(a x_i +b , \\sigma^2)\n",
    "\\propto \\frac{1}{2 \\sigma^2}\\sum_{i=1}^n (y_i-a x_i-b)^2\n",
    "$$\n",
    "\n",
    " Note that we suppressed  the terms that are irrelevent to\n",
    "the maximum-finding.\n",
    "Taking the derivative of this with respect to $a$ gives\n",
    "the following equation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}(a,b)}{\\partial a}= 2 \\sum_{i=1}^n x_i (b+ a x_i\n",
    "-y_i) =0\n",
    "$$\n",
    "\n",
    " Likewise, we do the same for the $b$ parameter\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}(a,b)}{\\partial b}=2\\sum_{i=1}^n (b+a x_i-y_i) =0\n",
    "$$\n",
    "\n",
    " The following code simulates some data and uses Numpy tools to\n",
    "compute the\n",
    "parameters as shown,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = 6;b = 1 # parameters to estimate\n",
    "x = np.linspace(0,1,100)\n",
    "y = a*x + np.random.randn(len(x))+b\n",
    "p,var_=np.polyfit(x,y,1,cov=True) # fit data to line\n",
    "y_ = np.polyval(p,x) # estimated by linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import subplots\n",
    "fig,axs=subplots(1,2)\n",
    "fig.set_size_inches((9,3.5))\n",
    "_=ax =axs[0]\n",
    "_=ax.plot(x,y,'o',alpha=.3,color='gray',ms=10)\n",
    "_=ax.plot(x,y_,color='k',lw=3)\n",
    "_=ax.set_xlabel(\"$x$\",fontsize=22)\n",
    "_=ax.set_ylabel(\"$y$\",fontsize=22)\n",
    "_=ax.set_title(\"Linear regression; a=%3.3g b=%3.3g\"%(p[0],p[1]))\n",
    "_=ax = axs[1]\n",
    "_=ax.hist(y_-y,color='gray')\n",
    "_=ax.set_xlabel(r\"$\\Delta y$\",fontsize=22)\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-statistics/Regression_001.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @@@CODE src-statistics/Regression.py fromto: ^a@^# draw -->\n",
    "\n",
    "<!--\n",
    "dom:FIGURE: [fig-statistics/Regression_001.png, width=500 frac=1.] The panel on\n",
    "the left shows the data and regression line. The panel on the right shows a\n",
    "histogram of the regression errors. <div id=\"fig:Regression_001\"></div> -->\n",
    "<!--\n",
    "begin figure -->\n",
    "<div id=\"fig:Regression_001\"></div>\n",
    "\n",
    "<p>The panel on the left\n",
    "shows the data and regression line. The panel on the right shows a histogram of\n",
    "the regression errors.</p>\n",
    "<img src=\"fig-statistics/Regression_001.png\"\n",
    "width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    " The graph on the left of\n",
    "[Figure](#fig:Regression_001)\n",
    "shows the regression line plotted against the\n",
    "data. The estimated\n",
    "parameters are noted in the title.  The histogram on the\n",
    "right of\n",
    "[Figure](#fig:Regression_001) shows the residual errors in the model.\n",
    "It is always a good idea to inspect the residuals of any regression\n",
    "for\n",
    "normality. These are the differences between the fitted line for\n",
    "each $x_i$\n",
    "value and the corresponding $y_i$ value in the data.\n",
    "Note that the $x$ term does\n",
    "not have to be uniformly monotone.\n",
    "\n",
    "To decouple the deterministic variation from\n",
    "the random variation, we can fix\n",
    "the index and write separate problems of the\n",
    "form\n",
    "\n",
    "$$\n",
    "y_i = a x_i +b + \\epsilon_i\n",
    "$$\n",
    "\n",
    " where $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$. What could we do with\n",
    "just\n",
    "this one component of the problem? In other words, suppose we had\n",
    "$m$-samples of\n",
    "this component as in $\\lbrace y_{i,k}\\rbrace_{k=1}^m$. Following\n",
    "the usual\n",
    "procedure, we could obtain estimates of the mean of $y_i$ as\n",
    "\n",
    "$$\n",
    "\\hat{y_i} = \\frac{1}{m}\\sum_{k=1}^m y_{i,k}\n",
    "$$\n",
    "\n",
    " However, this tells us nothing about the individual parameters $a$\n",
    "and $b$\n",
    "because they are not separable in the terms that are computed, namely,\n",
    "we may\n",
    "have\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(y_i) = a x_i +b\n",
    "$$\n",
    "\n",
    " but we still only have one equation and the two unknowns, $a$ and\n",
    "$b$. How\n",
    "about if we consider and fix another component $j$ as in\n",
    "\n",
    "$$\n",
    "y_j = a x_j +b + \\epsilon_i\n",
    "$$\n",
    "\n",
    " Then, we have\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(y_j) = a x_j +b\n",
    "$$\n",
    "\n",
    " so at least now we have two equations and two unknowns and we know how to\n",
    "estimate the left hand sides of these equations from the data using the\n",
    "estimators $\\hat{y_i}$ and  $\\hat{y_j}$. Let's see how this works in the code\n",
    "sample below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, xn =x[0],x[80]\n",
    "# generate synthetic data\n",
    "y_0 = a*x0 + np.random.randn(20)+b\n",
    "y_1 = a*xn + np.random.randn(20)+b\n",
    "# mean along sample dimension\n",
    "yhat = np.array([y_0,y_1]).mean(axis=1)\n",
    "a_,b_=np.linalg.solve(np.array([[x0,1],\n",
    "                                [xn,1]]),yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-statistics/Regression_002.png, width=500 frac=0.95] The\n",
    "fitted and true lines are plotted with the data values. The squares at either\n",
    "end of the solid line show the mean value for each of the data groups shown.\n",
    "<div id=\"fig:Regression_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div\n",
    "id=\"fig:Regression_002\"></div>\n",
    "\n",
    "<p>The fitted and true lines are plotted with\n",
    "the data values. The squares at either end of the solid line show the mean value\n",
    "for each of the data groups shown.</p>\n",
    "<img src=\"fig-\n",
    "statistics/Regression_002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "**Programming\n",
    "Tip.**\n",
    "\n",
    "The prior code uses the `solve` function in the Numpy `linalg` module,\n",
    "which\n",
    "contains the core linear algebra codes in Numpy that incorporate the\n",
    "battle-tested LAPACK library.\n",
    "\n",
    "\n",
    "\n",
    " We can write out the solution for the\n",
    "estimated parameters for this\n",
    "case where $x_0 =0$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{a} &= \\frac{\\hat{y}_i - \\hat{y}_0}{x_i} \\\\\\\n",
    "\\hat{b} &=\n",
    "\\hat{y_0} \\\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    " The expectations and variances of these estimators are the following,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(\\hat{a}) &= \\frac{a x_i }{x_i}=a  \\\\\\\n",
    "\\mathbb{E}(\\hat{b}) &=b \\\\\\\n",
    "\\mathbb{V}(\\hat{a}) &= \\frac{2 \\sigma^2}{x_i^2} \\\\\\\n",
    "\\mathbb{V}(\\hat{b}) &= \\sigma^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    " The expectations show that the estimators are unbiased.  The\n",
    "estimator\n",
    "$\\hat{a}$ has a variance that decreases as larger points $x_i$ are\n",
    "selected.\n",
    "That is, it is better to have samples further out along the\n",
    "horizontal axis for\n",
    "fitting the line.  This variance quantifies the *leverage*\n",
    "of those distant\n",
    "points.\n",
    "\n",
    "**Regression From Projection Methods.** Let's see if we can apply our\n",
    "knowledge\n",
    "of projection methods to the general case.  In vector notation, we can\n",
    "write\n",
    "the following:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = a \\mathbf{x} + b\\mathbf{1} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    " where $\\mathbf{1}$ is the vector of all ones. \n",
    "Let's use the inner-product\n",
    "notation,\n",
    "\n",
    "$$\n",
    "\\langle \\mathbf{x},\\mathbf{y} \\rangle = \\mathbb{E}(\\mathbf{x}^T \\mathbf{y})\n",
    "$$\n",
    "\n",
    " Then, by taking the inner-product with some $\\mathbf{x}_1 \\in\n",
    "\\mathbf{1}^\\perp$\n",
    "we obtain [^perp],\n",
    "\n",
    "[^perp]: The space of all vectors, $\\mathbf{a}$ such that\n",
    "$\\langle\n",
    "\\mathbf{a},\\mathbf{1} \\rangle = 0$ is denoted $\\mathbf{1}^\\perp$.\n",
    "\n",
    "$$\n",
    "\\langle \\mathbf{y},\\mathbf{x}_1  \\rangle = a \\langle \\mathbf{x},\\mathbf{x}_1\n",
    "\\rangle\n",
    "$$\n",
    "\n",
    " Recall that $\\mathbb{E}(\\boldsymbol{\\epsilon})=\\mathbf{0}$.  We\n",
    "can finally\n",
    "solve for $a$ as\n",
    "\n",
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:ahat\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{a} = \\frac{\\langle\\mathbf{y},\\mathbf{x}_1 \\rangle}{\\langle\n",
    "\\mathbf{x},\\mathbf{x}_1 \\rangle} \n",
    "\\end{equation}\n",
    "\\label{eq:ahat} \\tag{1}\n",
    "$$\n",
    "\n",
    " That was pretty neat but now we have the mysterious $\\mathbf{x}_1$\n",
    "vector.\n",
    "Where does this come from?  If we project $\\mathbf{x}$ onto the\n",
    "$\\mathbf{1}^\\perp$, then we get the MMSE  approximation to $\\mathbf{x}$ in the\n",
    "$\\mathbf{1}^\\perp$ space. Thus, we take\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_1 = P_{\\mathbf{1}^\\perp} (\\mathbf{x})\n",
    "$$\n",
    "\n",
    " Remember that $P_{\\mathbf{1}^\\perp} $ is a projection matrix so the length of\n",
    "$\\mathbf{x}_1$ is at most $\\mathbf{x}$. This means that the denominator in\n",
    "the\n",
    "$\\hat{a}$ equation above is really just the length of the $\\mathbf{x}$\n",
    "vector in\n",
    "the coordinate system of $P_{\\mathbf{1}^\\perp} $. Because the\n",
    "projection is\n",
    "orthogonal (namely, of minimum length), the Pythagorean theorem\n",
    "gives this\n",
    "length as the following:\n",
    "\n",
    "$$\n",
    "\\langle \\mathbf{x},\\mathbf{x}_1 \\rangle ^2=\\langle \\mathbf{x},\\mathbf{x}\n",
    "\\rangle- \\langle\\mathbf{1},\\mathbf{x} \\rangle^2\n",
    "$$\n",
    "\n",
    " The first term on the right is the length of the $\\mathbf{x}$ vector\n",
    "and last\n",
    "term is the length of $\\mathbf{x}$ in the coordinate system orthogonal\n",
    "to\n",
    "$P_{\\mathbf{1}^\\perp} $, namely that of $\\mathbf{1}$.  We\n",
    "can use this geometric\n",
    "interpretation to understand what is going on in\n",
    "typical linear regression in\n",
    "much more detail. The fact that the denominator is\n",
    "the orthogonal projection of\n",
    "$\\mathbf{x}$ tells us that the choice of\n",
    "$\\mathbf{x}_1$ has the strongest effect\n",
    "(i.e., largest value) on reducing\n",
    "the variance of $\\hat{a}$. That is, the more\n",
    "$\\mathbf{x}$ is aligned with\n",
    "$\\mathbf{1}$, the worse the variance of $\\hat{a}$.\n",
    "This makes intuitive sense\n",
    "because the closer $\\mathbf{x}$ is to $\\mathbf{1}$,\n",
    "the more constant it is,\n",
    "and we have already seen from our one-dimensional\n",
    "example that distance between\n",
    "the $x$ terms pays off in reduced variance. We\n",
    "already know that $\\hat{a}$\n",
    "is an unbiased estimator, and, because we chose\n",
    "$\\mathbf{x}_1$ deliberately as a\n",
    "projection, we know that it is also of minimum\n",
    "variance. Such estimators are\n",
    "known as Minimum-Variance Unbiased Estimators\n",
    "(MVUE).\n",
    "\n",
    "In the same spirit, let's examine the numerator of $\\hat{a}$ in\n",
    "Equation [1](#eq:ahat). We can write\n",
    "$\\mathbf{x}_{1}$ as the following\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{1} = \\mathbf{x} -  P_{\\mathbf{1}} \\mathbf{x}\n",
    "$$\n",
    "\n",
    " where $P_{\\mathbf{1}}$ is projection matrix  of $\\mathbf{x}$ onto the\n",
    "$\\mathbf{1}$ vector. Using this, the numerator of $\\hat{a}$ becomes\n",
    "\n",
    "$$\n",
    "\\langle \\mathbf{y}, \\mathbf{x}_1\\rangle  =\\langle \\mathbf{y},\n",
    "\\mathbf{x}\\rangle -\\langle \\mathbf{y}, P_{\\mathbf{1}} \\mathbf{x}\\rangle\n",
    "$$\n",
    "\n",
    " Note that,\n",
    "\n",
    "$$\n",
    "P_{\\mathbf{1}}  = \\mathbf{1} \\mathbf{1}^T \\frac{1}{n}\n",
    "$$\n",
    "\n",
    " so that writing this out explicitly gives\n",
    "\n",
    "$$\n",
    "\\langle \\mathbf{y}, P_{\\mathbf{1}} \\mathbf{x}\\rangle = \\left(\\mathbf{y}^T\n",
    "\\mathbf{1}\\right) \\left(\\mathbf{1}^T \\mathbf{x}\\right)/n =\\left(\\sum\n",
    "y_i\\right)\\left(\\sum x_{i}\\right)/n\n",
    "$$\n",
    "\n",
    " and similarly, we have the following for the denominator:\n",
    "\n",
    "$$\n",
    "\\langle \\mathbf{x}, P_{\\mathbf{1}} \\mathbf{x}\\rangle = \\left(\\mathbf{x}^T\n",
    "\\mathbf{1}\\right) \\left(\\mathbf{1}^T \\mathbf{x}\\right)/n =\\left(\\sum\n",
    "x_i\\right)\\left(\\sum x_{i}\\right)/n\n",
    "$$\n",
    "\n",
    " So, plugging all of this together gives the following,\n",
    "\n",
    "$$\n",
    "\\hat{a} = \\frac{\\mathbf{x}^T\\mathbf{y}-(\\sum x_i)(\\sum y_i)/n}{\\mathbf{x}^T\n",
    "\\mathbf{x}  -(\\sum x_i)^2/n}\n",
    "$$\n",
    "\n",
    " with corresponding variance,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{V}(\\hat{a}) &= \\sigma^2\n",
    "\\frac{\\|\\mathbf{x}_1\\|^2}{\\langle\\mathbf{x},\\mathbf{x}_1\\rangle^2} \\\\\\\n",
    "&= \\frac{\\sigma^2}{\\Vert \\mathbf{x}\\Vert^2-n(\\overline{x}^2)} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    " Using the same approach with $\\hat{b}$ gives,\n",
    "\n",
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{b}  = \\frac{\\langle \\mathbf{y},\\mathbf{x}^{\\perp}\n",
    "\\rangle}{\\langle \\mathbf{1},\\mathbf{x}^{\\perp}\\rangle} \n",
    "\\label{_auto1} \\tag{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\\n",
    "         = \\frac{\\langle\n",
    "\\mathbf{y},\\mathbf{1}-P_{\\mathbf{x}}(\\mathbf{1})\\rangle}{\\langle\n",
    "\\mathbf{1},\\mathbf{1}-P_{\\mathbf{x}}(\\mathbf{1})\\rangle} \n",
    "\\label{_auto2} \\tag{3}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\\n",
    "         = \\frac{\\mathbf{x}^T \\mathbf{x}(\\sum y_i)/n\n",
    "-\\mathbf{x}^T\\mathbf{y}(\\sum x_i)/n}{\\mathbf{x}^T \\mathbf{x} -(\\sum x_i)^2/n}\n",
    "\\label{_auto3} \\tag{4}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    " where\n",
    "\n",
    "$$\n",
    "P_{\\mathbf{x}} = \\frac{\\mathbf{\\mathbf{x} \\mathbf{x}^T}}{\\| \\mathbf{x} \\|^2}\n",
    "$$\n",
    "\n",
    " with variance\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{V}(\\hat{b})&=\\sigma^2 \\frac{\\langle\n",
    "\\boldsymbol{\\mathbf{1}-P_{\\mathbf{x}}(\\mathbf{1})},\\boldsymbol{\\mathbf{1}-P_{\\mathbf{x}}(\\mathbf{1})}\\rangle}{\\langle\n",
    "\\mathbf{1},\\boldsymbol{\\mathbf{1}-P_{\\mathbf{x}}(\\mathbf{1})}\\rangle^2} \\\\\\\n",
    "&=\\frac{\\sigma^2}{n-\\frac{(n\\overline{x})^2}{\\Vert\\mathbf{x}\\Vert^2}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Qualifying the Estimates.** Our formulas for the variance above include the\n",
    "unknown $\\sigma^2$, which we must estimate from the data itself using our\n",
    "plug-\n",
    "in estimates.  We can form the residual sum of squares as\n",
    "\n",
    "$$\n",
    "\\texttt{RSS} = \\sum_i (\\hat{a} x_i + \\hat{b} - y_i)^2\n",
    "$$\n",
    "\n",
    " Thus, the estimate of $\\sigma^2$ can be expressed as\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{\\texttt{RSS}}{n-2}\n",
    "$$\n",
    "\n",
    " where $n$ is the number of samples. This is also known as the\n",
    "*residual mean\n",
    "square*. The $n-2$  represents the *degrees of freedom* (`df`).\n",
    "Because we\n",
    "estimated two parameters from the same data we have $n-2$ instead of\n",
    "$n$. Thus,\n",
    "in general, $\\texttt{df} = n - p$, where $p$ is the number of\n",
    "estimated\n",
    "parameters.  Under the assumption that the noise is Gaussian,  the\n",
    "$\\texttt{RSS}/\\sigma^2$ is chi-squared distributed with $n-2$ degrees of\n",
    "freedom. Another important term is the *sum of squares about the mean*, (a.k.a\n",
    "*corrected* sum of squares),\n",
    "\n",
    "$$\n",
    "\\texttt{SYY} = \\sum (y_i - \\bar{y})^2\n",
    "$$\n",
    "\n",
    " The $\\texttt{SYY}$ captures the idea of not using the $x_i$ data and\n",
    "just using\n",
    "the mean of the $y_i$ data to estimate $y$. These two terms lead\n",
    "to the $R^2$\n",
    "term,\n",
    "\n",
    "$$\n",
    "R^2=1-\\frac{\\texttt{RSS}}{ \\texttt{SYY} }\n",
    "$$\n",
    "\n",
    " Note that for perfect regression, $R^2=1$.  That is, if the\n",
    "regression gets\n",
    "each $y_i$ data point exactly right, then\n",
    "$\\texttt{RSS}=0$ this term equals one.\n",
    "Thus, this term is used to\n",
    "measure of goodness-of-fit. The `stats` module in\n",
    "`scipy` computes\n",
    "many of these terms automatically,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "slope,intercept,r_value,p_value,stderr = stats.linregress(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the square of the `r_value` variable is the $R^2$ above. The\n",
    "computed\n",
    "p-value is the two-sided hypothesis test with a null hypothesis that\n",
    "the slope\n",
    "of the line is zero. In other words, this tests whether or not the\n",
    "linear\n",
    "regression  makes sense for the data for that hypothesis.  The\n",
    "Statsmodels\n",
    "module provides a powerful extension to Scipy's stats module by\n",
    "making it easy\n",
    "to do regression and keep track of these parameters. Let's\n",
    "reformulate our\n",
    "problem using the Statsmodels framework by creating\n",
    "a Pandas dataframe for the\n",
    "data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "d = DataFrame({'x':np.linspace(0,1,10)}) # create data\n",
    "d['y'] = a*d.x+ b + np.random.randn(*d.x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ** -->\n",
    "\n",
    " Now that we have  the input data in the above\n",
    "Pandas dataframe, we\n",
    "can perform the regression as in the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols('y ~ x', data=d).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\sim$ symbol is notation for $y = a x + b + \\epsilon$, where the\n",
    "constant\n",
    "$b$ is implicit in this usage of Statsmodels. The names in the string\n",
    "are taken\n",
    "from the columns in the dataframe. This makes it very easy to build\n",
    "models with\n",
    "complicated interactions between the named columns in the\n",
    "dataframe. We can\n",
    "examine a report of the model fit by looking at the summary,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (results.summary2())\n",
    "                 Results: Ordinary least squares\n",
    "=================================================================\n",
    "Model:              OLS              Adj. R-squared:     0.808   \n",
    "Dependent Variable: y                AIC:                28.1821 \n",
    "Date:               0000-00-00 00:00 BIC:                00.0000 \n",
    "No. Observations:   10               Log-Likelihood:     -12.091 \n",
    "Df Model:           1                F-statistic:        38.86   \n",
    "Df Residuals:       8                Prob (F-statistic): 0.000250\n",
    "R-squared:          0.829            Scale:              0.82158 \n",
    "-------------------------------------------------------------------\n",
    "              Coef.    Std.Err.     t      P>|t|    [0.025   0.975]\n",
    "-------------------------------------------------------------------\n",
    "Intercept     1.5352     0.5327   2.8817   0.0205   0.3067   2.7637\n",
    "x             5.5990     0.8981   6.2340   0.0003   3.5279   7.6701"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot more here than we have discussed so far, but the\n",
    "Statsmodels\n",
    "documentation is the best place to go for complete information\n",
    "about this\n",
    "report. The F-statistic attempts to capture the contrast between\n",
    "including the\n",
    "slope parameter or leaving it off. That is, consider two\n",
    "hypotheses:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H_0 \\colon  \\mathbb{E}(Y|X=x) &= b \\\\\\\n",
    "H_1 \\colon\n",
    "\\mathbb{E}(Y|X=x) &= b + a x\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    " In order to quantify how much better adding the slope term is for\n",
    "the\n",
    "regression, we compute  the following:\n",
    "\n",
    "$$\n",
    "F = \\frac{\\texttt{SYY} - \\texttt{RSS}}{ \\hat{\\sigma}^2 }\n",
    "$$\n",
    "\n",
    " The numerator computes the difference in the residual squared errors\n",
    "between\n",
    "including the slope in the regression or just using the mean of the\n",
    "$y_i$\n",
    "values. Once again, if we assume (or can claim asymptotically) that the\n",
    "$\\epsilon$ noise term is Gaussian, $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$,\n",
    "then\n",
    "the $H_0$ hypothesis will follow an F-distribution [^fdist] with degrees of\n",
    "freedom from the numerator and denominator. In this case, $F \\sim F(1,n-2)$.\n",
    "The\n",
    "value of this statistic is reported by Statsmodels above. The corresponding\n",
    "reported probability shows the chance of $F$ exceeding its computed value  if\n",
    "$H_0$ were true. So, the take-home message from all this is that including the\n",
    "slope leads to a much smaller reduction in squared error than could be expected\n",
    "from a favorable draw of $n$ points of this data, under the Gaussian additive\n",
    "noise assumption. This is evidence that including the slope is meaningful for\n",
    "this data.\n",
    "\n",
    "[^fdist]: The $F(m,n)$ F-distribution has two integer degree-of-\n",
    "freedom parameters, $m$\n",
    "and $n$.\n",
    "\n",
    "\n",
    "The Statsmodels report also shows the\n",
    "adjusted $R^2$ term.\n",
    "This is a correction to the $R^2$ calculation that accounts\n",
    "for the number of parameters $p$ that the regression is\n",
    "fitting and the sample\n",
    "size $n$,\n",
    "\n",
    "$$\n",
    "\\texttt{Adjusted } R^2 = 1- \\frac{\\texttt{RSS}/(n-p)}{\\texttt{SYY}/(n-1)}\n",
    "$$\n",
    "\n",
    " This is always lower than $R^2$ except when $p=1$ (i.e., estimating\n",
    "only $b$).\n",
    "This becomes a better way to compare regressions when one is\n",
    "attempting to fit\n",
    "many parameters with comparatively small $n$.\n",
    "\n",
    "**Linear Prediction.**  Using\n",
    "linear regression for prediction introduces\n",
    "some other issues. Recall the\n",
    "following expectation,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(Y|X=x) \\approx \\hat{a} x + \\hat{b}\n",
    "$$\n",
    "\n",
    " where we have determined $\\hat{a}$  and $\\hat{b}$  from the data.\n",
    "Given a new\n",
    "point of interest, $x_p$, we would certainly compute\n",
    "\n",
    "$$\n",
    "\\hat{y}_p = \\hat{a} x_p + \\hat{b}\n",
    "$$\n",
    "\n",
    " as the predicted value for $\\hat{ y_p }$. This is the same as saying\n",
    "that our\n",
    "best prediction for $y$ based on $x_p$ is the above conditional\n",
    "expectation. The\n",
    "variance for this is the following,\n",
    "\n",
    "$$\n",
    "\\mathbb{V}(y_p) = x_p^2 \\mathbb{V}(\\hat{a}) +\\mathbb{V}(\\hat{b})+2 x_p\n",
    "\\texttt{cov}(\\hat{a}\\hat{b})\n",
    "$$\n",
    "\n",
    " Note that we have the covariance above because $\\hat{a}$ and\n",
    "$\\hat{b}$ are\n",
    "derived from the same data.  We can work this out below using\n",
    "our previous\n",
    "notation from [1](#eq:ahat),\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\texttt{cov}(\\hat{a}\\hat{b})=&\\frac{\\mathbf{x}_1^T\n",
    "\\mathbb{V}\\lbrace\\mathbf{y}\\mathbf{y}^T\\rbrace\\mathbf{x}^{\\perp}}{(\\mathbf{x}_1^T\n",
    "\\mathbf{x})(\\mathbf{1}^T \\mathbf{x}^{\\perp})} = \\frac{\\mathbf{x}_1^T\n",
    "\\sigma^2\\mathbf{I}\\mathbf{x}^{\\perp}}{(\\mathbf{x}_1^T \\mathbf{x})(\\mathbf{1}^T\n",
    "\\mathbf{x}^{\\perp})}\\\\\\\n",
    "=&\\sigma^2\\frac{\\mathbf{x}_1^T\\mathbf{x}^{\\perp}}{(\\mathbf{x}_1^T\n",
    "\\mathbf{x})(\\mathbf{1}^T \\mathbf{x}^{\\perp})} =\n",
    "\\sigma^2\\frac{\\left(\\mathbf{x}-P_1\\mathbf{x}\\right)^T\\mathbf{x}^{\\perp}}{(\\mathbf{x}_1^T\n",
    "\\mathbf{x})(\\mathbf{1}^T \\mathbf{x}^{\\perp})}\\\\\\\n",
    "=&\\sigma^2\\frac{-\\mathbf{x}^T P_1^T\\mathbf{x}^{\\perp}}{(\\mathbf{x}_1^T\n",
    "\\mathbf{x})(\\mathbf{1}^T \\mathbf{x}^{\\perp})} =\n",
    "\\sigma^2\\frac{-\\mathbf{x}^T\\frac{1}{n}\\mathbf{1}\n",
    "\\mathbf{1}^T\\mathbf{x}^{\\perp}}{(\\mathbf{x}_1^T \\mathbf{x})(\\mathbf{1}^T\n",
    "\\mathbf{x}^{\\perp})}\\\\\\\n",
    "=&\\sigma^2\\frac{-\\mathbf{x}^T\\frac{1}{n}\\mathbf{1}}{(\\mathbf{x}_1^T \\mathbf{x})}\n",
    "= \\frac{-\\sigma^2\\overline{x}}{\\sum_{i=1}^n(x_i^2-\\overline{x}^2)}\\\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    " After plugging all this in, we obtain the following,\n",
    "\n",
    "$$\n",
    "\\mathbb{V}(y_p)=\\sigma^2 \\frac{x_p^2-2 x_p\\overline{x}+\\Vert\n",
    "\\mathbf{x}\\Vert^2/n}{\\Vert\\mathbf{x}\\Vert^2-n\\overline{x}^2}\n",
    "$$\n",
    "\n",
    " where, in practice, we use the plug-in estimate for\n",
    "the  $\\sigma^2$.\n",
    "\n",
    "There is\n",
    "an important consequence for the confidence interval for\n",
    "$y_p$. We cannot simply\n",
    "use the square root of $\\mathbb{V}(y_p)$\n",
    "to form the confidence interval because\n",
    "the model includes the\n",
    "extra $\\epsilon$ noise term. In particular, the\n",
    "parameters were\n",
    "computed using a set of statistics from the data, but now must\n",
    "include different realizations for the noise term for the\n",
    "prediction part.  This\n",
    "means we have to compute\n",
    "\n",
    "$$\n",
    "\\eta^2=\\mathbb{V}(y_p)+\\sigma^2\n",
    "$$\n",
    "\n",
    " Then, the 95\\% confidence interval $y_p \\in\n",
    "(y_p-2\\hat{\\eta},y_p+2\\hat{\\eta})$\n",
    "is the following,\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(y_p-2\\hat{\\eta}< y_p <\n",
    "y_p+2\\hat{\\eta})\\approx\\mathbb{P}(-2<\\mathcal{N}(0,1)<2) \\approx 0.95\n",
    "$$\n",
    "\n",
    " where $\\hat{\\eta}$ comes from substituting the\n",
    "plug-in estimate for $\\sigma$.\n",
    "## Extensions to Multiple Covariates\n",
    "\n",
    "With all the machinery we have, it is a\n",
    "short notational hop to consider\n",
    "multiple regressors as in the following,\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} +\\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    " with the usual $\\mathbb{E}(\\boldsymbol{\\epsilon})=\\mathbf{0}$ and\n",
    "$\\mathbb{V}(\\boldsymbol{\\epsilon})=\\sigma^2\\mathbf{I}$. Thus, $\\mathbf{X}$ is a\n",
    "$n \\times p$ full rank matrix of regressors and $\\mathbf{Y}$ is the $n$-vector\n",
    "of observations. Note that the constant term has been incorporated into\n",
    "$\\mathbf{X}$ as a column of ones.  The corresponding estimated solution for\n",
    "$\\boldsymbol{\\beta}$  is the following,\n",
    "\n",
    "$$\n",
    "\\hat{ \\boldsymbol{\\beta} } = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T\n",
    "\\mathbf{Y}\n",
    "$$\n",
    "\n",
    " with corresponding variance,\n",
    "\n",
    "$$\n",
    "\\mathbb{V}(\\hat{\\boldsymbol{\\beta}})=\\sigma^2(\\mathbf{X}^T \\mathbf{X})^{-1}\n",
    "$$\n",
    "\n",
    " and with the assumption of Gaussian errors, we have\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}\\sim \\mathcal{N}(\\boldsymbol{\\beta},\n",
    "\\sigma^2(\\mathbf{X}^T \\mathbf{X})^{-1})\n",
    "$$\n",
    "\n",
    " The unbiased estimate of $\\sigma^2$ is the following,\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n-p}\\sum \\hat{\\epsilon}_i^2\n",
    "$$\n",
    "\n",
    " where $\\hat{ \\boldsymbol{\\epsilon}}=\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n",
    "-\\mathbf{Y}$ \n",
    "is the vector of residuals. Tukey christened the following matrix\n",
    "as the *hat*\n",
    "matrix (a.k.a. influence matrix),\n",
    "\n",
    "$$\n",
    "\\mathbf{V}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\n",
    "$$\n",
    "\n",
    " because it maps $\\mathbf{Y}$ into $\\hat{ \\mathbf{Y} }$,\n",
    "\n",
    "$$\n",
    "\\hat{ \\mathbf{Y} } = \\mathbf{V} \\mathbf{Y}\n",
    "$$\n",
    "\n",
    " As an exercise you can check that $\\mathbf{V}$ is a projection\n",
    "matrix. Note\n",
    "that that matrix is solely a function of $\\mathbf{X}$. The\n",
    "diagonal elements of\n",
    "$\\mathbf{V}$ are called the *leverage values* and are\n",
    "contained in the closed\n",
    "interval $[1/n,1]$. These terms measure of distance\n",
    "between the values of $x_i$\n",
    "and the mean values over the $n$ observations.\n",
    "Thus, the leverage terms depend\n",
    "only on $\\mathbf{X}$. This is the\n",
    "generalization of our initial discussion of\n",
    "leverage where we had multiple\n",
    "samples at only two $x_i$ points. Using the hat\n",
    "matrix, we can compute the\n",
    "variance of each residual, $e_i = \\hat{y}-y_i$ as\n",
    "\n",
    "$$\n",
    "\\mathbb{V}(e_i) = \\sigma^2 (1-v_{i})\n",
    "$$\n",
    "\n",
    " where $v_i=V_{i,i}$. Given the above-mentioned bounds on $v_{i}$,\n",
    "these are\n",
    "always less than $\\sigma^2$.\n",
    "\n",
    "Degeneracy in the columns of $\\mathbf{X}$ can\n",
    "become a problem. This is when\n",
    "two or more of the columns become co-linear. We\n",
    "have already seen this with our\n",
    "single regressor example wherein $\\mathbf{x}$\n",
    "close to $\\mathbf{1}$ was bad\n",
    "news. To compensate for this effect we can load\n",
    "the diagonal elements and solve\n",
    "for the unknown parameters as in the following,\n",
    "\n",
    "$$\n",
    "\\hat{ \\boldsymbol{\\beta} } = (\\mathbf{X}^T \\mathbf{X}+\\alpha \\mathbf{I})^{-1}\n",
    "\\mathbf{X}^T \\mathbf{Y}\n",
    "$$\n",
    "\n",
    " where $\\alpha>0$ is a tunable hyper-parameter. This method is known\n",
    "as *ridge\n",
    "regression* and was proposed in 1970 by Hoerl and Kenndard. It can be\n",
    "shown that\n",
    "this is the equivalent to minimizing the following objective,\n",
    "\n",
    "$$\n",
    "\\Vert \\mathbf{Y}- \\mathbf{X} \\boldsymbol{\\beta}\\Vert^2  + \\alpha \\Vert\n",
    "\\boldsymbol{\\beta}\\Vert^2\n",
    "$$\n",
    "\n",
    " In other words, the length of the estimated $\\boldsymbol{\\beta}$ is\n",
    "penalized\n",
    "with larger $\\alpha$. This has the effect of stabilizing the\n",
    "subsequent inverse\n",
    "calculation and also providing a means to trade bias and\n",
    "variance, which we will\n",
    "discuss at length in the section\n",
    "[ch:ml:sec:regularization](#ch:ml:sec:regularization)\n",
    "\n",
    "**Interpreting\n",
    "Residuals.**  Our model assumes an additive Gaussian noise term.\n",
    "We can check\n",
    "the voracity of this assumption by examining the residuals after\n",
    "fitting.  The\n",
    "residuals are the difference between the fitted values and the\n",
    "original data\n",
    "\n",
    "$$\n",
    "\\hat{\\epsilon}_i = \\hat{a} x_i + \\hat{b} - y_i\n",
    "$$\n",
    "\n",
    " While the p-value and the F-ratio provide some indication of whether\n",
    "or not\n",
    "computing the slope of the regression makes sense, we can get directly\n",
    "at the\n",
    "key assumption of additive Gaussian noise.\n",
    "\n",
    "For sufficiently small dimensions,\n",
    "the `scipy.stats.probplot` we discussed in\n",
    "the last chapter provides quick\n",
    "visual evidence one way or another by plotting\n",
    "the standardized residuals,\n",
    "\n",
    "$$\n",
    "r_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-v_i}}\n",
    "$$\n",
    "\n",
    " The other part of the iid assumption implies homoscedasticity (all\n",
    "$r_i$ have\n",
    "equal variances). Under the additive Gaussian noise assumption, the\n",
    "$e_i$ should\n",
    "also be distributed according to $\\mathcal{N}(0,\\sigma^2(1-v_i))$.\n",
    "The\n",
    "normalized residuals $r_i$ should then be distributed according to\n",
    "$\\mathcal{N}(0,1)$. Thus, the presence of any $r_i \\notin [-1.96,1.96]$ should\n",
    "not be common at the 5% significance level and is thereby breeds suspicion\n",
    "regarding the homoscedasticity assumption.\n",
    "\n",
    "The Levene test in\n",
    "`scipy.stats.leven` tests the null hypothesis that all the\n",
    "variances are equal.\n",
    "This basically checks whether or not the standardized\n",
    "residuals vary across\n",
    "$x_i$ more than expected. Under the homoscedasticity\n",
    "assumption, the variance\n",
    "should be independent of $x_i$. If not, then this is a\n",
    "clue that there is a\n",
    "missing variable in the analysis or that the variables\n",
    "themselves should be\n",
    "transformed (e.g., using the $\\log$ function) into another\n",
    "format that can\n",
    "reduce this effect. Also, we can use weighted least-squares\n",
    "instead of ordinary\n",
    "least-squares.\n",
    "\n",
    "**Variable Scaling.** It is tempting to conclude in a multiple\n",
    "regression that small coefficients in any of the $\\boldsymbol{\\beta}$ terms\n",
    "implies that those terms are not important.  However, simple unit conversions\n",
    "can cause this effect. For example, if one of the regressors is in\n",
    "units of\n",
    "kilometers and the others are in meters, then just the scale\n",
    "factor can give the\n",
    "impression of outsized or under-sized effects. The\n",
    "common way to account for\n",
    "this is to scale the regressors so that\n",
    "\n",
    "$$\n",
    "x^\\prime = \\frac{x-\\bar{x}}{\\sigma_x}\n",
    "$$\n",
    "\n",
    " This has the side effect of converting the slope parameters\n",
    "into correlation\n",
    "coefficients, which is bounded by $\\pm 1$.\n",
    "\n",
    "**Influential Data.** We have\n",
    "already discussed the idea\n",
    "of leverage. The concept of *influence* combines\n",
    "leverage with\n",
    "outliers. To understand influence, consider\n",
    "[Figure](#fig:Regression_005).\n",
    "\n",
    "<!-- dom:FIGURE: [fig-\n",
    "statistics/Regression_005.png, width=500 frac=0.65] The point on the right has\n",
    "outsized influence in this data because it is the only one used to determine the\n",
    "slope of the fitted line.  <div id=\"fig:Regression_005\"></div> -->\n",
    "<!-- begin\n",
    "figure -->\n",
    "<div id=\"fig:Regression_005\"></div>\n",
    "\n",
    "<p>The point on the right has\n",
    "outsized influence in this data because it is the only one used to determine the\n",
    "slope of the fitted line.</p>\n",
    "<img src=\"fig-statistics/Regression_005.png\"\n",
    "width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "The point on the right in\n",
    "[Figure](#fig:Regression_005) is the only one that\n",
    "contributes to the\n",
    "calculation of the slope for the fitted line. Thus, it\n",
    "is very influential in\n",
    "this sense. Cook's distance is a good way to get at this\n",
    "concept numerically.\n",
    "To compute this, we have to compute the $j^{th}$\n",
    "component of the estimated\n",
    "target variable with the $i^{th}$ point deleted. We\n",
    "call this $\\hat{y}_{j(i)}$.\n",
    "Then, we compute the following,\n",
    "\n",
    "$$\n",
    "D_i =\\frac{\\sum_j (\\hat{y}_j- \\hat{y}_{j(i)})^2}{p/n \\sum_j (\\hat{y}_j-\n",
    "y_j)^2}\n",
    "$$\n",
    "\n",
    " where, as before, $p$ is the number of estimated terms\n",
    "(e.g., $p=2$ in the\n",
    "bivariate case). This calculation emphasizes the\n",
    "effect of the outlier by\n",
    "predicting the target variable with and\n",
    "without each point. In the case of\n",
    "[Figure](#fig:Regression_005),\n",
    "losing any of the points on the left cannot\n",
    "change the estimated\n",
    "target variable much, but losing the single point on the\n",
    "right surely\n",
    "does. The point on the right does not seem to be an outlier (it\n",
    "*is*\n",
    "on the fitted line), but this is because it is influential enough to\n",
    "rotate\n",
    "the line to align with it. Cook's distance helps capture this\n",
    "effect by leaving\n",
    "each sample out and re-fitting the remainder as\n",
    "shown in the last equation.\n",
    "[Figure](#fig:Regression_006) shows the\n",
    "calculated Cook's distance for the data\n",
    "in [Figure](#fig:Regression_005), showing that the data point on the right\n",
    "(sample index `5`) has outsized influence on the fitted line. As a\n",
    "rule of\n",
    "thumb, Cook's distance values that  are larger than one are\n",
    "suspect.\n",
    "\n",
    "<!--\n",
    "dom:FIGURE: [fig-statistics/Regression_006.png, width=500 frac=0.65] The\n",
    "calculated Cook's distance for the data in [Figure](#fig:Regression_005).  <div\n",
    "id=\"fig:Regression_006\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div\n",
    "id=\"fig:Regression_006\"></div>\n",
    "\n",
    "<p>The calculated Cook's distance for the data\n",
    "in [Figure](#fig:Regression_005).</p>\n",
    "<img src=\"fig-\n",
    "statistics/Regression_006.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "As another\n",
    "illustration of influence, consider [Figure](#fig:Regression_007) which shows\n",
    "some data that nicely line up, but\n",
    "with one outlier (filled black circle) in the\n",
    "upper panel.  The lower\n",
    "panel shows so-computed Cook's distance for this data\n",
    "and emphasizes\n",
    "the presence of the outlier.  Because the calculation involves\n",
    "leaving\n",
    "a single sample out and re-calculating the rest, it can be a\n",
    "time-\n",
    "consuming operation suitable to relatively small data sets. There\n",
    "is always the\n",
    "temptation to downplay the importance of outliers\n",
    "because they conflict with a\n",
    "favored model, but outliers must be\n",
    "carefully examined to understand why the\n",
    "model is unable to capture\n",
    "them. It could be something as simple as faulty data\n",
    "collection, or it\n",
    "could be an indication of deeper issues that have been\n",
    "overlooked. The\n",
    "following code shows how Cook's distance was compute for\n",
    "[Figure](#fig:Regression_006) and [Figure](#fig:Regression_007)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = lambda i,x,y: np.polyval(np.polyfit(x,y,1),i)\n",
    "omit = lambda i,x: ([k for j,k in enumerate(x) if j !=i])\n",
    "def cook_d(k):\n",
    "   num = sum((fit(j,omit(k,x),omit(k,y))-fit(j,x,y))**2 for j in x)\n",
    "   den = sum((y-np.polyval(np.polyfit(x,y,1),x))**2/len(x)*2)\n",
    "   return num/den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The function `omit` sweeps through the data and excludes\n",
    "the $i^{th}$ data element. The embedded `enumerate` function\n",
    "associates every\n",
    "element in the iterable with its corresponding\n",
    "index.\n",
    "\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig-\n",
    "statistics/Regression_007.png, width=500 frac=0.65] The upper panel shows data\n",
    "that fit on a line and an outlier point (filled black circle). The lower panel\n",
    "shows the calculated Cook's distance for the data in upper panel and shows that\n",
    "the tenth point (i.e., the outlier) has disproportionate influence.  <div\n",
    "id=\"fig:Regression_007\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div\n",
    "id=\"fig:Regression_007\"></div>\n",
    "\n",
    "<p>The upper panel shows data that fit on a line\n",
    "and an outlier point (filled black circle). The lower panel shows the calculated\n",
    "Cook's distance for the data in upper panel and shows that the tenth point\n",
    "(i.e., the outlier) has disproportionate influence.</p>\n",
    "<img src=\"fig-\n",
    "statistics/Regression_007.png\" width=500>\n",
    "\n",
    "<!-- end figure -->"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
