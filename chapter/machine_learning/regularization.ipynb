{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # TODO: Elastic Net -->\n",
    "<!-- # TODO: Nice intuition about ridge regression Data_Analysis_Data_Mining_Azzalini, p. 64 -->\n",
    "\n",
    "\n",
    "We have referred to regularization in the section [ch:ml:sec:logreg](#ch:ml:sec:logreg), but we want to develop this important\n",
    "idea more fully. Regularization is the mechanism by which we\n",
    "navigate the bias/variance trade-off.  To get started, let's\n",
    "consider a classic constrained least squares problem,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\mathbf{x}}{\\text{minimize}}\n",
    "& & \\Vert\\mathbf{x}\\Vert_2^2 \\\\\n",
    "& \\text{subject to:}\n",
    "& & x_0 + 2 x_1 = 1\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\Vert\\mathbf{x}\\Vert_2=\\sqrt{x_0^2+x_1^2}$ is the\n",
    "$L_2$ norm.  Without the constraint, it would be easy to minimize\n",
    "the objective function --- just take $\\mathbf{x}=0$. Otherwise,\n",
    "suppose we somehow know  that $\\Vert\\mathbf{x}\\Vert_2<c$, then\n",
    "the locus of points defined by this inequality is the circle in\n",
    "[Figure](#fig:regularization_001). The constraint is the line in\n",
    "the same figure. Because every value of $c$ defines a circle, the\n",
    "constraint is satisfied  when the circle touches the line. The\n",
    "circle can touch the line at many different points, but we are\n",
    "only interested in the smallest such circle because this is a\n",
    "minimization problem.  Intuitively, this means that we *inflate* a\n",
    "$L_2$  ball at the origin and stop when it just touches the\n",
    "contraint.  The point of contact is our $L_2$ minimization\n",
    "solution.   \n",
    "\n",
    "<!-- dom:FIGURE: [fig-machine_learning/regularization_001.png, width=500 frac=0.75] The solution of the constrained $L_2$ minimization problem is at the point where the constraint (dark line) intersects the $L_2$ ball (gray circle) centered at the origin. The point of intersection is indicated by the dark circle. The two neighboring squares indicate points on the line that are close to the solution. <div id=\"fig:regularization_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:regularization_001\"></div>\n",
    "\n",
    "<p>The solution of the constrained $L_2$ minimization problem is at the point where the constraint (dark line) intersects the $L_2$ ball (gray circle) centered at the origin. The point of intersection is indicated by the dark circle. The two neighboring squares indicate points on the line that are close to the solution.</p>\n",
    "<img src=\"fig-machine_learning/regularization_001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "We can obtain the same result using the method of Lagrange\n",
    "multipliers. We can rewrite the entire $L_2$ minimization problem\n",
    "as one objective function using the Lagrange multiplier,\n",
    "$\\lambda$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(x_0,x_1,\\lambda) = x_0^2+x_1^2 + \\lambda (1-x_0-x_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and solve this as an ordinary function using calculus. Let's\n",
    "do this using Sympy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sympy as S\n",
    "S.var('x:2 l',real=True)\n",
    "J=S.Matrix([x0,x1]).norm()**2 + l*(1-x0-2*x1)\n",
    "sol=S.solve(map(J.diff,[x0,x1,l]))\n",
    "print(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "Using  the `Matrix` object is overkill for this problem but it\n",
    "does demonstrate how Sympy's matrix machinery works. In this case,\n",
    "we are using the `norm` method to compute the $L_2$  norm of the\n",
    "given elements. Using `S.var` defines Sympy variables and injects\n",
    "them into the global namespace. It is more Pythonic to do\n",
    "something like `x0 = S.symbols('x0',real=True)` instead but the\n",
    "other way is quicker, especially for variables with many\n",
    "dimensions.\n",
    "\n",
    "\n",
    "\n",
    " The solution defines the exact point where the line is\n",
    "tangent to the circle in [Figure](#fig:regularization_001).  The\n",
    "Lagrange multiplier has incorporated the constraint into the objective\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "from numpy import pi, linspace, sqrt\n",
    "from matplotlib.patches import Circle\n",
    "from matplotlib.pylab import subplots\n",
    "\n",
    "x1 = linspace(-1,1,10)\n",
    "dx=linspace(.7,1.3,3)\n",
    "fline = lambda x:(1-x)/2.\n",
    "\n",
    "fig,ax=subplots()\n",
    "_=ax.plot(dx*1/5,fline(dx*1/5),'s',ms=10,color='gray')\n",
    "_=ax.plot(x1,fline(x1),color='gray',lw=3)\n",
    "_=ax.add_patch(Circle((0,0),1/sqrt(5),alpha=0.3,color='gray'))\n",
    "_=ax.plot(1/5,2/5,'o',color='k',ms=15)\n",
    "_=ax.set_xlabel('$x_1$',fontsize=24)\n",
    "_=ax.set_ylabel('$x_2$',fontsize=24)\n",
    "_=ax.axis((-0.6,0.6,-0.6,0.6))\n",
    "ax.set_aspect(1)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-machine_learning/regularization_001.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is something subtle and very important about the nature of the solution,\n",
    "however. Notice that there are other points very close to the solution on the\n",
    "circle, indicated by the squares in [Figure](#fig:regularization_001). This\n",
    "closeness could be a good thing, in case it helps us actually find a solution\n",
    "in the first place, but it may be unhelpful in so far as it creates ambiguity.\n",
    "Let's hold that thought and try the same problem using the $L_1$ norm instead\n",
    "of the $L_2$ norm. Recall that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Vert \\mathbf{x}\\Vert_1 = \\sum_{i=1}^d \\vert x_i \\vert\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $d$ is the dimension of the vector $\\mathbf{x}$. Thus, we can\n",
    "reformulate the same problem in the $L_1$  norm as in the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\mathbf{x}}{\\text{minimize}}\n",
    "& & \\Vert\\mathbf{x}\\Vert_1 \\\\\n",
    "& \\text{subject to:}\n",
    "& & x_1 + 2 x_2 = 1\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  It turns out that this problem is somewhat harder to\n",
    "solve using Sympy, but we have convex optimization modules in Python\n",
    "that can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cvxpy import Variable, Problem, Minimize, norm1, norm\n",
    "x=Variable((2,1),name='x')\n",
    "constr=[np.matrix([[1,2]])*x==1]\n",
    "obj=Minimize(norm1(x))\n",
    "p= Problem(obj,constr)\n",
    "p.solve()\n",
    "print(x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The `cvxy` module provides a unified and accessible interface to the powerful\n",
    "`cvxopt` convex optimization package, as well as other open-source solver\n",
    "packages.\n",
    "\n",
    "\n",
    "\n",
    " As shown in [Figure](#fig:regularization_002), the constant-norm\n",
    "contour in the $L_1$ norm is shaped like a diamond instead of a circle.\n",
    "Furthermore, the solutions found in each case are different.  Geometrically,\n",
    "this is because inflating the circular $L_2$ reaches out in all directions\n",
    "whereas the $L_1$ ball creeps out along the principal axes.  This effect is\n",
    "much more pronounced in higher dimensional spaces where $L_1$-balls get more\n",
    "spikey  [^spikey].  Like the $L_2$ case, there are also neighboring points on\n",
    "the constraint line, but notice that these are not close to the boundary of the\n",
    "corresponding $L_1$ ball, as they were in the $L_2$ case.  This means that\n",
    "these would be harder to confuse with the optimal solution because they\n",
    "correspond to a substantially different $L_1$ ball.\n",
    "\n",
    "[^spikey]: We discussed the geometry of high dimensional space \n",
    "when we covered the curse of dimensionality in the\n",
    "statistics chapter.\n",
    "\n",
    "To double-check our earlier $L_2$ result, we can also use the\n",
    "`cvxpy` module to find the $L_2$ solution as in the following\n",
    "code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "constr=[np.matrix([[1,2]])*x==1]\n",
    "obj=Minimize(norm(x,2)) #L2 norm\n",
    "p= Problem(obj,constr)\n",
    "p.solve()\n",
    "print(x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The only change to the code is the $L_2$ norm and we get\n",
    "the same solution as before.  \n",
    "\n",
    "Let's see what happens in higher dimensions for both $L_2$ and\n",
    "$L_1$ as we move from two dimensions to four dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=Variable((4,1),name='x')\n",
    "constr=[np.matrix([[1,2,3,4]])*x==1]\n",
    "obj=Minimize(norm1(x))\n",
    "p= Problem(obj,constr)\n",
    "p.solve()\n",
    "print(x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " And also in the $L_2$ case with the following code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "constr=[np.matrix([[1,2,3,4]])*x==1]\n",
    "obj=Minimize(norm(x,2))\n",
    "p= Problem(obj,constr)\n",
    "p.solve()\n",
    "print(x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that the $L_1$ solution has selected out only one\n",
    "dimension for the solution, as the other components are\n",
    "effectively zero. This is not so with the $L_2$ solution, which\n",
    "has meaningful elements in multiple coordinates.  This is because\n",
    "the $L_1$ problem has many pointy corners in the four dimensional\n",
    "space that poke at the hyperplane that is defined by the\n",
    "constraint. This essentially means the subsets (namely, the points\n",
    "at the corners) are found as solutions because these touch the\n",
    "hyperplane. This effect becomes more pronounced in higher\n",
    "dimensions, which is the main benefit of using the $L_1$ norm\n",
    "as we will see in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle, RegularPolygon\n",
    "\n",
    "r=RegularPolygon((0,0),4,1/2,pi/2,alpha=0.5,color='gray')\n",
    "fig,ax=subplots()\n",
    "dx = np.array([-0.1,0.1])\n",
    "_=ax.plot(dx,fline(dx),'s',ms=10,color='gray')\n",
    "_=ax.plot(x1,fline(x1),color='gray',lw=3)\n",
    "_=ax.plot(0,1/2,'o',color='k',ms=15)\n",
    "_=ax.add_patch(r)\n",
    "_=ax.set_xlabel('$x_1$',fontsize=24)\n",
    "_=ax.set_ylabel('$x_2$',fontsize=24)\n",
    "_=ax.axis((-0.6,0.6,-0.6,0.6))\n",
    "_=ax.set_aspect(1)\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-machine_learning/regularization_002.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/regularization_002.png, width=500 frac=0.75] The diamond is the $L_1$ ball in two dimensions and the line is the constraint. The point of intersection is the solution to the optimization problem. Note that for $L_1$ optimization, the two nearby points on the constraint (squares) do not touch the $L_1$ ball. Compare this with [Figure](#fig:regularization_001). <div id=\"fig:regularization_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:regularization_002\"></div>\n",
    "\n",
    "<p>The diamond is the $L_1$ ball in two dimensions and the line is the constraint. The point of intersection is the solution to the optimization problem. Note that for $L_1$ optimization, the two nearby points on the constraint (squares) do not touch the $L_1$ ball. Compare this with [Figure](#fig:regularization_001).</p>\n",
    "<img src=\"fig-machine_learning/regularization_002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "<!-- p. 168 D:\\Volume2_Indexed\\Statistical_Machine_Learning_Notes_Tibshirani.pdf -->\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "Now that we have a sense of the geometry of the situation, let's revisit\n",
    "our classic linear regression probem. To recap, we want to solve the following\n",
    "problem,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{\\boldsymbol{\\upbeta}\\in \\mathbb{R}^p} \\Vert y - \\mathbf{X}\\boldsymbol{\\upbeta}\\Vert\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\mathbf{X}=\\left[\n",
    "\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_p \\right]$ and $\\mathbf{x}_i\\in\n",
    "\\mathbb{R}^n$. Furthermore, we assume that the $p$ column vectors are linearly\n",
    "independent (i.e., $\\texttt{rank}(\\mathbf{X})=p$). Linear regression produces\n",
    "the $\\boldsymbol{\\upbeta}$ that minimizes the mean squared error above.  In the\n",
    "case where $p=n$, there is a unique solution to this problem. However, when\n",
    "$p<n$, then there are infinitely many solutions.\n",
    "\n",
    "To make this concrete, let's work this out using Sympy. First,\n",
    "let's define an example $\\mathbf{X}$ and $\\mathbf{y}$ matrix,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sympy as S\n",
    "from sympy import Matrix\n",
    "X = Matrix([[1,2,3],\n",
    "            [3,4,5]])\n",
    "y = Matrix([[1,2]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Now, we can define our coefficient vector $\\boldsymbol{\\upbeta}$\n",
    "using the following code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b0,b1,b2=S.symbols('b:3',real=True)\n",
    "beta = Matrix([[b0,b1,b2]]).T # transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next, we define the objective function we are trying to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obj=(X*beta -y).norm(ord=2)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The Sympy `Matrix` class has useful methods like the `norm` function\n",
    "used above to define the objective function. The `ord=2` means we want\n",
    "to use the $L_2$ norm. The expression in parenthesis evaluates to a\n",
    "`Matrix` object.\n",
    "\n",
    "\n",
    "\n",
    " Note that it is helpful to define real variables using\n",
    "the keyword argument whenever applicable because it relieves\n",
    "Sympy's internal machinery of dealing with complex numbers.\n",
    "Finally, we can use calculus to solve this by setting the\n",
    "derivatives of the objective function to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sol=S.solve([obj.diff(i) for i in beta])\n",
    "beta.subs(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Notice that the solution does not uniquely specify all the components\n",
    "of the `beta` variable. This is a consequence of the $p<n$ nature of this\n",
    "problem where $p=2$ and $n=3$.  While the the existence of this ambiguity does\n",
    "not alter the solution,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obj.subs(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " But it does change the length of the solution vector\n",
    "`beta`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta.subs(sol).norm(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If we want to minimize this length we can easily\n",
    "use the same calculus as before,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S.solve((beta.subs(sol).norm()**2).diff())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This provides the solution of minimum length\n",
    "in the $L_2$ sense,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betaL2=beta.subs(sol).subs(b2,S.Rational(1,6))\n",
    "betaL2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " But what is so special about solutions of minimum length? For machine\n",
    "learning, driving the objective function to zero is symptomatic of overfitting\n",
    "the data. Usually, at the zero bound, the machine learning method has\n",
    "essentially memorized the training data, which is bad for generalization. Thus,\n",
    "we can effectively stall this problem by defining a region for the solution\n",
    "that is away from the zero-bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\boldsymbol{\\upbeta}}{\\text{minimize}}\n",
    "& & \\Vert y - \\mathbf{X}\\boldsymbol{\\upbeta}\\Vert_2^2 \\\\\n",
    "& \\text{subject to:}\n",
    "& & \\Vert\\boldsymbol{\\upbeta}\\Vert_2 < c\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $c$ is the tuning parameter. Using the same process as before,\n",
    "we can re-write this as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{\\boldsymbol{\\upbeta}\\in\\mathbb{R}^p}\\Vert y-\\mathbf{X}\\boldsymbol{\\upbeta}\\Vert_2^2 +\\alpha\\Vert\\boldsymbol{\\upbeta}\\Vert_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\alpha$ is the tuning parameter. These are the *penalized* or\n",
    "Lagrange forms of these problems derived from the constrained versions. The\n",
    "objective function is penalized by the $\\Vert\\boldsymbol{\\upbeta}\\Vert_2$ term.\n",
    "For $L_2$ penalization, this is called *ridge* regression.    This is\n",
    "implemented in Scikit-learn as `Ridge`. The following code sets this up for\n",
    "our example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "clf = Ridge(alpha=100.0,fit_intercept=False)\n",
    "clf.fit(np.array(X).astype(float),np.array(y).astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that the `alpha` scales of the penalty for the\n",
    "$\\Vert\\boldsymbol{\\upbeta}\\Vert_2$. We set the `fit_intercept=False` argument to\n",
    "omit the extra offset term from our example. The corresponding solution is the\n",
    "following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To double-check the solution, we can use some optimization tools from\n",
    "Scipy and our previous Sympy analysis, as in the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "f   = S.lambdify((b0,b1,b2),obj+beta.norm()**2*100.)\n",
    "g   = lambda x:f(x[0],x[1],x[2])\n",
    "out = minimize(g,[.1,.2,.3]) # initial guess\n",
    "out.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "We had to define the additional `g` function from the lambda function we\n",
    "created from the Sympy expression in `f` because the `minimize` function\n",
    "expects a single object vector as input instead of a three separate arguments.\n",
    "\n",
    "\n",
    "\n",
    " which produces the same answer as the `Ridge` object. To\n",
    "better understand the meaning of this result, we can re-compute the\n",
    "mean squared error solution to this problem in one step using matrix\n",
    "algebra instead of calculus,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betaLS=X.T*(X*X.T).inv()*y\n",
    "betaLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Notice that this solves the posited problem exactly,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X*betaLS-y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This means that the first term in the objective function\n",
    "goes to zero,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Vert y-\\mathbf{X}\\boldsymbol{\\upbeta}_{LS}\\Vert=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " But, let's examine the $L_2$ length of this solution versus\n",
    "the ridge regression solution,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(betaLS.norm().evalf(), np.linalg.norm(clf.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Thus, the ridge regression solution is shorter in the $L_2$\n",
    "sense, but the first term in the objective function is not zero for\n",
    "ridge regression,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print((y-X*clf.coef_.T).norm()**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ridge regression solution trades fitting error\n",
    "($\\Vert y-\\mathbf{X} \\boldsymbol{\\upbeta}\\Vert_2$) for solution\n",
    "length ($\\Vert\\boldsymbol{\\upbeta}\\Vert_2$). \n",
    "\n",
    "Let's see this in action with a familiar example from\n",
    "[ch:stats:sec:nnreg](#ch:stats:sec:nnreg).  Consider [Figure](#fig:regularization_003).\n",
    "For this example, we created our usual chirp signal and attempted to\n",
    "fit it with a high-dimensional polynomial, as we did in\n",
    "the section [ch:ml:sec:cv](#ch:ml:sec:cv).  The lower panel is the same except with ridge\n",
    "regression. The shaded gray area is the space between the true signal\n",
    "and the approximant in both cases. The horizontal hash marks indicate\n",
    "the subset of $x_i$ values that each regressor was trained on.\n",
    "Thus, the training set represents a non-uniform sample of the\n",
    "underlying chirp waveform.  The top panel shows the usual polynomial\n",
    "regression. Note that the regressor fits the given points extremely\n",
    "well, but fails at the endpoint. The ridge regressor misses many of\n",
    "the points in the middle, as indicated by the gray area, but does not\n",
    "overshoot at the ends as much as the plain polynomial regression. This\n",
    "is the basic trade-off for ridge regression. The Jupyter\n",
    "notebook corresponding to this section has the code for this graph, but the main steps\n",
    "are shown in the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create chirp signal\n",
    "xi = np.linspace(0,1,100)[:,None]\n",
    "# sample chirp randomly\n",
    "xin= np.sort(np.random.choice(xi.flatten(),20,replace=False))[:,None]\n",
    "# create sampled waveform\n",
    "y = np.cos(2*pi*(xin+xin**2))\n",
    "# create full waveform for reference\n",
    "yi = np.cos(2*pi*(xi+xi**2))\n",
    "\n",
    "# create polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "qfit = PolynomialFeatures(degree=8) # quadratic\n",
    "Xq = qfit.fit_transform(xin)\n",
    "# reformat input as polynomial\n",
    "Xiq = qfit.fit_transform(xi)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr=LinearRegression() # create linear model \n",
    "lr.fit(Xq,y) # fit linear model\n",
    "\n",
    "# create ridge regression model and fit\n",
    "clf = Ridge(alpha=1e-9,fit_intercept=False)\n",
    "clf.fit(Xq,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from numpy import cos, pi\n",
    "\n",
    "np.random.seed(1234567)\n",
    "xi = np.linspace(0,1,100)[:,None]\n",
    "xin = np.linspace(0,1,20)[:,None]\n",
    "xin= np.sort(np.random.choice(xi.flatten(),20,replace=False))[:,None]\n",
    "f0 = 1 # init frequency\n",
    "BW = 2\n",
    "y = cos(2*pi*(f0*xin+(BW/2.0)*xin**2))\n",
    "yi = cos(2*pi*(f0*xi+(BW/2.0)*xi**2))\n",
    "\n",
    "qfit = PolynomialFeatures(degree=8) # quadratic\n",
    "Xq = qfit.fit_transform(xin)\n",
    "Xiq = qfit.fit_transform(xi)\n",
    "\n",
    "lr=LinearRegression() # create linear model \n",
    "_=lr.fit(Xq,y)\n",
    "\n",
    "fig,axs=subplots(2,1,sharex=True,sharey=True)\n",
    "fig.set_size_inches((6,6))\n",
    "ax=axs[0]\n",
    "_=ax.plot(xi,yi,label='true',ls='--',color='k')\n",
    "_=ax.plot(xi,lr.predict(Xiq),label=r'$\\beta_{LS}$',color='k')\n",
    "_=ax.legend(loc=0)\n",
    "_=ax.set_ylabel(r'$\\hat{y}$  ',fontsize=22,rotation='horizontal')\n",
    "_=ax.fill_between(xi.flatten(),yi.flatten(),lr.predict(Xiq).flatten(),color='gray',alpha=.3)\n",
    "_=ax.set_title('Polynomial Regression of Chirp Signal')\n",
    "_=ax.plot(xin, -1.5+np.array([0.01]*len(xin)), '|', color='k',mew=3)\n",
    "\n",
    "clf = Ridge(alpha=1e-9,fit_intercept=False)\n",
    "_=clf.fit(Xq,y)\n",
    "ax=axs[1]\n",
    "_=ax.plot(xi,yi,label=r'true',ls='--',color='k')\n",
    "_=ax.plot(xi,clf.predict(Xiq),label=r'$\\beta_{RR}$',color='k')\n",
    "_=ax.legend(loc=(0.25,0.70))\n",
    "_=ax.fill_between(xi.flatten(),yi.flatten(),clf.predict(Xiq).flatten(),color='gray',alpha=.3)\n",
    "# add rug plot\n",
    "_=ax.plot(xin, -1.5+np.array([0.01]*len(xin)), '|', color='k',mew=3)\n",
    "_=ax.set_xlabel('$x$',fontsize=22)\n",
    "_=ax.set_ylabel(r'$\\hat{y}$  ',fontsize=22,rotation='horizontal')\n",
    "_=ax.set_title('Ridge Regression of Chirp Signal')\n",
    "fig.savefig('fig-machine_learning/regularization_003.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/regularization_003.png, width=500 frac=0.85] The top figure shows polynomial regression and the lower panel shows polynomial ridge regression. The ridge regression does not match as well throughout most of the domain, but it does not flare as violently at the ends.  This is because the ridge constraint holds the coefficient vector down at the expense of poorer performance along the middle of the domain. <div id=\"fig:regularization_003\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:regularization_003\"></div>\n",
    "\n",
    "<p>The top figure shows polynomial regression and the lower panel shows polynomial ridge regression. The ridge regression does not match as well throughout most of the domain, but it does not flare as violently at the ends.  This is because the ridge constraint holds the coefficient vector down at the expense of poorer performance along the middle of the domain.</p>\n",
    "<img src=\"fig-machine_learning/regularization_003.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "## Lasso Regression\n",
    "\n",
    "Lasso regression follows the same basic pattern as ridge regression,\n",
    "except with the $L_1$ norm in the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{\\boldsymbol{\\upbeta}\\in\\mathbb{R}^p}\\Vert y-\\mathbf{X}\\boldsymbol{\\upbeta}\\Vert^2 +\\alpha\\Vert\\boldsymbol{\\upbeta}\\Vert_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The interface in Scikit-learn is likewise the same. \n",
    "The following is the same problem as before using lasso\n",
    "instead of ridge regression,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.matrix([[1,2,3],\n",
    "               [3,4,5]])\n",
    "y = np.matrix([[1,2]]).T\n",
    "from sklearn.linear_model import Lasso\n",
    "lr = Lasso(alpha=1.0,fit_intercept=False)\n",
    "_=lr.fit(X,y)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As before, we can use the optimization tools in Scipy to solve this\n",
    "also,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin\n",
    "obj = 1/4.*(X*beta-y).norm(2)**2 + beta.norm(1)*l\n",
    "f = S.lambdify((b0,b1,b2),obj.subs(l,1.0))\n",
    "g = lambda x:f(x[0],x[1],x[2])\n",
    "fmin(g,[0.1,0.2,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The `fmin` function from Scipy's optimization module uses an\n",
    "algorithm that does not depend upon derivatives. This is useful\n",
    "because, unlike the $L_2$ norm, the $L_1$ norm has sharp corners\n",
    "that make it harder to estimate derivatives.\n",
    "\n",
    "\n",
    "\n",
    " This result matches the previous one from the\n",
    "Scikit-learn `Lasso` object. Solving it using Scipy is motivating\n",
    "and provides a good sanity check, but specialized algorithms are\n",
    "required in practice. The following code block re-runs the lasso\n",
    "with varying $\\alpha$ and plots the coefficients in [Figure](#fig:regularization_004). Notice that as $\\alpha$ increases, all\n",
    "but one of the coefficients is driven to zero. Increasing $\\alpha$\n",
    "makes the trade-off between fitting the data in the $L_1$ sense\n",
    "and wanting to reduce the number of nonzero coefficients\n",
    "(equivalently, the number of features used) in the model. For a\n",
    "given problem, it may be more practical to focus on reducing the\n",
    "number of features in the model (i.e., large $\\alpha$) than the\n",
    "quality of the data fit in the training data. The lasso provides a\n",
    "clean way to navigate this trade-off.\n",
    "\n",
    "The following code loops over a set of $\\alpha$ values and\n",
    "collects the corresponding lasso coefficients to be plotted\n",
    "in [Figure](#fig:regularization_004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o=[]\n",
    "alphas= np.logspace(-3,0,10)\n",
    "for a in alphas:\n",
    "    clf = Lasso(alpha=a,fit_intercept=False)\n",
    "    _=clf.fit(X,y)\n",
    "    o.append(clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "fig.set_size_inches((8,5))\n",
    "k=np.vstack(o)\n",
    "ls = ['-','--',':','-.']\n",
    "for i in range(k.shape[1]):\n",
    "    _=ax.semilogx(alphas,k[:,i],'o-',\n",
    "                label='coef %d'%(i),\n",
    "                color='k',ls=ls[i],\n",
    "                alpha=.8,)\n",
    "\n",
    "_=ax.axis(ymin=-1e-1)\n",
    "_=ax.legend(loc=0)\n",
    "_=ax.set_xlabel(r'$\\alpha$',fontsize=20)\n",
    "_=ax.set_ylabel(r'Lasso coefficients',fontsize=16)\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-machine_learning/regularization_004.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/regularization_004.png, width=500 frac=0.85] As $\\alpha$ increases, more of the model coefficients are driven to zero for lasso regression.  <div id=\"fig:regularization_004\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:regularization_004\"></div>\n",
    "\n",
    "<p>As $\\alpha$ increases, more of the model coefficients are driven to zero for lasso regression.</p>\n",
    "<img src=\"fig-machine_learning/regularization_004.png\" width=500>\n",
    "\n",
    "<!-- end figure -->"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
