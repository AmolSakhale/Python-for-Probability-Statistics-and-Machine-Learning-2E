{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- add feature importances from RandomForestClassifier -->\n",
    "<!-- TODO: [stochastic gradient trees and boosting](http://nbviewer.ipython.org/github/pprett/pydata-gbrt-tutorial/blob/master/gbrt-tutorial.ipynb) -->\n",
    "<!-- <https://en.wikipedia.org/wiki/Gradient_boosting> -->\n",
    "\n",
    "<!-- Elements_of_Statistical_Learning_Hastie.pdf, Chap 10 -->\n",
    "\n",
    "## Boosting Trees\n",
    "\n",
    "To understand additive modeling using trees, recall the\n",
    "Gram-Schmidt orthogonalization procedure for vectors.  The\n",
    "purpose of this orthogonalization procedure is to create an\n",
    "orthogonal set of vectors starting with a given vector\n",
    "$\\mathbf{u}_1$. We have already discussed the projection\n",
    "operator in the section [ch:prob:sec:projection](#ch:prob:sec:projection).  The\n",
    "Gram-Schmidt orthogonalization procedure starts with a\n",
    "vector $\\mathbf{v}_1$, which we define as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{u}_1 = \\mathbf{v}_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " with the corresponding projection operator $proj_{\\mathbf{u}_1}$. The\n",
    "next step in the procedure is to remove the residual of  $\\mathbf{u}_1$\n",
    "from $\\mathbf{v}_2$, as in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{u}_2 = \\mathbf{v}_2 -  proj_{\\mathbf{u}_1}(\\mathbf{v}_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This procedure continues for $\\mathbf{v}_3$ as in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{u}_3 = \\mathbf{v}_3 - proj_{\\mathbf{u}_1}(\\mathbf{v}_3) - proj_{\\mathbf{u}_2}(\\mathbf{v}_3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and so on. The important aspect of this procedure is that new\n",
    "incoming vectors (i.e., $\\mathbf{v}_k$) are stripped of any pre-existing\n",
    "components already present in the set of $\\lbrace\\mathbf{u}_1,\\mathbf{u}_2,\\ldots,\\mathbf{u}_M \\rbrace$.\n",
    "\n",
    "Note that this procedure is sequential. That is, the *order*\n",
    "of the incoming $\\mathbf{v}_i$ matters [^rotation]. Thus,\n",
    "any new vector can be expressed using the so-constructed\n",
    "$\\lbrace\\mathbf{u}_1,\\mathbf{u}_2,\\ldots,\\mathbf{u}_M\n",
    "\\rbrace$ basis set, as in the following:\n",
    "\n",
    "[^rotation]: At least up to a rotation of the resulting\n",
    "orthonormal basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{x} = \\sum \\alpha_i \\mathbf{u}_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The idea behind additive trees is to reproduce this procedure for\n",
    "trees instead of vectors. There are many natural topological and algebraic\n",
    "properties that we lack for the general problem, however. For example, we already\n",
    "have well established methods for measuring distances between vectors for\n",
    "the Gram-Schmidt procedure outlined above (namely, the $L_2$\n",
    "distance), which we lack here.  Thus, we need the concept of *loss function*,\n",
    "which is a way of measuring how well the process is working out at each\n",
    "sequential step. This loss function is parameterized by the training data and\n",
    "by the classification function under consideration: $ L_{\\mathbf{y}}(f(x)) $.\n",
    "For example, if we want a classifier ($f$) that selects the label $y_i$\n",
    "based upon the input data $\\mathbf{x}_i$ ($f :\\mathbf{x}_i \\rightarrow y_i)$,\n",
    "then the squared error loss function would be the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L_{\\mathbf{y}}(f(x))  = \\sum_i (y_i - f(x_i))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We represent the classifier in terms of a set of basis trees:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = \\sum_k \\alpha_k u_{\\mathbf{x}}(\\theta_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The general algorithm for forward stagewise  additive modeling is \n",
    "the following:\n",
    "\n",
    "   * Initialize $f(x)=0$\n",
    "\n",
    "   * For $m=1$ to $m=M$, compute the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\beta_m,\\gamma_m) = \\argmin_{\\beta,\\gamma} \\sum_i  L(y_i,f_{m-1}(x_i)+\\beta b(x_i;\\gamma))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set $f_m(x) = f_{m-1}(x) +\\beta_m b(x;\\gamma_m) $\n",
    "\n",
    "The key point is that the residuals from the prior step are used to fit the\n",
    "basis function for the subsequent iteration.  That is, the following \n",
    "equation is being sequentially approximated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f_m(x) - f_{m-1}(x) =\\beta_m b(x_i;\\gamma_m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's see how this works for decision trees and the exponential loss\n",
    "function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(x,f(x)) = \\exp(-y f(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Recall that for the classification problem, $y\\in \\lbrace -1,1 \\rbrace$. For \n",
    "AdaBoost, the basis functions are the individual classifiers, $G_m(x)\\mapsto \\lbrace -1,1 \\rbrace$\n",
    "The key step in the algorithm is the minimization step for\n",
    "the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\beta,G) = \\sum_i \\exp(y_i(f_{m-1}(x_i)+\\beta G(x_i)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\beta_m,G_m) = \\argmin_{\\beta,G} \\sum_i \\exp(y_i(f_{m-1}(x_i)+\\beta G(x_i)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, because of the exponential, we can factor out the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w_i^{(m)} = \\exp(y_i f_{m-1}(x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " as a weight on each data element and re-write the objective function as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\beta,G) = \\sum_i w_i^{(m)} \\exp(y_i \\beta G(x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The important observation here is that $y_i G(x_i)\\mapsto 1$ if\n",
    "the tree classifies $x_i$ correctly and $y_i G(x_i)\\mapsto -1$ otherwise. \n",
    "Thus, the above sum has terms like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\beta,G) = \\sum_{y_i\\ne G(x_i)} w_i^{(m)} \\exp(-\\beta) + \\sum_{y_i= G(x_i)} w_i^{(m)}\\exp(\\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For $\\beta>0$, this means that  the best $G(x)$ is\n",
    "the one that incorrectly classifies for the largest weights. Thus, \n",
    "the minimizer is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "G_m = \\argmin_G \\sum_i w_i^{(m)} I(y_i \\ne G(x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $I$ is the indicator function (i.e.,\n",
    "$I(\\textnormal{True})=1,I(\\textnormal{False})=0$). \n",
    "\n",
    " For $\\beta>0$, we can re-write the objective function\n",
    "as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J= (\\exp(\\beta)-\\exp(-\\beta))\\sum_i w_i^{(m)} I(y_i \\ne G(x_i)) + \\exp(-\\beta) \\sum_i w_i^{(m)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and substitute $\\theta=\\exp(-\\beta)$ so that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:bt001\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{J}{\\sum_i w_i^{(m)}} = \\left(\\frac{1}{\\theta} - \\theta\\right)\\epsilon_m+\\theta\n",
    "\\label{eq:bt001} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\epsilon_m = \\frac{\\sum_i w_i^{(m)}I(y_i\\ne G(x_i))}{\\sum_i w_i^{(m)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  is the error rate of the classifier with $0\\le \\epsilon_m\\le 1$. Now,\n",
    "finding $\\beta$ is a straight-forward calculus minimization exercise on the\n",
    "right side of Equation ([1](#eq:bt001)), which gives the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_m  = \\frac{1}{2}\\log \\frac{1-\\epsilon_m}{\\epsilon_m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Importantly, $\\beta_m$ can become negative if\n",
    "$\\epsilon_m<\\frac{1}{2}$, which would would violate our assumptions on $\\beta$.\n",
    "This is captured in the requirement that the base learner be better than just\n",
    "random guessing, which would correspond to $\\epsilon_m>\\frac{1}{2}$.\n",
    "Practically speaking, this means that boosting cannot fix a base learner that\n",
    "is no better than a random guess. Formally speaking, this is known as the\n",
    "*empirical weak learning assumption* [[schapire2012boosting]](#schapire2012boosting). \n",
    "\n",
    "Now we can move to the iterative weight update. Recall that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w_i^{(m+1)} = \\exp(y_i f_{m}(x_i)) = w_i^{(m)}\\exp(y_i \\beta_m G_m(x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which we can rewrite as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w_i^{(m+1)} = w_i^{(m)}\\exp(\\beta_m)\\exp(-2\\beta_m I(G_m(x_i)=y_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This means that the data elements that are incorrectly classified have their\n",
    "corresponding weights increased by $\\exp(\\beta_m)$ and those that are correctly\n",
    "classified have their corresponding weights reduced by $\\exp(-\\beta_m)$.\n",
    "The reason for the choice of the exponential loss function comes from the\n",
    "following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f^{*}(x) = \\argmin_{f(x)} \\mathbb{E}_{Y\\vert x}(\\exp(-Y f(x))) = \\frac{1}{2}\\log \\frac{\\mathbb{P}(Y=1\\vert x)}{\\mathbb{P}(Y=\\minus 1\\vert x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This means that boosting is approximating a $f(x)$ that is actually\n",
    "half the log-odds of the conditional class probabilities.  This can be\n",
    "rearranged as the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(Y=1\\vert x) = \\frac{1}{1+\\exp(\\minus 2f^{*}(x))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The important benefit of this general formulation for boosting, as a sequence\n",
    "of additive approximations,  is that it opens the door to other choices of loss\n",
    "function, especially loss functions that are based on robust statistics that\n",
    "can account for errors in the training data (c.f. Hastie).\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "Given a differentiable loss function, the optimization process can be\n",
    "formulated using numerical gradients. The fundamental idea is to treat the\n",
    "$f(x_i)$ as a scalar parameter to be optimized over. Generally speaking,\n",
    "we can think of the following loss function,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(f) = \\sum_{i=1}^N L(y_i,f(x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " as a vectorized quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{f} = \\lbrace f(x_1),f(x_2),\\ldots, f(x_N) \\rbrace\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " so that the optimization is over this vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\mathbf{f}} = \\argmin_{\\mathbf{f}} L(\\mathbf{f})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " With this general formulation we can use numerical optimization methods \n",
    "to solve for the optimal $\\mathbf{f}$ as a sum of component vectors\n",
    "as in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{f}_M = \\sum_{m=0}^M \\mathbf{h}_m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that this leaves aside the prior assumption that $f$ \n",
    "is parameterized as a sum of individual decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "g_{i,m} = \\left[ \\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x_i)=f_{m-1}(x_i)}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
