{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bernoulli distribution we studied earlier answers the question of which of\n",
    "two outcomes ($Y \\in \\lbrace 0,1 \\rbrace$) would be selected with probability, $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(Y) = p^Y (1-p)^{ 1-Y }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We also know how to solve the corresponding likelihood function for\n",
    "the maximum likelihood estimate of $p$ given observations of the output,\n",
    "$\\lbrace Y_i \\rbrace_{i=1}^n$. However, now we want to include other factors in\n",
    "our estimate of $p$. For example, suppose we observe not just the outcomes, but\n",
    "a corresponding continuous variable, $x$. That is, the observed data is now\n",
    "$\\lbrace (x_i,Y_i) \\rbrace_{i=1}^n$  How can we incorporate $x$ into our\n",
    "estimation of $p$?\n",
    "\n",
    "The most straightforward idea is to model $p= a x + b$ where $a,b$ are\n",
    "parameters of a fitted line. However, because $p$ is a probability with value\n",
    "bounded between zero and one, we need to wrap this estimate in another function\n",
    "that can map the entire real line into the $[0,1]$ interval. The logistic\n",
    "(a.k.a. sigmoid) function has this property,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta(s) = \\frac{e^s}{1+e^s}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Thus, the new parameterized estimate for $p$ is the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:prob\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{p} = \\theta(a x+b)= \\frac{e^{a x + b}}{1+e^{a x + b}} \n",
    "\\label{eq:prob} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The *logit* function is defined as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\texttt{logit}(t)= \\log \\frac{t}{1-t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It has the important property of extracting the regression components \n",
    "from the probability estimator,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\texttt{logit}(p) = b + a x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " More continuous variables can be accommodated easily as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\texttt{logit}(p) = b + \\sum_k a_k x_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This can be further extended beyond the binary case to multiple\n",
    "target labels. The maximum likelihood estimate of this uses\n",
    "numerical optimization methods that are implemented in Scikit-learn.\n",
    "\n",
    "Let's construct some data to see how this works. In the following, we assign\n",
    "class labels to a set of randomly scattered points in the two-dimensional\n",
    "plane,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.pylab import subplots\n",
    "v = 0.9\n",
    "@np.vectorize\n",
    "def gen_y(x):\n",
    "    if x<5: return np.random.choice([0,1],p=[v,1-v]) \n",
    "    else:   return np.random.choice([0,1],p=[1-v,v])\n",
    "\n",
    "xi = np.sort(np.random.rand(500)*10)\n",
    "yi = gen_y(xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The `np.vectorize` decorator used in the code above makes it easy to avoid\n",
    "looping in code that uses Numpy arrays by embedding the looping semantics\n",
    "inside of the so-decorated function. Note, however, that this does not\n",
    "necessarily accelerate the wrapped function. It's mainly for convenience.\n",
    "\n",
    "\n",
    "\n",
    "[Figure](#fig:logreg_001) shows a scatter plot of the data we constructed in\n",
    "the above code, $\\lbrace (x_i,Y_i) \\rbrace$. As constructed, it is more\n",
    "likely that large values of $x$ correspond to $Y=1$. On the other hand, values\n",
    "of $x \\in [4,6]$ of either category are heavily overlapped. This means that $x$\n",
    "is not a particularly strong indicator of $Y$ in this region.  [Figure](#fig:logreg_002) shows the fitted logistic regression curve against the same\n",
    "data. The points along the curve are the probabilities that each point lies in\n",
    "either of the two categories. For large values of $x$ the curve is near one,\n",
    "meaning that the probability that the associated $Y$ value is equal to one. On\n",
    "the other extreme, small values of $x$ mean that this probability is close to\n",
    "zero.  Because there are only two possible categories, this means that the\n",
    "probability of $Y=0$ is thereby higher. The region in the middle corresponding\n",
    "to the middle probabilities reflect the ambiguity between the two catagories\n",
    "because of the overlap in the data for this region. Thus, logistic regression\n",
    "cannot make a strong case for one category here.\n",
    "The following code fits the logistic regression model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "_=ax.plot(xi,yi,'o',color='gray',alpha=.3)\n",
    "_=ax.axis(ymax=1.1,ymin=-0.1)\n",
    "_=ax.set_xlabel(r'$X$',fontsize=22)\n",
    "_=ax.set_ylabel(r'$Y$',fontsize=22)\n",
    "fig.savefig('fig-machine_learning/logreg_001.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(np.c_[xi],yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "xii=np.linspace(0,10,20)\n",
    "_=ax.plot(xii,lr.predict_proba(np.c_[xii])[:,1],'k-',lw=3)\n",
    "_=ax.plot(xi,yi,'o',color='gray',alpha=.3)\n",
    "_=ax.axis(ymax=1.1,ymin=-0.1)\n",
    "_=ax.set_xlabel(r'$x$',fontsize=20)\n",
    "_=ax.set_ylabel(r'$\\mathbb{P}(Y)$',fontsize=20)\n",
    "fig.savefig('fig-machine_learning/logreg_002.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/logreg_001.png, width=500 frac=0.75]  This scatterplot shows the binary $Y$ variables and the corresponding $x$ data for each category. <div id=\"fig:logreg_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:logreg_001\"></div>\n",
    "\n",
    "<p>This scatterplot shows the binary $Y$ variables and the corresponding $x$ data for each category.</p>\n",
    "<img src=\"fig-machine_learning/logreg_001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig-machine_learning/logreg_002.png, width=500 frac=0.75]  This shows the fitted logistic regression on the data shown in [Figure](#fig:logreg_001). The points along the curve are the probabilities that each point lies in either of the two categories.  <div id=\"fig:logreg_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:logreg_002\"></div>\n",
    "\n",
    "<p>This shows the fitted logistic regression on the data shown in [Figure](#fig:logreg_001). The points along the curve are the probabilities that each point lies in either of the two categories.</p>\n",
    "<img src=\"fig-machine_learning/logreg_002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "For a deeper understanding of logistic regression, we need to alter our\n",
    "notation slightly and once again use our projection methods. More generally we\n",
    "can rewrite Equation ([1](#eq:prob)) as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:probbeta\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(\\mathbf{x}) = \\frac{1}{1+\\exp(-\\boldsymbol{\\beta}^T \\mathbf{x})}\n",
    "\\label{eq:probbeta} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\boldsymbol{\\beta}, \\mathbf{x}\\in \\mathbb{R}^n$.  From our\n",
    "prior work on projection we know that the signed perpendicular distance between\n",
    "$\\mathbf{x}$ and the linear boundary described by $\\boldsymbol{\\beta}$ is\n",
    "$\\boldsymbol{\\beta}^T \\mathbf{x}/\\Vert\\boldsymbol{\\beta}\\Vert$.  This means\n",
    "that the probability that is assigned to any point in $\\mathbb{R}^n$ is a\n",
    "function of how close that point is to the linear boundary described by the\n",
    "following equation,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{\\beta}^T \\mathbf{x} = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " But there is something subtle hiding here. Note that \n",
    "for any $\\alpha\\in\\mathbb{R}$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha\\boldsymbol{\\beta}^T \\mathbf{x} = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " describes the *same* hyperplane. This means that we can multiply\n",
    "$\\boldsymbol{\\beta}$ by an arbitrary scalar and still get the same geometry.\n",
    "However, because of $\\exp(-\\alpha\\boldsymbol{\\beta}^T \\mathbf{x})$ in Equation\n",
    "([2](#eq:probbeta)), this scaling determines the intensity of the probability\n",
    "attributed to $\\mathbf{x}$. This is illustrated in [Figure](#fig:logreg_003).\n",
    "The panel on the left shows two categories (squares/circles) split by the\n",
    "dotted line that is determined by $\\boldsymbol{\\beta}^T\\mathbf{x}=0$. The\n",
    "background colors show the probabilities assigned to points in the plane.  The\n",
    "right panel shows that by scaling with $\\alpha$, we can increase the\n",
    "probabilities of class membership for the given points, given the exact same\n",
    "geometry. The points near the boundary have lower probabilities because they\n",
    "could easily be on the opposite side.  However, by scaling by $\\alpha$, we can\n",
    "raise those probabilities to any desired level at the cost of driving the\n",
    "points further from the boundary closer to one. Why is this a problem? By\n",
    "driving the probabilities arbitrarily using $\\alpha$, we can overemphasize the\n",
    "training set at the cost of out-of-sample data. That is, we may wind up\n",
    "insisting on emphatic class membership of yet unseen points that are close to\n",
    "the boundary that otherwise would have more equivocal probabilities (say, near\n",
    "$1/2$).  Once again, this is another manifestation of bias/variance trade-off.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-machine_learning/logreg_003.png, width=500 frac=1.25] Scaling can arbitrarily increase the probabilities of points near the decision boundary.   <div id=\"fig:logreg_003\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:logreg_003\"></div>\n",
    "\n",
    "<p>Scaling can arbitrarily increase the probabilities of points near the decision boundary.</p>\n",
    "<img src=\"fig-machine_learning/logreg_003.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "Regularization is a method that controls this effect by penalizing the size of\n",
    "$\\beta$ as part of its solution. Algorithmically, logistic regression works by\n",
    "iteratively solving a sequence of weighted least squares problems. Regression\n",
    "adds a $\\Vert\\boldsymbol{\\beta}\\Vert/C$ term to the least squares error. To see\n",
    "this in action, let's create some data from a logistic regression and see if we\n",
    "can recover it using Scikit-learn. Let's start with a scatter of points in the\n",
    "two-dimensional  plane,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x0,x1=np.random.rand(2,20)*6-3\n",
    "X = np.c_[x0,x1,x1*0+1] # stack as columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that `X` has a third column of all ones. This is a\n",
    "trick to allow the corresponding line to be offset from the origin\n",
    "in the two-dimensional plane. Next, we create a linear boundary\n",
    "and assign the class probabilities according to proximity to the\n",
    "boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta = np.array([1,-1,1]) # last coordinate for affine offset\n",
    "prd = X.dot(beta)\n",
    "probs = 1/(1+np.exp(-prd/np.linalg.norm(beta)))\n",
    "c = (prd>0) # boolean array class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  This establishes the training data.  The next block\n",
    "creates the logistic regression object and fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "_=lr.fit(X[:,:-1],c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that we have to omit the third dimension because of\n",
    "how Scikit-learn internally breaks down the components of the\n",
    "boundary. The resulting code extracts the corresponding\n",
    "$\\boldsymbol{\\beta}$ from the `LogisticRegression` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betah = np.r_[lr.coef_.flat,lr.intercept_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The Numpy `np.r_`  object provides a quick way to stack Numpy\n",
    "arrays horizontally instead of using `np.hstack`.\n",
    "\n",
    "\n",
    "\n",
    " The resulting boundary is shown in the left panel in [Figure](#fig:logreg_004). The crosses and triangles represent the two classes we\n",
    "created above, along with the separating gray line.  The logistic regression\n",
    "fit produces the dotted black line. The dark circle is the point that logistic\n",
    "regression categorizes incorrectly. The regularization parameter is $C=1$ by\n",
    "default. Next, we can change the strength of the regularization parameter as in\n",
    "the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and the re-fit the data to produce the right panel in\n",
    "[Figure](#fig:logreg_004). By increasing the regularization\n",
    "parameter, we essentially nudged the fitting algorithm to\n",
    "*believe* the data more than the general model. That is, by doing\n",
    "this we accepted more variance in exchange for better bias.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-machine_learning/logreg_004.png, width=500 frac=1.25]  The left panel shows the resulting boundary (dashed line) with $C=1$ as the regularization parameter. The right panel is for $C=1000$. The gray line is the boundary used to assign the class membership for the synthetic data. The dark circle is the point that logistic regression categorizes incorrectly. <div id=\"fig:logreg_004\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:logreg_004\"></div>\n",
    "\n",
    "<p>The left panel shows the resulting boundary (dashed line) with $C=1$ as the regularization parameter. The right panel is for $C=1000$. The gray line is the boundary used to assign the class membership for the synthetic data. The dark circle is the point that logistic regression categorizes incorrectly.</p>\n",
    "<img src=\"fig-machine_learning/logreg_004.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "### Maximum Likelihood Estimation for Logistic Regression\n",
    "\n",
    "Let us again consider the binary classification problem.  We define $y_k =\n",
    "\\mathbb{P}(C_1\\vert \\mathbf{x}_k)$, the conditional probability of the data as\n",
    "a member of given class. Our construction of this problem provides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_k = \\theta([\\mathbf{w},w_0] \\cdot [\\mathbf{x}_k,1])\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\theta$ is the logistic function.  Recall that there are only\n",
    "two classes for this problem. The data set looks like the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\lbrace(\\mathbf{x}_0,r_0),\\ldots,(\\mathbf{x}_k,r_k),\\ldots,(\\mathbf{x}_{n-1},r_{n-1})\\rbrace\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $r_k\\in \\lbrace 0,1 \\rbrace$. For example, we could have the\n",
    "following sequence of observed classes,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\lbrace C_0,C_1,C_1,C_0,C_1 \\rbrace\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For this case the likelihood is then the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\ell= \\mathbb{P}(C_0\\vert\\mathbf{x}_0)\\mathbb{P}(C_1\\vert\\mathbf{x}_1)\\mathbb{P}(C_1\\vert\\mathbf{x}_1) \\mathbb{P}(C_0\\vert\\mathbf{x}_0)\\mathbb{P}(C_1\\vert\\mathbf{x}_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which we can rewrite as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\ell(\\mathbf{w},w_0)= (1-y_0) y_1 y_2 (1-y_3) y_4\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Recall that there are two mutually exhaustive classes. More\n",
    "generally, this can be written as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\ell(\\mathbf{w}\\vert\\mathcal{X})=\\prod_k^n y_k^{r_k} (1-y_k)^{1-r_k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Naturally, we want to compute the logarithm of this as the cross-entropy,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E = -\\sum_k r_k \\log(y_k) + (1-r_k)\\log(1-y_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and then minimize this to find $\\mathbf{w}$ and $w_0$. This is\n",
    "difficult to do with calculus because the derivatives have non-linear terms in\n",
    "them that are hard to solve for. \n",
    "\n",
    "### Multi-Class Logistic Regression Using Softmax\n",
    "\n",
    "The logistic regression problem provides a solution for the probability\n",
    "between exactly two alternative classes. To extend to the multi-class\n",
    "problem, we need the *softmax* function. Consider the likelihood \n",
    "ratio between the $i^{th}$ class and the reference class, $\\mathcal{C}_k$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\log\\frac{p(\\mathbf{x}\\vert \\mathcal{C}_i)}{p(\\mathbf{x}\\vert\\mathcal{C}_k)} = \\mathbf{w}_i^T \\mathbf{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Taking the exponential of this and normalizing across all\n",
    "the classes gives the softmax function,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_i=p(\\mathcal{C}_i \\vert\\mathbf{x})=\\frac{\\exp\\left(\\mathbf{w}_i^T\\mathbf{x}\\right)}{\\sum_k \\exp\\left( \\mathbf{w}_k^T \\mathbf{x}\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that $\\sum_i y_i=1$. If the $\\mathbf{w}_i^T \\mathbf{x}$ term is\n",
    "larger than the others, after the exponentiation and normalization, it\n",
    "automatically suppresses the other $y_j \\forall j \\neq i$, which acts like the\n",
    "maximum function, except this function is differentiable, hence *soft*, as in\n",
    "*softmax*.  While that is all straightforward, the trick is deriving the\n",
    "$\\mathbf{w}_i$ vectors from the training data $\\lbrace\\mathbf{x}_i,y_i\\rbrace$.\n",
    "\n",
    "Once again, the launching point is the likelihood function. As with the\n",
    "two-class logistic regression problem, we have the likelihood as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\ell = \\prod_k \\prod_i (y_i^k)^{r_i^k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The log-likelihood of this is the same as the cross-entropy,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E = - \\sum_k \\sum_i r_i^k \\log y_i^k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is the error function we want to minimize. The computation works\n",
    "as before with logistic regression, except there are more derivatives to keep\n",
    "track of in this case.\n",
    "\n",
    "### Understanding Logistic Regression\n",
    "\n",
    "To generalize this technique beyond logistic regression, we need to re-think\n",
    "the problem more abstractly as the data set $\\lbrace x_i, y_i \\rbrace$.  We\n",
    "have the $y_i \\in \\lbrace 0,1 \\rbrace$ data modeled as Bernoulli random\n",
    "variables. We also have the $x_i$ data associated with each $y_i$, but it is\n",
    "not clear how to exploit this association.  What we would like is to construct\n",
    "$\\mathbb{E}(Y|X)$ which we already know (see [ch:prob](#ch:prob)) is the\n",
    "best MSE estimator. For this problem, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(Y|X) =\\mathbb{P}(Y|X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " because only $Y=1$ is nonzero in the summation. Regardless, we \n",
    "don't have the conditional probabilities anyway. One way to look at \n",
    "logistic regression is as a way to build in the functional relationship \n",
    "between $y_i$ and $x_i$. The simplest thing we could do is \n",
    "approximate,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(Y|X) \\approx \\beta_0 + \\beta_1 x := \\eta(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If this is the model, then the target would be the $y_i$ data. We can\n",
    "force the output of this linear regression into the interval $[0,1]$ by composing \n",
    "it with a sigmoidal function,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta(x) = \\frac{1}{1+\\exp(-x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then we have a new function $\\theta(\\eta(x))$ to match against $y_i$ using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\beta_0,\\beta_1) = \\sum_i (\\theta(\\eta(x_i))-y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is a nice setup for an optimization problem. We could certainly\n",
    "solve this numerically using `scipy.optimize`. Unfortunately, this would take\n",
    "us into the black box of the optimization algorithm where we would lose all of\n",
    "our intuitions and experience with linear regression.  We can take the opposite\n",
    "approach. Instead of trying to squash the output of the linear estimator into\n",
    "the desired domain, we can map the $y_i$ data into the unbounded space of the\n",
    "linear estimator. Thus, we define the inverse of the above $\\theta$ function as\n",
    "the *link* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "g(y) = \\log \\left( \\frac{y}{1-y} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This means that our approximation to the \n",
    "unknown conditional expectation is the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "g(\\mathbb{E}(Y|X)) \\approx \\beta_0 + \\beta_1 x := \\eta(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We cannot apply this directly to the $y_i$, so we compute the Taylor\n",
    "series expansion centered on $\\mathbb{E}(Y|X)$, up to the linear term, to obtain the\n",
    "following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "g(Y) & \\approx & g(\\mathbb{E}(Y|X)) + (Y-\\mathbb{E}(Y|X)) g'(\\mathbb{E}(Y|X))  \\\\\n",
    "     & \\approx & \\eta(x) + (Y-\\theta(\\eta(x))) g'(\\theta(\\eta(x))) := z\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Because we do not know the conditional expectation, we replaced these\n",
    "terms with our earlier $\\theta(\\eta(x))$ function.  This new approximation\n",
    "defines our transformed data that we will use to feed the linear model.  Note\n",
    "that the $\\beta$ parameters are embedded in this transformation.  The\n",
    "$(Y-\\theta(\\eta(x)))$ term acts as the usual additive noise term. Also,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "g'(x) = \\frac{1}{x(1-x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following code applies this transformation to the `xi,yi` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "v = 0.9\n",
    "@np.vectorize\n",
    "def gen_y(x):\n",
    "    if x<5: return np.random.choice([0,1],p=[v,1-v]) \n",
    "    else:   return np.random.choice([0,1],p=[1-v,v])\n",
    "\n",
    "xi = np.sort(np.random.rand(500)*10)\n",
    "yi = gen_y(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b0, b1 = -2,0.5\n",
    "g = lambda x: np.log(x/(1-x))\n",
    "theta = lambda x: 1/(1+np.exp(-x))\n",
    "eta = lambda x: b0 + b1*x\n",
    "theta_ = theta(eta(xi))\n",
    "z=eta(xi)+(yi-theta_)/(theta_*(1-theta_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.pylab import subplots\n",
    "fig,ax=subplots()\n",
    "_=ax.plot(xi,z,'k.',alpha=.3)\n",
    "_=ax2 = ax.twinx()\n",
    "_=ax2.plot(xi,yi,'.r',alpha=.3)\n",
    "_=ax2.set_ylabel('Y',fontsize=16,rotation=0,color='r')\n",
    "_=ax2.spines['right'].set_color('red')\n",
    "_=ax.set_xlabel('X',fontsize=16)\n",
    "_=ax.set_ylabel('Z',fontsize=16,rotation=0)\n",
    "_=ax2.tick_params(axis='y', colors='red')\n",
    "fig.savefig('fig-machine_learning/glm_001.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/glm_001.png, width=500 frac=0.65] The transformation underlying logistic regression. <div id=\"fig:glm_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:glm_001\"></div>\n",
    "\n",
    "<p>The transformation underlying logistic regression.</p>\n",
    "<img src=\"fig-machine_learning/glm_001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "Note the two vertical scales shown in [Figure](#fig:glm_001). The red scale on the\n",
    "right is the $\\lbrace 0,1 \\rbrace$ domain of the $y_i$ data (red dots) and the\n",
    "left scale is transformed $z_i$ data (black dots). Note that the transformed\n",
    "data is more linear where the original data is less equivocal at the extremes.\n",
    "Also, this transformation used a specific pair of  $\\beta_i$ parameters. The\n",
    "idea is to iterate over this transformation and derive new $\\beta_i$\n",
    "parameters.  With this approach,  we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{V}(Z|X) = (g')^2 \\mathbb{V}(Y|X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Recall that, for this binary variable, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(Y|X) = \\theta(\\eta(x)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Thus,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{V}(Y|X) = \\theta(\\eta(x)) (1-\\theta(\\eta(x)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " from which we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{V}(Z|X) = \\left[ \\theta(\\eta(x))(1-\\theta(\\eta(x))) \\right]^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The important fact here is the variance is a function of the $X$\n",
    "(i.e., heteroskedastic). As we discussed with Gauss-Markov, the appropriate\n",
    "linear regression is  weighted least-squares where the weights at each data\n",
    "point are inversely proportional to the variance. This ensures that the\n",
    "regression process accounts for this heteroskedasticity. Numpy has a weighted\n",
    "least squares implemented in the `polyfit` function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w=(theta_*(1-theta_))\n",
    "p=np.polyfit(xi,z,1,w=np.sqrt(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The output of this fit is shown in [Figure](#fig:glm_002), along\n",
    "with the raw data and $\\mathbb{V}(Z|X)$ for this particular fitted $\\beta_i$.\n",
    "Iterating a few more times refines the estimated line but it does not take many\n",
    "such iterations to converge. As indicated by the variance line, the fitted line\n",
    "favors the data at either extreme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "ax2 = ax.twinx()\n",
    "theta_ = theta(eta(xi))\n",
    "_=ax.plot(xi,\n",
    "        z,\n",
    "        '.',alpha=.3,\n",
    "        label='z'\n",
    "       );\n",
    "_=ax.axis(ymax=30,ymin=-30);\n",
    "_=ax2.plot(xi,theta_*(1-theta_),'.r',alpha=.3,label='$\\mathbb{V}(Z|X)$');\n",
    "_=ax2.axis(ymax=0.27,ymin=0);\n",
    "p=np.polyfit(xi,z,1,w=np.sqrt(w));\n",
    "_=ax.plot(xi,np.polyval(p,xi),'k',label='fitted');\n",
    "_=ax2.legend(fontsize=16);\n",
    "_=ax.legend(loc=2);\n",
    "_=ax.set_xlabel('X');\n",
    "_=ax2.tick_params(axis='y', colors='red');\n",
    "fig.savefig('fig-machine_learning/glm_002.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/glm_002.png, width=500 frac=0.65] The output of the weighted least squares fit is shown, along with the raw data and $\\mathbb{V}(Z|X)$.  <div id=\"fig:glm_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:glm_002\"></div>\n",
    "\n",
    "<p>The output of the weighted least squares fit is shown, along with the raw data and $\\mathbb{V}(Z|X)$.</p>\n",
    "<img src=\"fig-machine_learning/glm_002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
