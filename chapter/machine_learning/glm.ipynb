{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # TODO: create separate section score statistic (Dobson, p.77) -->\n",
    "<!-- # TODO: change confusing theta notation (Dobson, p.77) -->\n",
    "<!-- # TODO: add comment about deviance matching xi^2 DOF in (Dobson, p.) -->\n",
    "<!-- # TODO: goodness-of-fit analysis (feature selection) Introduction_Generalized_Linear_Models_Madsen -->\n",
    "\n",
    "\n",
    "Logistic regression is one example of a wider class of Generalized Linear\n",
    "Models (GLMs). These GLMs have the following three key features\n",
    "\n",
    "* A target $Y$ variable distributed according to one of the exponential family of distributions (e.g., Normal, binomial, Poisson)\n",
    "\n",
    "* An equation that links the expected value of $Y$ with a linear combination \n",
    "  of the observed variables (i.e., $\\left\\{ x_1,x_2,\\ldots,x_n \\right\\}$). \n",
    "\n",
    "* A smooth invertible *link* function $g(x)$ such that $g(\\mathbb{E}(Y)) = \\sum_k \\upbeta_k x_k$\n",
    "\n",
    "### Exponential Family\n",
    "\n",
    "Here is the one-parameter exponential family,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(y;\\lambda) = e^{\\lambda y - \\upgamma(\\lambda)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The *natural parameter* is $\\lambda$ and $y$ is the sufficient\n",
    "statistic. For example, for logistic regression, we have\n",
    "$\\upgamma(\\lambda)=-\\log(1+e^{\\lambda})$ and $\\lambda=\\log{\\frac{p}{1-p}}$. \n",
    "\n",
    "An important property of this exponential family is that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:dgamma\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{\\lambda}(y) = \\frac{d\\upgamma(\\lambda)}{d\\lambda}=\\upgamma'(\\lambda)\n",
    "\\end{equation}\n",
    "\\label{eq:dgamma} \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To see this, we compute the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "1 &=  \\int f(y;\\lambda) dy  =  \\int e^{\\lambda y - \\upgamma(\\lambda)} dy \\\\\n",
    "0 &=  \\int \\frac{df(y;\\lambda)}{d\\lambda} dy =\\int e^{\\lambda y-\\upgamma (\\lambda)} \\left(y-\\upgamma'(\\lambda)\\right) dy  \\\\\n",
    "\\int y e^{\\lambda  y-\\upgamma (\\lambda )} dy &= \\mathbb{E}_{\\lambda}(y)=\\upgamma'(\\lambda ) \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Using the same technique, we also have,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{V}_{\\lambda}(Y) = \\upgamma''(\\lambda)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which explains the usefulness of this generalized notation for the\n",
    "exponential family.\n",
    "\n",
    "### Deviance\n",
    "\n",
    "The scaled Kullback-Leibler divergence is called the *deviance* as\n",
    "defined below,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D(f_1,f_2) = 2 \\int f_1(y) \\log{\\frac{f_1(y)}{f_2(y)}}dy\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hoeffding's Lemma.**\n",
    "\n",
    "Using our exponential family notation, we can write out the deviance as\n",
    "the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{1}{2} D(f(y;\\lambda_1), f(y;\\lambda_2)) & = \\int f(y;\\lambda_1)\\log \\frac{f(y;\\lambda_1)}{f(y;\\lambda_2)}  dy   \\\\\n",
    "                                              & = \\int f(y;\\lambda_1) ((\\lambda_1-\\lambda_2) y -(\\gamma(\\lambda_1)-\\gamma(\\lambda_2))) dy \\\\\n",
    "                                              & = \\mathbb{E}_{\\lambda_1}  [ (\\lambda_1-\\lambda_2) y -(\\gamma(\\lambda_1)-\\gamma(\\lambda_2)) ] \\\\\n",
    "                                              & = (\\lambda_1-\\lambda_2) \\mathbb{E}_{\\lambda_1}(y) -(\\gamma(\\lambda_1)-\\gamma(\\lambda_2)) \\\\\n",
    "                                              & = (\\lambda_1-\\lambda_2) \\upmu_1 -(\\gamma(\\lambda_1)-\\gamma(\\lambda_2)) \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\upmu_1:=\\mathbb{E}_{\\lambda_1}(y)$. For the maximum likelihood\n",
    "estimate $\\hat{\\lambda}_1$, we have $\\upmu_1=y$. Plugging this\n",
    "into the above equation gives the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{1}{2} D(f(y;\\hat{\\lambda}_1),f(y;\\lambda_2))&=(\\hat{\\lambda}_1-\\lambda_2) y -(\\gamma(\\hat{\\lambda}_1)-\\gamma(\\lambda_2))  \\\\\n",
    "&= \\log{f(y;\\hat{\\lambda}_1)} - \\log{f(y;\\lambda_2)} \\\\\n",
    "&= \\log\\frac{f(y;\\hat{\\lambda}_1)}{f(y;\\lambda_2)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Taking the negative exponential of both sides gives,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(y;\\lambda_2) = f(y;\\hat{\\lambda}_1) e^{-\\frac{1}{2} D(f(y;\\hat{\\lambda}_1),f(y;\\lambda_2)) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Because $D$ is always non-negative, the likelihood\n",
    "is maximized when the the deviance is zero. In particular,\n",
    "for the scalar case, it means that $y$ itself is the\n",
    "best maximum likelihood estimate for the mean. Also,\n",
    "$f(y;\\hat{\\lambda}_1)$ is called the *saturated* model.\n",
    "We write Hoeffding's Lemma as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lemma\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f(y;\\upmu) = f(y;y)e^{-\\frac{1}{2} D(f(y;y),f(y;\\upmu))} \n",
    "\\end{equation}\n",
    "\\label{eq:lemma} \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  to emphasize that $f(y;y)$ is the likelihood function when the \n",
    "mean is replaced by the sample itself and $f(y;\\upmu)$ is the likelihood function \n",
    "when the mean is replaced by $\\upmu$.\n",
    "\n",
    "\n",
    "\n",
    "Vectorizing Equation ([2](#eq:lemma)) using mutual independence gives the\n",
    "following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(\\mathbf{y};\\boldsymbol{\\upmu}) = e^{-\\sum_i D(y_i,\\upmu_i)} \\prod f(y_i;y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The idea now is to minimize the deviance by deriving,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{\\upmu}(\\boldsymbol{\\upbeta}) = g^{-1}(\\mathbf{M}^T\\boldsymbol{\\upbeta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This means the the MLE $\\hat{\\boldsymbol{\\upbeta}}$ is the\n",
    "best $p\\times 1$ vector $\\boldsymbol{\\upbeta}$ that minimizes the\n",
    "total deviance where $g$ is the *link* function and $\\mathbf{M}$ is\n",
    "the $p\\times n$ *structure* matrix. This is the key step with GLM\n",
    "estimation because it reduces the number of parameters from $n$ to\n",
    "$p$. The structure matrix is where the associated $x_i$ data enters\n",
    "into the problem.  Thus, GLM maximum likelihood fitting minimizes the\n",
    "total deviance like plain linear regression minimizes the sum of\n",
    "squares.\n",
    "\n",
    "With the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{\\lambda} = \\mathbf{M}^T \\boldsymbol{\\upbeta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " with $2\\times n$ dimensional $\\mathbf{M}$.  The corresponding joint\n",
    "density function is the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(\\mathbf{y};\\upbeta)=e^{\\boldsymbol{\\upbeta}^T\\boldsymbol{\\xi}-\\psi(\\boldsymbol{\\upbeta})} f_0(\\mathbf{y})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{\\xi} = \\mathbf{M} \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\psi(\\boldsymbol{\\upbeta}) = \\sum \\upgamma(\\mathbf{m}_i^T \\boldsymbol{\\upbeta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where now the sufficient statistic is $\\boldsymbol{\\xi}$ and the\n",
    "parameter vector is $\\boldsymbol{\\upbeta}$, which fits into our exponential\n",
    "family format, and $\\mathbf{m}_i$  is the $i^{th}$ column of $\\mathbf{M}$. \n",
    "\n",
    "Given this joint density, we can compute the log likelihood as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\ell = \\boldsymbol{\\upbeta}^T\\boldsymbol{\\xi}-\\psi(\\boldsymbol{\\upbeta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To maximize this likelihood, we take the derivative of this with\n",
    "respect to $\\boldsymbol{\\upbeta}$ to obtain the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{d\\ell}{d\\boldsymbol{\\upbeta}}=\\mathbf{M}\\mathbf{y}-\\mathbf{M}\\boldsymbol{\\upmu}(\\mathbf{M}^T\\boldsymbol{\\upbeta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " since $\\upgamma'(\\mathbf{m}_i^T \\boldsymbol{\\upbeta}) =\n",
    "\\mathbf{m}_i^T \\upmu_i(\\boldsymbol{\\upbeta})$ and (c.f. Equation ([1](#eq:dgamma))), \n",
    "$\\upgamma' = \\upmu_{\\lambda}$. \n",
    "Setting this derivative equal to zero gives the conditions\n",
    "for the maximum likelihood solution,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:conditions\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{M}(\\mathbf{y}- \\boldsymbol{\\upmu}(\\mathbf{M}^T\\boldsymbol{\\upbeta})) = \\mathbf{0}\n",
    "\\end{equation}\n",
    "\\label{eq:conditions} \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\boldsymbol{\\upmu}$ is the element-wise inverse of the link function. This \n",
    "leads us to exactly the same place we started: trying to regress $\\mathbf{y}$ against \n",
    "$\\boldsymbol{\\upmu}(\\mathbf{M}^T\\boldsymbol{\\upbeta})$. \n",
    "\n",
    "### Example\n",
    "\n",
    "The structure matrix $\\mathbf{M}$ is where the $x_i$ data \n",
    "associated with the corresponding $y_i$ enters the problem. If we choose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{M}^T = [\\mathbf{1}, \\mathbf{x}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\mathbf{1}$ is an $n$-length vector and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{\\upbeta} = [\\upbeta_0, \\upbeta_1]^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " with $\\upmu(x) = 1/(1+e^{-x})$, we have the original logistic regression problem.\n",
    "\n",
    "Generally, $\\boldsymbol{\\upmu}(\\boldsymbol{\\upbeta})$ is a nonlinear\n",
    "function and thus we regress against our transformed variable $\\mathbf{z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{z} = \\mathbf{M}^T\\boldsymbol{\\upbeta} + \\diag(g'(\\boldsymbol{\\upmu}))(\\mathbf{y}-\\boldsymbol{\\upmu}(\\mathbf{M}^T\\boldsymbol{\\upbeta}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This fits the format of the Gauss Markov\n",
    "(see [ch:stats:sec:gauss](#ch:stats:sec:gauss)) problem and has the \n",
    "\n",
    "\n",
    "following solution,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:bhat\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{\\upbeta}}=(\\mathbf{M} \\mathbf{R}_z^{-1}\\mathbf{M}^T)^{-1}\\mathbf{M} \\mathbf{R}_z^{-1}\\mathbf{z}\n",
    "\\end{equation}\n",
    "\\label{eq:bhat} \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{R}_z:=\\mathbb{V}(\\mathbf{z})=\\diag(g'(\\boldsymbol{\\upmu}))^2\\mathbf{R}=\\mathbf{v}(\\upmu)\\diag(g'(\\boldsymbol{\\upmu}))^2\\mathbf{I}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $g$ is the link function and $\\mathbf{v}$ is the variance\n",
    "function on the designated distribution of the $y_i$.\n",
    "Thus, $\\hat{\\boldsymbol{\\upbeta}}$ has the following covariance matrix,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{V}(\\hat{\\boldsymbol{\\upbeta}}) = (\\mathbf{M}\\mathbf{R}_z^{-1}\\mathbf{M}^T)^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " These results allow inferences about the estimated parameters\n",
    "$\\hat{\\boldsymbol{\\upbeta}}$.  We can easily write Equation ([4](#eq:bhat))\n",
    "as an iteration as follow,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\upbeta}}_{k+1}=(\\mathbf{M} \\mathbf{R}_{z_k}^{-1}\\mathbf{M}^T)^{-1}\\mathbf{M} \\mathbf{R}_{z_k}^{-1}\\mathbf{z}_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Consider the data shown in [Figure](#fig:glm_003). Note that the variance of\n",
    "the data increases for each $x$ and the data increases as a power of $x$ along\n",
    "$x$. This makes this data a good candidate for a Poisson GLM with $g(\\upmu) =\n",
    "\\log(\\upmu)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats.distributions import poisson\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.pylab import subplots\n",
    "import numpy as np\n",
    "\n",
    "def gen_data(n,ns=10):\n",
    "    param = n**2\n",
    "    return poisson(param).rvs(ns)\n",
    "\n",
    "fig,ax=subplots()\n",
    "xi = []\n",
    "yi = []\n",
    "for i in [2,3,4,5,6]:\n",
    "    xi.append([i]*10)\n",
    "    yi.append(gen_data(i))\n",
    "    _=ax.plot(xi[-1],yi[-1],'ko',alpha=.3)\n",
    "\n",
    "_=ax.axis(xmin=1,xmax=7)\n",
    "xi = np.array(xi)\n",
    "yi = np.array(yi)\n",
    "x = xi.flatten()\n",
    "y = yi.flatten()\n",
    "_=ax.set_xlabel('x');\n",
    "fig.savefig('fig-machine_learning/glm_003.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/glm_003.png, width=500 frac=0.55] Some data for Poisson example.  <div id=\"fig:glm_003\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:glm_003\"></div>\n",
    "\n",
    "<p>Some data for Poisson example.</p>\n",
    "<img src=\"fig-machine_learning/glm_003.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "We can use our iterative matrix-based approach. The following code \n",
    "initializes the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M   = np.c_[x*0+1,x].T\n",
    "gi  = np.exp             # inverse g link function\n",
    "bk  = np.array([.9,0.5]) # initial point\n",
    "muk = gi(M.T @ bk).flatten() \n",
    "Rz  = np.diag(1/muk)\n",
    "zk  = M.T @ bk + Rz @ (y-muk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and this next block establishes the main iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while abs(sum(M @ (y-muk))) > .01: # orthogonality condition as threshold\n",
    "   Rzi = np.linalg.inv(Rz)\n",
    "   bk = (np.linalg.inv(M @ Rzi @ M.T)) @ M @ Rzi @ zk\n",
    "   muk = gi(M.T @ bk).flatten()\n",
    "   Rz =np.diag(1/muk)\n",
    "   zk = M.T @ bk + Rz @ (y-muk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " with corresponding final $\\boldsymbol{\\upbeta}$ computed as the \n",
    "following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(bk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " with corresponding estimated $\\mathbb{V}(\\hat{\\boldsymbol{\\upbeta}})$ as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.linalg.inv(M @ Rzi @ M.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The orthogonality condition Equation ([3](#eq:conditions)) is the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(M @ (y-muk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, the `statsmodels` module provides the Poisson GLM object.\n",
    "Note that the reported standard error is the square root of the \n",
    "diagonal elements of $\\mathbb{V}(\\hat{\\boldsymbol{\\upbeta}})$. A plot of \n",
    "the data and the fitted model is shown below in Figure ([fig:glm_004](#fig:glm_004))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pm=sm.GLM(y, sm.tools.add_constant(x), \n",
    "                        family=sm.families.Poisson())\n",
    "pm_results=pm.fit()\n",
    "pm_results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b0,b1=pm_results.params\n",
    "fig,ax=subplots()\n",
    "_=ax.plot(xi[:,0],np.exp(xi[:,0]*b1+b0),'-o');\n",
    "_=ax.plot(xi,yi,'ok',alpha=.3);\n",
    "fig.savefig('fig-machine_learning/glm_004.png')\n",
    "_=ax.set_xlabel('x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/glm_004.png, width=500 frac=0.55] Fitted using the Poisson GLM.   <div id=\"fig:glm_004\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:glm_004\"></div>\n",
    "\n",
    "<p>Fitted using the Poisson GLM.</p>\n",
    "<img src=\"fig-machine_learning/glm_004.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "<!-- #  Good reference [[efron2016computer]](#efron2016computer) -->\n",
    "<!-- #  [[fox2015applied]](#fox2015applied) -->\n",
    "<!-- #  [[lindsey1997applying]](#lindsey1997applying) -->\n",
    "\n",
    "<!-- # *Applied Predictive Modeling by Kuhn*, p. 283, -->\n",
    "<!-- # *generalized linear models by Rodriguez*, p.72, p. 119 -->"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
