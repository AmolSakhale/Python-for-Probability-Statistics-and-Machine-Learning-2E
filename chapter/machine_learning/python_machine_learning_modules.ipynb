{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python provides many bindings for machine learning libraries, some\n",
    "specialized for technologies such as neural networks, and others\n",
    "geared towards novice users. For our discussion, we focus on the\n",
    "powerful and popular Scikit-learn module.  Scikit-learn is\n",
    "distinguished by its consistent and sensible API, its wealth of\n",
    "machine learning algorithms, its clear documentation, and its readily\n",
    "available datasets that make it easy to follow along with the online\n",
    "documentation. Like Pandas, Scikit-learn relies on Numpy for numerical\n",
    "arrays. Since its release in 2007, Scikit-learn has become the most\n",
    "widely-used, general-purpose, open-source machine learning modules\n",
    "that is popular in both industry and academia.  As with all of the\n",
    "Python modules we use, Scikit-learn is available on all the major\n",
    "platforms.\n",
    "\n",
    "To get started, let's revisit the familiar ground of linear regression using\n",
    "Scikit-learn. First, let's create some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.pylab import subplots\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = np.arange(10)         # create some data\n",
    "Y = X+np.random.randn(10) # linear with noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We next import and create an instance of the `LinearRegression`\n",
    "class from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr=LinearRegression() # create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Scikit-learn has a wonderfully consistent API. All\n",
    "Scikit-learn objects use the `fit` method to compute model parameters\n",
    "and the `predict` method to evaluate the model.  For the\n",
    "`LinearRegression` instance, the `fit` method computes the\n",
    "coefficients of the linear fit. This method requires a matrix of\n",
    "inputs where the rows are the samples and the columns are the\n",
    "features. The *target* of the regression are the `Y` values, which\n",
    "must be correspondingly shaped, as in the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,Y = X.reshape((-1,1)), Y.reshape((-1,1))\n",
    "lr.fit(X,Y)\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The negative one in the `reshape((-1,1))` call above is for the truly\n",
    "lazy.  Using a negative one tells Numpy to figure out what that\n",
    "dimension should be given the other dimension and number of array\n",
    "elements.\n",
    "\n",
    "\n",
    "\n",
    " the `coef_` property of the linear regression object shows\n",
    "the estimated parameters for the fit. The convention is to denote\n",
    "estimated parameters with a trailing underscore. The model has a\n",
    "`score` method that computes the $R^2$ value for the regression.\n",
    "Recall from our statistics chapter ([ch:stats:sec:reg](#ch:stats:sec:reg)) that the\n",
    "$R^2$ value is an indicator of the quality of the fit and varies\n",
    "between zero (bad fit) and one (perfect fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr.score(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, that we have this fitted, we can evaluate\n",
    "the fit using the `predict` method,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xi = np.linspace(0,10,15) # more points to draw\n",
    "xi = xi.reshape((-1,1)) # reshape as columns\n",
    "yp = lr.predict(xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The resulting fit is shown in [Figure](#fig:python_machine_learning_modules_002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "_=ax.plot(xi,yp,'-k',lw=3)\n",
    "_=ax.plot(X,Y,'o',ms=20,color='gray')\n",
    "_=ax.tick_params(labelsize='x-large')\n",
    "_=ax.set_xlabel('$X$')\n",
    "_=ax.set_ylabel('$Y$')\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-machine_learning/python_machine_learning_modules_002.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/python_machine_learning_modules_002.png, width=500 frac=0.75] The Scikit-learn module can easily perform basic linear regression. The circles show the *training* data and the fitted line is shown in black. <div id=\"fig:python_machine_learning_modules_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:python_machine_learning_modules_002\"></div>\n",
    "\n",
    "<p>The Scikit-learn module can easily perform basic linear regression. The circles show the <em>training</em> data and the fitted line is shown in black.</p>\n",
    "<img src=\"fig-machine_learning/python_machine_learning_modules_002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "**Multilinear Regression.** The Scikit-learn module easily extends\n",
    "linear regression to multiple dimensions.  For example, for\n",
    "multi-linear regression,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y = \\alpha_0 + \\alpha_1 x_1 + \\alpha_2 x_2 + \\ldots + \\alpha_n x_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The problem is to find all of the $\\alpha$ terms given the\n",
    "training set $\\left\\{x_1,x_2,\\ldots,x_n,y\\right\\}$. We can create another\n",
    "example data set and see how this works,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=np.random.randint(20,size=(10,2))\n",
    "Y=X.dot([1,3])+1 + np.random.randn(X.shape[0])*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ym=Y/Y.max() # scale for marker size\n",
    "fig,ax=subplots()\n",
    "_=ax.scatter(X[:,0],X[:,1],ym*400,color='gray',alpha=.7,label=r'$Y=f(X_1,X_2)$')\n",
    "_=ax.set_xlabel(r'$X_1$',fontsize=22)\n",
    "_=ax.set_ylabel(r'$X_2$',fontsize=22)\n",
    "_=ax.set_title('Two dimensional regression',fontsize=20)\n",
    "_=ax.legend(loc=4,fontsize=22)\n",
    "fig.tight_layout()\n",
    "fig.savefig('fig-machine_learning/python_machine_learning_modules_003.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/python_machine_learning_modules_003.png, width=500 frac=0.85] Scikit-learn can easily perform multi-linear regression. The size of the circles indicate the value of the two-dimensional function of $(X_1,X_2)$.  <div id=\"fig:python_machine_learning_modules_003\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:python_machine_learning_modules_003\"></div>\n",
    "\n",
    "<p>Scikit-learn can easily perform multi-linear regression. The size of the circles indicate the value of the two-dimensional function of $(X_1,X_2)$.</p>\n",
    "<img src=\"fig-machine_learning/python_machine_learning_modules_003.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    " [Figure](#fig:python_machine_learning_modules_003) shows the\n",
    "two dimensional regression example, where the size of the circles is\n",
    "proportional to the targetted $Y$ value.  Note that we salted the output\n",
    "with random noise just to keep things interesting. Nonetheless, the\n",
    "interface with Scikit-learn is the same,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr=LinearRegression()\n",
    "lr.fit(X,Y)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `coef_` variable now has two terms in it,\n",
    "corresponding to the two input dimensions. Note that the constant\n",
    "offset is already built-in and is an option on the `LinearRegression`\n",
    "constructor. [Figure](#fig:python_machine_learning_modules_004) shows\n",
    "how the regression performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_=lr.fit(X,Y)\n",
    "yp=lr.predict(X)\n",
    "ypm=yp/yp.max() # scale for marker size\n",
    "_=ax.scatter(X[:,0],X[:,1],ypm*400,marker='x',color='k',lw=2,alpha=.7,label=r'$\\hat{Y}$')\n",
    "_=ax.legend(loc=0,fontsize=22)\n",
    "_=fig.canvas.draw()\n",
    "fig.savefig('fig-machine_learning/python_machine_learning_modules_004.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/python_machine_learning_modules_004.png, width=500 frac=0.85] The predicted data is plotted in black. It overlays the training data, indicating a good fit.  <div id=\"fig:python_machine_learning_modules_004\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:python_machine_learning_modules_004\"></div>\n",
    "\n",
    "<p>The predicted data is plotted in black. It overlays the training data, indicating a good fit.</p>\n",
    "<img src=\"fig-machine_learning/python_machine_learning_modules_004.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "**Polynomial Regression.** We can extend this to include polynomial\n",
    "regression by using the `PolynomialFeatures` in the `preprocessing`\n",
    "sub-module. To keep it simple, let's go back to our one-dimensional\n",
    "example. First, let's create some synthetic data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = np.arange(10).reshape(-1,1) # create some data\n",
    "Y = X+X**2+X**3+ np.random.randn(*X.shape)*80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- *f* -->\n",
    "\n",
    " Next, we have to create a transformation\n",
    "from `X` to a polynomial of `X`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qfit = PolynomialFeatures(degree=2) # quadratic\n",
    "Xq = qfit.fit_transform(X)\n",
    "print(Xq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note there is an automatic constant term in the output $0^{th}$\n",
    "column where `fit_transform` has mapped the single-column input into a set of\n",
    "columns representing the individual polynomial terms.  The middle column has\n",
    "the linear term, and the last has the quadratic term. With these polynomial\n",
    "features stacked as columns of `Xq`, all we have to do is `fit` and `predict`\n",
    "again. The following draws a comparison between the linear regression and the\n",
    "quadratic regression (see [Figure](#fig:python_machine_learning_modules_005)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr=LinearRegression() # create linear model \n",
    "qr=LinearRegression() # create quadratic model \n",
    "lr.fit(X,Y)  # fit linear model\n",
    "qr.fit(Xq,Y) # fit quadratic model\n",
    "lp = lr.predict(xi)\n",
    "qp = qr.predict(qfit.fit_transform(xi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/python_machine_learning_modules_005.png, width=500 frac=0.85] The title shows the $R^2$ score for the linear and quadratic rogressions. <div id=\"fig:python_machine_learning_modules_005\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:python_machine_learning_modules_005\"></div>\n",
    "\n",
    "<p>The title shows the $R^2$ score for the linear and quadratic rogressions.</p>\n",
    "<img src=\"fig-machine_learning/python_machine_learning_modules_005.png\" width=500>\n",
    "\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "_=ax.plot(xi,lp,'--k',lw=3,label='linear fit')\n",
    "_=ax.plot(xi,qp,'-k',lw=3,label='quadratic fit')\n",
    "_=ax.plot(X.flat,Y.flat,'o',color='gray',ms=10,label='training data')\n",
    "_=ax.legend(loc=0)\n",
    "_=ax.set_title('quad R2={:.3}; linear R2={:.3}'.format(lr.score(X,Y),qr.score(Xq,Y)))\n",
    "_=ax.set_xlabel('$X$',fontsize=22)\n",
    "_=ax.set_ylabel(r'$X+X^2+X^3+\\epsilon$',fontsize=22)\n",
    "fig.savefig('fig-machine_learning/python_machine_learning_modules_005.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This just scratches the surface of Scikit-learn. We will go through\n",
    "many more examples later, but the main thing is to concentrate on the\n",
    "usage (i.e., `fit`, `predict`) which is standardized across all of the\n",
    "machine learning methods in Scikit-learn."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
